<!DOCTYPE html>
<html>
<head>
<title>resume_questions.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<!-- possible phone screen questions -->
<h2 id="phone-screen-questions">Phone Screen Questions</h2>
<h3 id="xr-perception-rd-position">XR Perception R&amp;D position</h3>
<p>Based on the job description for a position in the Engineering Group, specifically within Systems Engineering at Qualcomm Semiconductor Limited, focusing on XR (Augmented Reality and Virtual Reality) Perception R&amp;D, the interview questions are likely to cover a broad range of topics. These questions will not only assess the candidate's technical skills and experience but also their ability to contribute to Qualcomm's objectives in AR and VR technologies. Here are some of the most common interview questions that might be asked:</p>
<ol>
<li>
<p><strong>Technical Expertise in Computer Vision and Machine Learning</strong></p>
<ul>
<li>Can you explain the principles of Computer Vision?
Computer Vision: https://en.wikipedia.org/wiki/Computer_vision</li>
</ul>
<h4 id="image-pre-processing">Image Pre-processing</h4>
<ul>
<li>
<p><strong>Resampling</strong> methods such as bilinear, bicubic, and nearest neighbor interpolation adjust the resolution of images, useful for resizing images or changing their resolution for analysis consistency.</p>
<ul>
<li><strong>重采样</strong>：包括双线性插值、双三次插值、最近邻插值等方法，用于调整图像的分辨率，适用于改变图像大小或分辨率以保持分析的一致性。</li>
</ul>
</li>
<li>
<p><strong>Normalization</strong> techniques like min-max and z-score normalization standardize the range of pixel values, which is critical for machine learning models to perform optimally.</p>
<ul>
<li><strong>归一化</strong>：如最小-最大归一化和Z分数归一化等技术，用于标准化像素值的范围，对于机器学习模型的优化性能至关重要。</li>
</ul>
</li>
<li>
<p><strong>Color Space Conversion</strong> transforms images from one color representation to another, like RGB to grayscale or RGB to HSV. This is often done to simplify the analysis or to extract specific features from images.</p>
<ul>
<li><strong>色彩空间转换</strong>：将图像从一种色彩表示转换到另一种，例如RGB到灰度或RGB到HSV，通常是为了简化分析或从图像中提取特定特征。</li>
</ul>
</li>
<li>
<p><strong>Filtering</strong> is applied to remove noise, smooth images, or sharpen image features. Common filters include:</p>
</li>
<li>
<p><strong>Gaussian Filter</strong> for smoothing to reduce image noise and detail.</p>
</li>
<li>
<p><strong>Median Filter</strong> for noise reduction, particularly useful in removing 'salt and pepper' noise.</p>
</li>
<li>
<p><strong>Mean Filter</strong> for basic smoothing by averaging the pixels within a neighborhood.</p>
<ul>
<li><strong>滤波</strong>：应用于去除噪声、平滑图像或锐化图像特征。常用滤波器包括：</li>
<li><strong>高斯滤波</strong>：用于减少图像噪声和细节。</li>
<li><strong>中值滤波</strong>：特别适用于去除“盐和胡椒”噪声。</li>
<li><strong>均值滤波</strong>：通过对邻域内的像素进行平均来基本平滑图像。</li>
</ul>
</li>
<li>
<p><strong>Contrast Enhancement</strong> techniques like histogram equalization and adaptive histogram equalization improve the visibility of features in an image by adjusting the image's contrast.</p>
<ul>
<li><strong>对比度增强</strong>：技术如直方图均衡化和自适应直方图均衡化通过调整图像的对比度来改善图像中特征的可见性。</li>
</ul>
</li>
</ul>
<h4 id="feature-extraction">Feature Extraction</h4>
<ul>
<li>Detecting <strong>lines, edges, and ridges</strong> using methods like Canny edge detection and the Hough transform. These features are critical for understanding the structure within images.
<ul>
<li>使用如Canny边缘检测和霍夫变换等方法检测<strong>线条、边缘和脊线</strong>。这些特征对理解图像内的结构至关重要。</li>
</ul>
</li>
<li>Identifying <strong>corners and keypoints</strong> with algorithms such as Harris corner detection, SIFT (Scale-Invariant Feature Transform), SURF (Speeded Up Robust Features), and ORB (Oriented FAST and Rotated BRIEF). These features are essential for tasks like image matching, object detection, and tracking.
<ul>
<li>通过Harris角点检测、SIFT（尺度不变特征变换）、SURF（加速稳健特征）和ORB（定向快速旋转简明）等算法识别<strong>角点和关键点</strong>。这些特征对于图像匹配、物体检测和跟踪等任务至关重要。</li>
</ul>
</li>
<li><strong>Texture and shape descriptors</strong> like LBP (Local Binary Patterns) and HOG (Histogram of Oriented Gradients) are used to capture and represent the texture and shape characteristics of objects in images.
<ul>
<li>LBP（局部二值模式）和HOG（方向梯度直方图）等<strong>纹理和形状描述符</strong>用于捕捉和表示图像中物体的纹理和形状特征。</li>
</ul>
</li>
</ul>
<h4 id="detection-and-segmentation">Detection and Segmentation</h4>
<ul>
<li><strong>Selecting interest points</strong> involves methods like NMS (Non-Maximum Suppression) and RANSAC (Random Sample Consensus) to identify and refine the selection of key features or points in an image.
<ul>
<li><strong>选择兴趣点</strong>：涉及如NMS（非极大抑制）和RANSAC（随机样本一致性）等方法，用于识别和细化图像中的关键特征或点。</li>
</ul>
</li>
<li><strong>Segmentation</strong> divides images into parts or regions based on certain criteria. Traditional methods include thresholding, region growing, and watershed algorithms. Modern machine learning approaches like FCN (Fully Convolutional Networks), U-Net, and Mask R-CNN offer advanced capabilities for segmenting images more precisely.
<ul>
<li><strong>分割</strong>：基于某些标准将图像分成部分或区域。传统方法包括阈值法、区域生长和分水岭算法。现代机器学习方法如FCN（全卷积网络）、U-Net和Mask R-CNN为更精确地分割图像提供了高级功能。</li>
</ul>
</li>
</ul>
<h4 id="high-level-processing-and-decision-making">High-level Processing and Decision Making</h4>
<ul>
<li><strong>Pattern Recognition and Classification</strong>: At this stage, the system identifies patterns, objects, and scenarios within the images.</li>
<li><strong>模式识别和分类</strong>：此阶段系统识别图像中的模式、物体和场景。技术范围从传统的机器学习方法（如支持向量机SVM和决策树）到先进的深度学习模型（如卷积神经网络CNNs）。</li>
<li><strong>物体检测和识别</strong>：现代深度学习方法，如R-CNN系列（包括Fast R-CNN、Faster R-CNN）、YOLO（You Only Look Once）和SSD（单次多框检测器），极大地革新了机器检测和识别图像中物体的能力，提高了准确性和速度。</li>
</ul>
<h4 id="kalman-filter-usage">Kalman Filter Usage</h4>
<p>测动态系统的未来状态，以最小化平方误差的均值。它在计算机视觉中广泛用于实时跟踪移动对象、导航和机器人技术。它通过结合随时间的测量来估计过程的状态，有效地处理不确定性。</p>
<ul>
<li>Can you explain the principles of SLAM (Simultaneous Localization and Mapping) and how it applies to AR/VR technologies?</li>
</ul>
<pre class="hljs"><code><div>SLAM: https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping
SLAM技术（Simultaneous Localization and Mapping）
是一种用于构建环境地图并确定自身位置的技术。在AR/VR中，SLAM技术用于实时地图构建和定位，以便将虚拟对象与现实世界对齐。SLAM技术通常涉及传感器数据融合、特征提取和匹配、优化算法等方面。

流程通常包括：
1. 传感器数据采集：使用相机、激光雷达、惯性测量单元（IMU）等传感器采集环境数据。
2. 特征提取和匹配：从传感器数据中提取特征点，并将它们与先前的地图进行匹配。
3. 优化算法：使用优化算法（如图优化或非线性优化）来估计相机姿态和地图结构。
4. 实时定位和地图构建：在实时环境中更新地图并定位相机的位置。

数据融合:
- filter-based methods (e.g., Extended Kalman Filter, Unscented Kalman Filter)
- optimization-based methods (e.g., Bundle Adjustment, Pose Graph Optimization)
- deep learning-based methods (e.g., SLAMNet, VIO-Net)


</div></code></pre>
<ul>
<li>
<p>How have you contributed to the development of efficient and accurate computer vision and machine learning solutions for XR perception tasks in your previous roles?</p>
</li>
<li>
<p>Elaborate on your understanding of 3D computer vision methods and mathematics, covering areas like SLAM, 3D reconstruction, object detection, and sensor fusion.</p>
</li>
<li>
<p>Describe your experience with 3D pose tracking and scene understanding. How have you applied these in a project?</p>
</li>
<li>
<p>What challenges have you encountered while working on object detection and segmentation for real-time systems, and how did you overcome them?</p>
</li>
<li>
<p>Describe a project where you applied your knowledge of mathematical optimization to enhance the performance of XR perception systems.</p>
</li>
<li>
<p>How do you approach sensor fusion in XR systems, and what mathematical optimization techniques do you find most effective?</p>
</li>
</ul>
</li>
<li>
<p><strong>Programming and Software Development Skills</strong></p>
<ul>
<li>
<p>What experience do you have with Python and C++ programming in the context of XR applications?</p>
</li>
<li>
<p>Describe a project where you developed computer vision or machine learning solutions for embedded or mobile platforms. What were the key challenges and your solutions?</p>
</li>
<li>
<p>How have you implemented deep neural networks for edge devices? Can you discuss your experience with model optimization techniques like quantization and pruning?</p>
</li>
</ul>
</li>
<li>
<p><strong>Understanding and Analyzing Requirements</strong></p>
<ul>
<li>
<p>How do you approach requirement analysis for XR perception systems? Can you give an example of how you translated requirements into a successful project outcome?</p>
</li>
<li>
<p>UML diagrams</p>
</li>
<li>
<p>Agile framework</p>
</li>
<li>
<p>How do you approach understanding and analyzing requirements for perception systems in the context of XR platforms?</p>
</li>
<li>
<p>In your experience, how important is cross-functional collaboration in developing XR technologies? Can you share an instance where collaboration significantly impacted a project?</p>
</li>
</ul>
</li>
<li>
<p><strong>Practical Experience and Problem-Solving</strong></p>
<ul>
<li>
<p>Share an example of a complex problem you solved in the domain of XR perception. What was your thought process, and what solutions did you implement?</p>
</li>
<li>
<p>Given a scenario where you must optimize an XR application for better performance on mobile platforms, what steps would you take to analyze and improve it?</p>
</li>
</ul>
</li>
<li>
<p><strong>Industry Experience and Future Vision</strong></p>
<ul>
<li>With your background in AR/VR, where do you see the future of mobile perception technology heading?</li>
<li>vision for the future of XR, mentioning emerging technologies or trends you believe will be significant.</li>
<li>How do you stay updated with the latest advancements in computer vision, machine learning, and XR technologies? Can you discuss a recent breakthrough that excited you?</li>
<li>Neural Radiance Fields (NeRF)</li>
</ul>
</li>
<li>
<p><strong>Preferred Qualifications Specifics</strong></p>
<ul>
<li>
<p>Have you worked on testing and debugging XR devices or mobile platforms? What tools and processes do you use for effective debugging?</p>
</li>
<li>
<p>GDB</p>
</li>
<li>
<p>Valgrind</p>
</li>
<li>
<p>Cost functions</p>
</li>
<li>
<p>Loss curves</p>
</li>
<li>
<p>ROC curves</p>
</li>
<li>
<p>F1 score</p>
</li>
<li>
<p>Confusion matrices</p>
</li>
<li>
<p>Precision and recall</p>
</li>
<li>
<p>Residuals</p>
</li>
<li>
<p>Discuss your experience with deploying machine learning models on edge devices, especially in the context of XR. How do you ensure the balance between performance and accuracy?</p>
<ul>
<li>Quantization</li>
<li>Pruning</li>
<li>Knowledge distillation</li>
<li>Model compression</li>
</ul>
</li>
</ul>
</li>
</ol>

</body>
</html>
