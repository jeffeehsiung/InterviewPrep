{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Networks and Deep Learning  \n",
        "##Assignment 3.3 - Self-attention and Transformers\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ],
      "metadata": {
        "id": "3i1KfYGFRRWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LlTJbgaaRTct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9576dc-b8de-45e2-a74e-baaf9dd44852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Qu6w5GLkRezN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5eb33f2-5c34-4bd4-9ff4-4d0aeb977407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 29 13:38:02 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ],
      "metadata": {
        "id": "N-sUz1A9SzVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ],
      "metadata": {
        "id": "v6ol1XZtiPpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Define the input data X\n",
        "# X consists out of 32 samples, each sample has dimensionality 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to find its query, keys and values vectors\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ],
      "metadata": {
        "id": "AgWIgp51RgC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2f2637-d600-4e6e-9f90-0149e650ba55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attention outputs are\n",
            " [[ 5.94649427 -4.78885695  2.69339595 ...  2.67537566 -1.92788296\n",
            "  -5.84180026]\n",
            " [ 5.94649427 -4.78885695  2.69339595 ...  2.67537566 -1.92788296\n",
            "  -5.84180026]\n",
            " [ 5.94649427 -4.78885695  2.69339595 ...  2.67537566 -1.92788296\n",
            "  -5.84180026]\n",
            " ...\n",
            " [ 5.94649427 -4.78885695  2.69339595 ...  2.67537566 -1.92788296\n",
            "  -5.84180026]\n",
            " [ 5.94649427 -4.78885695  2.69339595 ...  2.67537566 -1.92788296\n",
            "  -5.84180026]\n",
            " [ 5.94649427 -4.78885695  2.69339595 ...  2.67537566 -1.92788296\n",
            "  -5.84180026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ],
      "metadata": {
        "id": "iozM1k4khO0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # number of samples in a batch\n",
        "dim_input = 128 # dimension of each item in the sample sequence\n",
        "seq_len = 20 # sequence length for each sample\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "print(attention)"
      ],
      "metadata": {
        "id": "qng07v8xdaPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f96f28-9eb3-40e4-b3f5-a991b30c3b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 32])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outputs:  torch.Size([32, 20, 32])\n",
            "tensor([[[ 0.1062,  0.2745, -0.1343,  ..., -0.0352, -0.1163,  0.0330],\n",
            "         [ 0.0970,  0.2121, -0.1087,  ..., -0.0788, -0.0703,  0.1035],\n",
            "         [ 0.0434,  0.2334, -0.1686,  ..., -0.0139, -0.1211,  0.0082],\n",
            "         ...,\n",
            "         [ 0.1437,  0.2548, -0.1055,  ...,  0.0412, -0.1330,  0.0568],\n",
            "         [ 0.1192,  0.3243, -0.2195,  ...,  0.0196, -0.1274, -0.0039],\n",
            "         [ 0.1215,  0.2546, -0.1763,  ...,  0.0532,  0.0028,  0.0379]],\n",
            "\n",
            "        [[-0.0564, -0.0443,  0.1304,  ...,  0.0649,  0.1988, -0.0550],\n",
            "         [-0.1243, -0.1221,  0.2133,  ...,  0.1476,  0.0563, -0.0584],\n",
            "         [-0.1881, -0.0823,  0.1961,  ...,  0.0323,  0.1730, -0.0402],\n",
            "         ...,\n",
            "         [-0.0858,  0.0633,  0.2266,  ...,  0.0645,  0.0847, -0.0446],\n",
            "         [-0.0828, -0.0642,  0.1553,  ...,  0.0583,  0.1639, -0.0678],\n",
            "         [-0.1290, -0.0361,  0.1747,  ...,  0.0824,  0.1010, -0.0458]],\n",
            "\n",
            "        [[ 0.0093, -0.1084, -0.0730,  ...,  0.0750,  0.2029, -0.0076],\n",
            "         [-0.0031, -0.1424, -0.1412,  ...,  0.0579,  0.0873, -0.0551],\n",
            "         [ 0.0416, -0.1391, -0.1247,  ...,  0.0488,  0.1132, -0.0535],\n",
            "         ...,\n",
            "         [ 0.0231, -0.1232, -0.0097,  ...,  0.1234,  0.2785,  0.0509],\n",
            "         [ 0.0666, -0.1019, -0.0280,  ...,  0.0840,  0.2840, -0.0740],\n",
            "         [ 0.0078, -0.0934, -0.0831,  ...,  0.0746,  0.1791,  0.0151]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.2236, -0.0816,  0.1841,  ...,  0.0362,  0.0123, -0.2813],\n",
            "         [-0.2727, -0.1933,  0.1958,  ...,  0.1471, -0.0060, -0.2870],\n",
            "         [-0.1661, -0.0671,  0.1609,  ...,  0.0629,  0.0448, -0.2161],\n",
            "         ...,\n",
            "         [-0.1142, -0.1132,  0.2761,  ...,  0.0911,  0.0565, -0.2266],\n",
            "         [-0.2667, -0.1362,  0.2213,  ...,  0.0657, -0.0057, -0.3731],\n",
            "         [-0.1793, -0.0992,  0.2167,  ...,  0.0153, -0.0305, -0.2855]],\n",
            "\n",
            "        [[ 0.1811, -0.0973,  0.0769,  ..., -0.0607,  0.1089, -0.0367],\n",
            "         [ 0.3397, -0.0393, -0.0369,  ..., -0.0269,  0.0237,  0.0818],\n",
            "         [ 0.2461, -0.1342, -0.0753,  ..., -0.0288,  0.0838, -0.1002],\n",
            "         ...,\n",
            "         [ 0.2760, -0.0529, -0.0543,  ..., -0.0472, -0.0059,  0.0063],\n",
            "         [ 0.2829, -0.0536, -0.0349,  ..., -0.0319,  0.0845, -0.0728],\n",
            "         [ 0.2976, -0.0695, -0.0856,  ..., -0.0055,  0.1468, -0.0765]],\n",
            "\n",
            "        [[-0.0646,  0.1055,  0.1855,  ...,  0.1602,  0.0136, -0.0844],\n",
            "         [-0.0649,  0.1095,  0.1538,  ...,  0.1091,  0.0138, -0.1464],\n",
            "         [-0.1239,  0.1419,  0.1445,  ...,  0.1913, -0.0294, -0.2071],\n",
            "         ...,\n",
            "         [-0.0773,  0.2062,  0.2638,  ...,  0.1708,  0.0221, -0.1515],\n",
            "         [-0.1721,  0.0860,  0.2000,  ...,  0.1149, -0.0237, -0.1477],\n",
            "         [-0.0619,  0.1817,  0.2400,  ...,  0.2474,  0.0485, -0.0204]]],\n",
            "       grad_fn=<BmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ],
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This package is needed to build the transformer\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "-C06IoPIjePg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e354af-3617-4924-a09e-043b81d622a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ],
      "metadata": {
        "id": "wx9eZrMpmA2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian Error Linear Units is another type of activation function\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and test function\n"
      ],
      "metadata": {
        "id": "YTawNC64mhBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ],
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remember the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ],
      "metadata": {
        "id": "DRYys50km0-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ],
      "metadata": {
        "id": "rVLJLLDuj7yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "metadata": {
        "id": "Vlt3tk-MkDB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d85abe1-2a20-4954-cd64-37004950fd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:03<00:00, 2605056.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 490050.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4362496.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10566017.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3878\n",
            "[12800/60000 ( 21%)]  Loss: 1.1405\n",
            "[25600/60000 ( 43%)]  Loss: 0.2831\n",
            "[38400/60000 ( 64%)]  Loss: 0.2775\n",
            "[51200/60000 ( 85%)]  Loss: 0.2736\n",
            "\n",
            "Average test loss: 0.1949  Accuracy: 9410/10000 (94.10%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2099\n",
            "[12800/60000 ( 21%)]  Loss: 0.1197\n",
            "[25600/60000 ( 43%)]  Loss: 0.2502\n",
            "[38400/60000 ( 64%)]  Loss: 0.1544\n",
            "[51200/60000 ( 85%)]  Loss: 0.2510\n",
            "\n",
            "Average test loss: 0.1407  Accuracy: 9560/10000 (95.60%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1514\n",
            "[12800/60000 ( 21%)]  Loss: 0.0877\n",
            "[25600/60000 ( 43%)]  Loss: 0.1231\n",
            "[38400/60000 ( 64%)]  Loss: 0.1312\n",
            "[51200/60000 ( 85%)]  Loss: 0.1697\n",
            "\n",
            "Average test loss: 0.0991  Accuracy: 9689/10000 (96.89%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0545\n",
            "[12800/60000 ( 21%)]  Loss: 0.1860\n",
            "[25600/60000 ( 43%)]  Loss: 0.0290\n",
            "[38400/60000 ( 64%)]  Loss: 0.0783\n",
            "[51200/60000 ( 85%)]  Loss: 0.0391\n",
            "\n",
            "Average test loss: 0.0999  Accuracy: 9687/10000 (96.87%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1040\n",
            "[12800/60000 ( 21%)]  Loss: 0.0372\n",
            "[25600/60000 ( 43%)]  Loss: 0.1130\n",
            "[38400/60000 ( 64%)]  Loss: 0.1313\n",
            "[51200/60000 ( 85%)]  Loss: 0.0287\n",
            "\n",
            "Average test loss: 0.0844  Accuracy: 9734/10000 (97.34%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0655\n",
            "[12800/60000 ( 21%)]  Loss: 0.0180\n",
            "[25600/60000 ( 43%)]  Loss: 0.0396\n",
            "[38400/60000 ( 64%)]  Loss: 0.0813\n",
            "[51200/60000 ( 85%)]  Loss: 0.0823\n",
            "\n",
            "Average test loss: 0.0740  Accuracy: 9772/10000 (97.72%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0089\n",
            "[12800/60000 ( 21%)]  Loss: 0.0115\n",
            "[25600/60000 ( 43%)]  Loss: 0.0111\n",
            "[38400/60000 ( 64%)]  Loss: 0.0387\n",
            "[51200/60000 ( 85%)]  Loss: 0.0670\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9793/10000 (97.93%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0354\n",
            "[12800/60000 ( 21%)]  Loss: 0.0222\n",
            "[25600/60000 ( 43%)]  Loss: 0.0083\n",
            "[38400/60000 ( 64%)]  Loss: 0.0577\n",
            "[51200/60000 ( 85%)]  Loss: 0.0334\n",
            "\n",
            "Average test loss: 0.0775  Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0112\n",
            "[12800/60000 ( 21%)]  Loss: 0.0254\n",
            "[25600/60000 ( 43%)]  Loss: 0.0107\n",
            "[38400/60000 ( 64%)]  Loss: 0.0389\n",
            "[51200/60000 ( 85%)]  Loss: 0.0077\n",
            "\n",
            "Average test loss: 0.0985  Accuracy: 9731/10000 (97.31%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0657\n",
            "[12800/60000 ( 21%)]  Loss: 0.0184\n",
            "[25600/60000 ( 43%)]  Loss: 0.0191\n",
            "[38400/60000 ( 64%)]  Loss: 0.0026\n",
            "[51200/60000 ( 85%)]  Loss: 0.0488\n",
            "\n",
            "Average test loss: 0.0837  Accuracy: 9771/10000 (97.71%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.1048\n",
            "[12800/60000 ( 21%)]  Loss: 0.0149\n",
            "[25600/60000 ( 43%)]  Loss: 0.0472\n",
            "[38400/60000 ( 64%)]  Loss: 0.0474\n",
            "[51200/60000 ( 85%)]  Loss: 0.0266\n",
            "\n",
            "Average test loss: 0.0693  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0038\n",
            "[12800/60000 ( 21%)]  Loss: 0.0254\n",
            "[25600/60000 ( 43%)]  Loss: 0.0032\n",
            "[38400/60000 ( 64%)]  Loss: 0.0123\n",
            "[51200/60000 ( 85%)]  Loss: 0.0143\n",
            "\n",
            "Average test loss: 0.0771  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0037\n",
            "[12800/60000 ( 21%)]  Loss: 0.0127\n",
            "[25600/60000 ( 43%)]  Loss: 0.0090\n",
            "[38400/60000 ( 64%)]  Loss: 0.0009\n",
            "[51200/60000 ( 85%)]  Loss: 0.0070\n",
            "\n",
            "Average test loss: 0.0779  Accuracy: 9799/10000 (97.99%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0019\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0036\n",
            "[38400/60000 ( 64%)]  Loss: 0.0031\n",
            "[51200/60000 ( 85%)]  Loss: 0.0015\n",
            "\n",
            "Average test loss: 0.0676  Accuracy: 9830/10000 (98.30%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0035\n",
            "[12800/60000 ( 21%)]  Loss: 0.0112\n",
            "[25600/60000 ( 43%)]  Loss: 0.0069\n",
            "[38400/60000 ( 64%)]  Loss: 0.0023\n",
            "[51200/60000 ( 85%)]  Loss: 0.0041\n",
            "\n",
            "Average test loss: 0.0626  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0208\n",
            "[12800/60000 ( 21%)]  Loss: 0.0010\n",
            "[25600/60000 ( 43%)]  Loss: 0.0110\n",
            "[38400/60000 ( 64%)]  Loss: 0.0026\n",
            "[51200/60000 ( 85%)]  Loss: 0.0032\n",
            "\n",
            "Average test loss: 0.0692  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0013\n",
            "[12800/60000 ( 21%)]  Loss: 0.0018\n",
            "[25600/60000 ( 43%)]  Loss: 0.0093\n",
            "[38400/60000 ( 64%)]  Loss: 0.0003\n",
            "[51200/60000 ( 85%)]  Loss: 0.0018\n",
            "\n",
            "Average test loss: 0.0774  Accuracy: 9819/10000 (98.19%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0230\n",
            "[12800/60000 ( 21%)]  Loss: 0.0062\n",
            "[25600/60000 ( 43%)]  Loss: 0.0024\n",
            "[38400/60000 ( 64%)]  Loss: 0.0178\n",
            "[51200/60000 ( 85%)]  Loss: 0.0072\n",
            "\n",
            "Average test loss: 0.0907  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0026\n",
            "[12800/60000 ( 21%)]  Loss: 0.0004\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0012\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0824  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0056\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[38400/60000 ( 64%)]  Loss: 0.0017\n",
            "[51200/60000 ( 85%)]  Loss: 0.0007\n",
            "\n",
            "Average test loss: 0.0828  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Execution time: 291.51 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "88S5BZe1SxQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = [\n",
        "    {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128},  # Base configuration\n",
        "    {'dim': 128, 'depth': 6, 'heads': 8, 'mlp_dim': 256},  # Increased dim and mlp_dim\n",
        "    {'dim': 32, 'depth': 6, 'heads': 8, 'mlp_dim': 64},  # Decreased dim and mlp_dim\n",
        "    {'dim': 64, 'depth': 8, 'heads': 16, 'mlp_dim': 128},  # Increased depth and heads\n",
        "    {'dim': 64, 'depth': 3, 'heads': 4, 'mlp_dim': 128},  # Decreased depth and heads\n",
        "    {'dim': 128, 'depth': 8, 'heads': 8, 'mlp_dim': 256},  # Increased dim, depth, and mlp_dim\n",
        "    {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256},  # Increased dim, depth, heads, and mlp_dim\n",
        "]"
      ],
      "metadata": {
        "id": "_h8yCrSQSya0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for config in configurations:\n",
        "    print(\"Testing configuration:\", config)\n",
        "    model = ViT(\n",
        "        image_size=28,\n",
        "        patch_size=7,\n",
        "        num_classes=10,\n",
        "        channels=1,\n",
        "        dim=config['dim'],\n",
        "        depth=config['depth'],\n",
        "        heads=config['heads'],\n",
        "        mlp_dim=config['mlp_dim']\n",
        "    ).cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        print('Epoch:', epoch, 'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enrIylmGSzJY",
        "outputId": "27357e83-688a-4c29-de4d-71c2d7454ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing configuration: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3273\n",
            "[12800/60000 ( 21%)]  Loss: 1.1520\n",
            "[25600/60000 ( 43%)]  Loss: 0.5383\n",
            "[38400/60000 ( 64%)]  Loss: 0.2058\n",
            "[51200/60000 ( 85%)]  Loss: 0.3641\n",
            "\n",
            "Average test loss: 0.1982  Accuracy: 9387/10000 (93.87%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2749\n",
            "[12800/60000 ( 21%)]  Loss: 0.1753\n",
            "[25600/60000 ( 43%)]  Loss: 0.1376\n",
            "[38400/60000 ( 64%)]  Loss: 0.2332\n",
            "[51200/60000 ( 85%)]  Loss: 0.1462\n",
            "\n",
            "Average test loss: 0.1226  Accuracy: 9612/10000 (96.12%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1119\n",
            "[12800/60000 ( 21%)]  Loss: 0.0744\n",
            "[25600/60000 ( 43%)]  Loss: 0.0811\n",
            "[38400/60000 ( 64%)]  Loss: 0.1008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0769\n",
            "\n",
            "Average test loss: 0.0977  Accuracy: 9694/10000 (96.94%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0813\n",
            "[12800/60000 ( 21%)]  Loss: 0.1002\n",
            "[25600/60000 ( 43%)]  Loss: 0.0712\n",
            "[38400/60000 ( 64%)]  Loss: 0.1059\n",
            "[51200/60000 ( 85%)]  Loss: 0.1267\n",
            "\n",
            "Average test loss: 0.0860  Accuracy: 9718/10000 (97.18%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0728\n",
            "[12800/60000 ( 21%)]  Loss: 0.0078\n",
            "[25600/60000 ( 43%)]  Loss: 0.0687\n",
            "[38400/60000 ( 64%)]  Loss: 0.0244\n",
            "[51200/60000 ( 85%)]  Loss: 0.0396\n",
            "\n",
            "Average test loss: 0.0778  Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0487\n",
            "[12800/60000 ( 21%)]  Loss: 0.0170\n",
            "[25600/60000 ( 43%)]  Loss: 0.0771\n",
            "[38400/60000 ( 64%)]  Loss: 0.1239\n",
            "[51200/60000 ( 85%)]  Loss: 0.0181\n",
            "\n",
            "Average test loss: 0.0627  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0116\n",
            "[12800/60000 ( 21%)]  Loss: 0.0350\n",
            "[25600/60000 ( 43%)]  Loss: 0.0381\n",
            "[38400/60000 ( 64%)]  Loss: 0.0972\n",
            "[51200/60000 ( 85%)]  Loss: 0.0481\n",
            "\n",
            "Average test loss: 0.0700  Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0034\n",
            "[12800/60000 ( 21%)]  Loss: 0.0525\n",
            "[25600/60000 ( 43%)]  Loss: 0.0293\n",
            "[38400/60000 ( 64%)]  Loss: 0.1588\n",
            "[51200/60000 ( 85%)]  Loss: 0.0479\n",
            "\n",
            "Average test loss: 0.0707  Accuracy: 9773/10000 (97.73%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0405\n",
            "[12800/60000 ( 21%)]  Loss: 0.0377\n",
            "[25600/60000 ( 43%)]  Loss: 0.0204\n",
            "[38400/60000 ( 64%)]  Loss: 0.0113\n",
            "[51200/60000 ( 85%)]  Loss: 0.0782\n",
            "\n",
            "Average test loss: 0.0673  Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0351\n",
            "[12800/60000 ( 21%)]  Loss: 0.1060\n",
            "[25600/60000 ( 43%)]  Loss: 0.0969\n",
            "[38400/60000 ( 64%)]  Loss: 0.0217\n",
            "[51200/60000 ( 85%)]  Loss: 0.0038\n",
            "\n",
            "Average test loss: 0.0623  Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0266\n",
            "[12800/60000 ( 21%)]  Loss: 0.0092\n",
            "[25600/60000 ( 43%)]  Loss: 0.0177\n",
            "[38400/60000 ( 64%)]  Loss: 0.0458\n",
            "[51200/60000 ( 85%)]  Loss: 0.0121\n",
            "\n",
            "Average test loss: 0.0577  Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0160\n",
            "[12800/60000 ( 21%)]  Loss: 0.0216\n",
            "[25600/60000 ( 43%)]  Loss: 0.0017\n",
            "[38400/60000 ( 64%)]  Loss: 0.0084\n",
            "[51200/60000 ( 85%)]  Loss: 0.0305\n",
            "\n",
            "Average test loss: 0.0650  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0332\n",
            "[12800/60000 ( 21%)]  Loss: 0.0176\n",
            "[25600/60000 ( 43%)]  Loss: 0.0104\n",
            "[38400/60000 ( 64%)]  Loss: 0.0056\n",
            "[51200/60000 ( 85%)]  Loss: 0.0078\n",
            "\n",
            "Average test loss: 0.0627  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0223\n",
            "[12800/60000 ( 21%)]  Loss: 0.0095\n",
            "[25600/60000 ( 43%)]  Loss: 0.0041\n",
            "[38400/60000 ( 64%)]  Loss: 0.0028\n",
            "[51200/60000 ( 85%)]  Loss: 0.0219\n",
            "\n",
            "Average test loss: 0.0701  Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0122\n",
            "[12800/60000 ( 21%)]  Loss: 0.0066\n",
            "[25600/60000 ( 43%)]  Loss: 0.0028\n",
            "[38400/60000 ( 64%)]  Loss: 0.0176\n",
            "[51200/60000 ( 85%)]  Loss: 0.0165\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0530\n",
            "[12800/60000 ( 21%)]  Loss: 0.0067\n",
            "[25600/60000 ( 43%)]  Loss: 0.0063\n",
            "[38400/60000 ( 64%)]  Loss: 0.0075\n",
            "[51200/60000 ( 85%)]  Loss: 0.0058\n",
            "\n",
            "Average test loss: 0.0667  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0141\n",
            "[12800/60000 ( 21%)]  Loss: 0.0074\n",
            "[25600/60000 ( 43%)]  Loss: 0.0051\n",
            "[38400/60000 ( 64%)]  Loss: 0.0022\n",
            "[51200/60000 ( 85%)]  Loss: 0.0011\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0209\n",
            "[12800/60000 ( 21%)]  Loss: 0.0007\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[38400/60000 ( 64%)]  Loss: 0.0264\n",
            "[51200/60000 ( 85%)]  Loss: 0.0086\n",
            "\n",
            "Average test loss: 0.0648  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0040\n",
            "[12800/60000 ( 21%)]  Loss: 0.0013\n",
            "[25600/60000 ( 43%)]  Loss: 0.0001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0019\n",
            "[51200/60000 ( 85%)]  Loss: 0.0003\n",
            "\n",
            "Average test loss: 0.0647  Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0014\n",
            "[12800/60000 ( 21%)]  Loss: 0.0011\n",
            "[25600/60000 ( 43%)]  Loss: 0.0049\n",
            "[38400/60000 ( 64%)]  Loss: 0.0050\n",
            "[51200/60000 ( 85%)]  Loss: 0.0012\n",
            "\n",
            "Average test loss: 0.0800  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Testing configuration: {'dim': 128, 'depth': 6, 'heads': 8, 'mlp_dim': 256}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3628\n",
            "[12800/60000 ( 21%)]  Loss: 0.6922\n",
            "[25600/60000 ( 43%)]  Loss: 0.2672\n",
            "[38400/60000 ( 64%)]  Loss: 0.2463\n",
            "[51200/60000 ( 85%)]  Loss: 0.1831\n",
            "\n",
            "Average test loss: 0.1553  Accuracy: 9520/10000 (95.20%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.0721\n",
            "[12800/60000 ( 21%)]  Loss: 0.1125\n",
            "[25600/60000 ( 43%)]  Loss: 0.0792\n",
            "[38400/60000 ( 64%)]  Loss: 0.0450\n",
            "[51200/60000 ( 85%)]  Loss: 0.0754\n",
            "\n",
            "Average test loss: 0.1080  Accuracy: 9665/10000 (96.65%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0347\n",
            "[12800/60000 ( 21%)]  Loss: 0.1427\n",
            "[25600/60000 ( 43%)]  Loss: 0.1077\n",
            "[38400/60000 ( 64%)]  Loss: 0.1253\n",
            "[51200/60000 ( 85%)]  Loss: 0.0734\n",
            "\n",
            "Average test loss: 0.0812  Accuracy: 9743/10000 (97.43%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0553\n",
            "[12800/60000 ( 21%)]  Loss: 0.0757\n",
            "[25600/60000 ( 43%)]  Loss: 0.0256\n",
            "[38400/60000 ( 64%)]  Loss: 0.1331\n",
            "[51200/60000 ( 85%)]  Loss: 0.0867\n",
            "\n",
            "Average test loss: 0.0848  Accuracy: 9742/10000 (97.42%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0513\n",
            "[12800/60000 ( 21%)]  Loss: 0.1006\n",
            "[25600/60000 ( 43%)]  Loss: 0.0394\n",
            "[38400/60000 ( 64%)]  Loss: 0.0195\n",
            "[51200/60000 ( 85%)]  Loss: 0.0912\n",
            "\n",
            "Average test loss: 0.0648  Accuracy: 9800/10000 (98.00%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0378\n",
            "[12800/60000 ( 21%)]  Loss: 0.0439\n",
            "[25600/60000 ( 43%)]  Loss: 0.0177\n",
            "[38400/60000 ( 64%)]  Loss: 0.0351\n",
            "[51200/60000 ( 85%)]  Loss: 0.0252\n",
            "\n",
            "Average test loss: 0.0757  Accuracy: 9764/10000 (97.64%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0106\n",
            "[12800/60000 ( 21%)]  Loss: 0.0150\n",
            "[25600/60000 ( 43%)]  Loss: 0.0498\n",
            "[38400/60000 ( 64%)]  Loss: 0.0384\n",
            "[51200/60000 ( 85%)]  Loss: 0.0685\n",
            "\n",
            "Average test loss: 0.0694  Accuracy: 9784/10000 (97.84%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0100\n",
            "[12800/60000 ( 21%)]  Loss: 0.0097\n",
            "[25600/60000 ( 43%)]  Loss: 0.0983\n",
            "[38400/60000 ( 64%)]  Loss: 0.0567\n",
            "[51200/60000 ( 85%)]  Loss: 0.0182\n",
            "\n",
            "Average test loss: 0.0768  Accuracy: 9785/10000 (97.85%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0414\n",
            "[12800/60000 ( 21%)]  Loss: 0.0209\n",
            "[25600/60000 ( 43%)]  Loss: 0.0308\n",
            "[38400/60000 ( 64%)]  Loss: 0.0469\n",
            "[51200/60000 ( 85%)]  Loss: 0.0063\n",
            "\n",
            "Average test loss: 0.0592  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0029\n",
            "[12800/60000 ( 21%)]  Loss: 0.0070\n",
            "[25600/60000 ( 43%)]  Loss: 0.0068\n",
            "[38400/60000 ( 64%)]  Loss: 0.0181\n",
            "[51200/60000 ( 85%)]  Loss: 0.0556\n",
            "\n",
            "Average test loss: 0.0692  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0981\n",
            "[12800/60000 ( 21%)]  Loss: 0.0045\n",
            "[25600/60000 ( 43%)]  Loss: 0.0052\n",
            "[38400/60000 ( 64%)]  Loss: 0.0024\n",
            "[51200/60000 ( 85%)]  Loss: 0.0373\n",
            "\n",
            "Average test loss: 0.0665  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0031\n",
            "[12800/60000 ( 21%)]  Loss: 0.0154\n",
            "[25600/60000 ( 43%)]  Loss: 0.0067\n",
            "[38400/60000 ( 64%)]  Loss: 0.0770\n",
            "[51200/60000 ( 85%)]  Loss: 0.0055\n",
            "\n",
            "Average test loss: 0.0602  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0059\n",
            "[12800/60000 ( 21%)]  Loss: 0.0044\n",
            "[25600/60000 ( 43%)]  Loss: 0.0316\n",
            "[38400/60000 ( 64%)]  Loss: 0.0548\n",
            "[51200/60000 ( 85%)]  Loss: 0.0019\n",
            "\n",
            "Average test loss: 0.0701  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0026\n",
            "[12800/60000 ( 21%)]  Loss: 0.0419\n",
            "[25600/60000 ( 43%)]  Loss: 0.0223\n",
            "[38400/60000 ( 64%)]  Loss: 0.0040\n",
            "[51200/60000 ( 85%)]  Loss: 0.0067\n",
            "\n",
            "Average test loss: 0.0717  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0035\n",
            "[12800/60000 ( 21%)]  Loss: 0.0202\n",
            "[25600/60000 ( 43%)]  Loss: 0.0131\n",
            "[38400/60000 ( 64%)]  Loss: 0.0251\n",
            "[51200/60000 ( 85%)]  Loss: 0.0214\n",
            "\n",
            "Average test loss: 0.0720  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0183\n",
            "[12800/60000 ( 21%)]  Loss: 0.0003\n",
            "[25600/60000 ( 43%)]  Loss: 0.0005\n",
            "[38400/60000 ( 64%)]  Loss: 0.0096\n",
            "[51200/60000 ( 85%)]  Loss: 0.0017\n",
            "\n",
            "Average test loss: 0.0845  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0319\n",
            "[12800/60000 ( 21%)]  Loss: 0.0004\n",
            "[25600/60000 ( 43%)]  Loss: 0.0031\n",
            "[38400/60000 ( 64%)]  Loss: 0.0003\n",
            "[51200/60000 ( 85%)]  Loss: 0.0002\n",
            "\n",
            "Average test loss: 0.0533  Accuracy: 9864/10000 (98.64%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0091\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0310\n",
            "[38400/60000 ( 64%)]  Loss: 0.0007\n",
            "[51200/60000 ( 85%)]  Loss: 0.0032\n",
            "\n",
            "Average test loss: 0.0631  Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0004\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0336\n",
            "[38400/60000 ( 64%)]  Loss: 0.0055\n",
            "[51200/60000 ( 85%)]  Loss: 0.0029\n",
            "\n",
            "Average test loss: 0.0727  Accuracy: 9845/10000 (98.45%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0051\n",
            "[12800/60000 ( 21%)]  Loss: 0.0014\n",
            "[25600/60000 ( 43%)]  Loss: 0.0762\n",
            "[38400/60000 ( 64%)]  Loss: 0.0001\n",
            "[51200/60000 ( 85%)]  Loss: 0.0000\n",
            "\n",
            "Average test loss: 0.0599  Accuracy: 9864/10000 (98.64%)\n",
            "\n",
            "Testing configuration: {'dim': 32, 'depth': 6, 'heads': 8, 'mlp_dim': 64}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.4045\n",
            "[12800/60000 ( 21%)]  Loss: 1.4533\n",
            "[25600/60000 ( 43%)]  Loss: 1.0147\n",
            "[38400/60000 ( 64%)]  Loss: 0.6228\n",
            "[51200/60000 ( 85%)]  Loss: 0.4542\n",
            "\n",
            "Average test loss: 0.4189  Accuracy: 8695/10000 (86.95%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.3263\n",
            "[12800/60000 ( 21%)]  Loss: 0.3826\n",
            "[25600/60000 ( 43%)]  Loss: 0.4107\n",
            "[38400/60000 ( 64%)]  Loss: 0.2148\n",
            "[51200/60000 ( 85%)]  Loss: 0.2907\n",
            "\n",
            "Average test loss: 0.1891  Accuracy: 9409/10000 (94.09%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1826\n",
            "[12800/60000 ( 21%)]  Loss: 0.1925\n",
            "[25600/60000 ( 43%)]  Loss: 0.1093\n",
            "[38400/60000 ( 64%)]  Loss: 0.0725\n",
            "[51200/60000 ( 85%)]  Loss: 0.1903\n",
            "\n",
            "Average test loss: 0.1532  Accuracy: 9543/10000 (95.43%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1616\n",
            "[12800/60000 ( 21%)]  Loss: 0.1275\n",
            "[25600/60000 ( 43%)]  Loss: 0.0748\n",
            "[38400/60000 ( 64%)]  Loss: 0.1625\n",
            "[51200/60000 ( 85%)]  Loss: 0.1145\n",
            "\n",
            "Average test loss: 0.1189  Accuracy: 9628/10000 (96.28%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1861\n",
            "[12800/60000 ( 21%)]  Loss: 0.0592\n",
            "[25600/60000 ( 43%)]  Loss: 0.1893\n",
            "[38400/60000 ( 64%)]  Loss: 0.0979\n",
            "[51200/60000 ( 85%)]  Loss: 0.0855\n",
            "\n",
            "Average test loss: 0.1219  Accuracy: 9610/10000 (96.10%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0566\n",
            "[12800/60000 ( 21%)]  Loss: 0.1250\n",
            "[25600/60000 ( 43%)]  Loss: 0.1165\n",
            "[38400/60000 ( 64%)]  Loss: 0.0155\n",
            "[51200/60000 ( 85%)]  Loss: 0.0644\n",
            "\n",
            "Average test loss: 0.1001  Accuracy: 9685/10000 (96.85%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.1101\n",
            "[12800/60000 ( 21%)]  Loss: 0.0936\n",
            "[25600/60000 ( 43%)]  Loss: 0.1558\n",
            "[38400/60000 ( 64%)]  Loss: 0.0525\n",
            "[51200/60000 ( 85%)]  Loss: 0.0592\n",
            "\n",
            "Average test loss: 0.0844  Accuracy: 9743/10000 (97.43%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0485\n",
            "[12800/60000 ( 21%)]  Loss: 0.0498\n",
            "[25600/60000 ( 43%)]  Loss: 0.0675\n",
            "[38400/60000 ( 64%)]  Loss: 0.1046\n",
            "[51200/60000 ( 85%)]  Loss: 0.1438\n",
            "\n",
            "Average test loss: 0.0832  Accuracy: 9747/10000 (97.47%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0395\n",
            "[12800/60000 ( 21%)]  Loss: 0.0541\n",
            "[25600/60000 ( 43%)]  Loss: 0.0130\n",
            "[38400/60000 ( 64%)]  Loss: 0.0567\n",
            "[51200/60000 ( 85%)]  Loss: 0.0916\n",
            "\n",
            "Average test loss: 0.0780  Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0194\n",
            "[12800/60000 ( 21%)]  Loss: 0.0342\n",
            "[25600/60000 ( 43%)]  Loss: 0.0609\n",
            "[38400/60000 ( 64%)]  Loss: 0.0104\n",
            "[51200/60000 ( 85%)]  Loss: 0.0616\n",
            "\n",
            "Average test loss: 0.0822  Accuracy: 9741/10000 (97.41%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0114\n",
            "[12800/60000 ( 21%)]  Loss: 0.0297\n",
            "[25600/60000 ( 43%)]  Loss: 0.1511\n",
            "[38400/60000 ( 64%)]  Loss: 0.0419\n",
            "[51200/60000 ( 85%)]  Loss: 0.0708\n",
            "\n",
            "Average test loss: 0.0865  Accuracy: 9728/10000 (97.28%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0597\n",
            "[12800/60000 ( 21%)]  Loss: 0.0389\n",
            "[25600/60000 ( 43%)]  Loss: 0.0375\n",
            "[38400/60000 ( 64%)]  Loss: 0.0467\n",
            "[51200/60000 ( 85%)]  Loss: 0.0943\n",
            "\n",
            "Average test loss: 0.0799  Accuracy: 9760/10000 (97.60%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0972\n",
            "[12800/60000 ( 21%)]  Loss: 0.0653\n",
            "[25600/60000 ( 43%)]  Loss: 0.0127\n",
            "[38400/60000 ( 64%)]  Loss: 0.0093\n",
            "[51200/60000 ( 85%)]  Loss: 0.0091\n",
            "\n",
            "Average test loss: 0.0754  Accuracy: 9781/10000 (97.81%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0181\n",
            "[12800/60000 ( 21%)]  Loss: 0.0246\n",
            "[25600/60000 ( 43%)]  Loss: 0.0038\n",
            "[38400/60000 ( 64%)]  Loss: 0.0327\n",
            "[51200/60000 ( 85%)]  Loss: 0.0226\n",
            "\n",
            "Average test loss: 0.0765  Accuracy: 9772/10000 (97.72%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0270\n",
            "[12800/60000 ( 21%)]  Loss: 0.0292\n",
            "[25600/60000 ( 43%)]  Loss: 0.0313\n",
            "[38400/60000 ( 64%)]  Loss: 0.0206\n",
            "[51200/60000 ( 85%)]  Loss: 0.0229\n",
            "\n",
            "Average test loss: 0.0780  Accuracy: 9788/10000 (97.88%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0213\n",
            "[12800/60000 ( 21%)]  Loss: 0.0050\n",
            "[25600/60000 ( 43%)]  Loss: 0.0184\n",
            "[38400/60000 ( 64%)]  Loss: 0.0217\n",
            "[51200/60000 ( 85%)]  Loss: 0.0519\n",
            "\n",
            "Average test loss: 0.0847  Accuracy: 9766/10000 (97.66%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0348\n",
            "[12800/60000 ( 21%)]  Loss: 0.0131\n",
            "[25600/60000 ( 43%)]  Loss: 0.0494\n",
            "[38400/60000 ( 64%)]  Loss: 0.0064\n",
            "[51200/60000 ( 85%)]  Loss: 0.0469\n",
            "\n",
            "Average test loss: 0.0799  Accuracy: 9786/10000 (97.86%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0109\n",
            "[12800/60000 ( 21%)]  Loss: 0.0086\n",
            "[25600/60000 ( 43%)]  Loss: 0.0040\n",
            "[38400/60000 ( 64%)]  Loss: 0.0620\n",
            "[51200/60000 ( 85%)]  Loss: 0.0067\n",
            "\n",
            "Average test loss: 0.0757  Accuracy: 9790/10000 (97.90%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0174\n",
            "[12800/60000 ( 21%)]  Loss: 0.0499\n",
            "[25600/60000 ( 43%)]  Loss: 0.0181\n",
            "[38400/60000 ( 64%)]  Loss: 0.0400\n",
            "[51200/60000 ( 85%)]  Loss: 0.0063\n",
            "\n",
            "Average test loss: 0.0840  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0272\n",
            "[12800/60000 ( 21%)]  Loss: 0.0083\n",
            "[25600/60000 ( 43%)]  Loss: 0.0052\n",
            "[38400/60000 ( 64%)]  Loss: 0.0139\n",
            "[51200/60000 ( 85%)]  Loss: 0.0123\n",
            "\n",
            "Average test loss: 0.0815  Accuracy: 9793/10000 (97.93%)\n",
            "\n",
            "Testing configuration: {'dim': 64, 'depth': 8, 'heads': 16, 'mlp_dim': 128}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.4675\n",
            "[12800/60000 ( 21%)]  Loss: 1.3518\n",
            "[25600/60000 ( 43%)]  Loss: 0.7033\n",
            "[38400/60000 ( 64%)]  Loss: 0.4101\n",
            "[51200/60000 ( 85%)]  Loss: 0.2374\n",
            "\n",
            "Average test loss: 0.2118  Accuracy: 9331/10000 (93.31%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2262\n",
            "[12800/60000 ( 21%)]  Loss: 0.3054\n",
            "[25600/60000 ( 43%)]  Loss: 0.1723\n",
            "[38400/60000 ( 64%)]  Loss: 0.2120\n",
            "[51200/60000 ( 85%)]  Loss: 0.2036\n",
            "\n",
            "Average test loss: 0.1236  Accuracy: 9600/10000 (96.00%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1188\n",
            "[12800/60000 ( 21%)]  Loss: 0.0502\n",
            "[25600/60000 ( 43%)]  Loss: 0.1019\n",
            "[38400/60000 ( 64%)]  Loss: 0.1057\n",
            "[51200/60000 ( 85%)]  Loss: 0.1025\n",
            "\n",
            "Average test loss: 0.0984  Accuracy: 9694/10000 (96.94%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1174\n",
            "[12800/60000 ( 21%)]  Loss: 0.0725\n",
            "[25600/60000 ( 43%)]  Loss: 0.1766\n",
            "[38400/60000 ( 64%)]  Loss: 0.1017\n",
            "[51200/60000 ( 85%)]  Loss: 0.0641\n",
            "\n",
            "Average test loss: 0.0882  Accuracy: 9724/10000 (97.24%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0465\n",
            "[12800/60000 ( 21%)]  Loss: 0.0519\n",
            "[25600/60000 ( 43%)]  Loss: 0.0907\n",
            "[38400/60000 ( 64%)]  Loss: 0.0119\n",
            "[51200/60000 ( 85%)]  Loss: 0.0442\n",
            "\n",
            "Average test loss: 0.0799  Accuracy: 9743/10000 (97.43%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0092\n",
            "[12800/60000 ( 21%)]  Loss: 0.0523\n",
            "[25600/60000 ( 43%)]  Loss: 0.0554\n",
            "[38400/60000 ( 64%)]  Loss: 0.0296\n",
            "[51200/60000 ( 85%)]  Loss: 0.0243\n",
            "\n",
            "Average test loss: 0.0804  Accuracy: 9743/10000 (97.43%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0480\n",
            "[12800/60000 ( 21%)]  Loss: 0.0090\n",
            "[25600/60000 ( 43%)]  Loss: 0.0181\n",
            "[38400/60000 ( 64%)]  Loss: 0.1024\n",
            "[51200/60000 ( 85%)]  Loss: 0.0078\n",
            "\n",
            "Average test loss: 0.0779  Accuracy: 9767/10000 (97.67%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0384\n",
            "[12800/60000 ( 21%)]  Loss: 0.0146\n",
            "[25600/60000 ( 43%)]  Loss: 0.0634\n",
            "[38400/60000 ( 64%)]  Loss: 0.0769\n",
            "[51200/60000 ( 85%)]  Loss: 0.0752\n",
            "\n",
            "Average test loss: 0.0783  Accuracy: 9769/10000 (97.69%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0119\n",
            "[12800/60000 ( 21%)]  Loss: 0.0583\n",
            "[25600/60000 ( 43%)]  Loss: 0.0224\n",
            "[38400/60000 ( 64%)]  Loss: 0.0893\n",
            "[51200/60000 ( 85%)]  Loss: 0.0482\n",
            "\n",
            "Average test loss: 0.0671  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0423\n",
            "[12800/60000 ( 21%)]  Loss: 0.0046\n",
            "[25600/60000 ( 43%)]  Loss: 0.0610\n",
            "[38400/60000 ( 64%)]  Loss: 0.0179\n",
            "[51200/60000 ( 85%)]  Loss: 0.0184\n",
            "\n",
            "Average test loss: 0.0754  Accuracy: 9772/10000 (97.72%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0367\n",
            "[12800/60000 ( 21%)]  Loss: 0.0169\n",
            "[25600/60000 ( 43%)]  Loss: 0.0050\n",
            "[38400/60000 ( 64%)]  Loss: 0.0012\n",
            "[51200/60000 ( 85%)]  Loss: 0.0198\n",
            "\n",
            "Average test loss: 0.0652  Accuracy: 9799/10000 (97.99%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0303\n",
            "[12800/60000 ( 21%)]  Loss: 0.0125\n",
            "[25600/60000 ( 43%)]  Loss: 0.0083\n",
            "[38400/60000 ( 64%)]  Loss: 0.0049\n",
            "[51200/60000 ( 85%)]  Loss: 0.0114\n",
            "\n",
            "Average test loss: 0.0707  Accuracy: 9775/10000 (97.75%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0092\n",
            "[12800/60000 ( 21%)]  Loss: 0.0007\n",
            "[25600/60000 ( 43%)]  Loss: 0.0327\n",
            "[38400/60000 ( 64%)]  Loss: 0.1091\n",
            "[51200/60000 ( 85%)]  Loss: 0.0041\n",
            "\n",
            "Average test loss: 0.0724  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0911\n",
            "[12800/60000 ( 21%)]  Loss: 0.0121\n",
            "[25600/60000 ( 43%)]  Loss: 0.0005\n",
            "[38400/60000 ( 64%)]  Loss: 0.0051\n",
            "[51200/60000 ( 85%)]  Loss: 0.0105\n",
            "\n",
            "Average test loss: 0.0681  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0413\n",
            "[12800/60000 ( 21%)]  Loss: 0.0105\n",
            "[25600/60000 ( 43%)]  Loss: 0.0182\n",
            "[38400/60000 ( 64%)]  Loss: 0.0050\n",
            "[51200/60000 ( 85%)]  Loss: 0.0401\n",
            "\n",
            "Average test loss: 0.0675  Accuracy: 9837/10000 (98.37%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0152\n",
            "[12800/60000 ( 21%)]  Loss: 0.0186\n",
            "[25600/60000 ( 43%)]  Loss: 0.0038\n",
            "[38400/60000 ( 64%)]  Loss: 0.0014\n",
            "[51200/60000 ( 85%)]  Loss: 0.0226\n",
            "\n",
            "Average test loss: 0.0793  Accuracy: 9818/10000 (98.18%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0046\n",
            "[12800/60000 ( 21%)]  Loss: 0.0029\n",
            "[25600/60000 ( 43%)]  Loss: 0.0068\n",
            "[38400/60000 ( 64%)]  Loss: 0.0080\n",
            "[51200/60000 ( 85%)]  Loss: 0.0060\n",
            "\n",
            "Average test loss: 0.0862  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0274\n",
            "[12800/60000 ( 21%)]  Loss: 0.0030\n",
            "[25600/60000 ( 43%)]  Loss: 0.0011\n",
            "[38400/60000 ( 64%)]  Loss: 0.0078\n",
            "[51200/60000 ( 85%)]  Loss: 0.0072\n",
            "\n",
            "Average test loss: 0.0692  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0002\n",
            "[12800/60000 ( 21%)]  Loss: 0.0021\n",
            "[25600/60000 ( 43%)]  Loss: 0.0007\n",
            "[38400/60000 ( 64%)]  Loss: 0.0114\n",
            "[51200/60000 ( 85%)]  Loss: 0.0007\n",
            "\n",
            "Average test loss: 0.0666  Accuracy: 9846/10000 (98.46%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0025\n",
            "[12800/60000 ( 21%)]  Loss: 0.0003\n",
            "[25600/60000 ( 43%)]  Loss: 0.0021\n",
            "[38400/60000 ( 64%)]  Loss: 0.0013\n",
            "[51200/60000 ( 85%)]  Loss: 0.0064\n",
            "\n",
            "Average test loss: 0.0770  Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "Testing configuration: {'dim': 64, 'depth': 3, 'heads': 4, 'mlp_dim': 128}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3851\n",
            "[12800/60000 ( 21%)]  Loss: 0.7920\n",
            "[25600/60000 ( 43%)]  Loss: 0.4266\n",
            "[38400/60000 ( 64%)]  Loss: 0.2351\n",
            "[51200/60000 ( 85%)]  Loss: 0.2119\n",
            "\n",
            "Average test loss: 0.2305  Accuracy: 9237/10000 (92.37%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2849\n",
            "[12800/60000 ( 21%)]  Loss: 0.2144\n",
            "[25600/60000 ( 43%)]  Loss: 0.1615\n",
            "[38400/60000 ( 64%)]  Loss: 0.0803\n",
            "[51200/60000 ( 85%)]  Loss: 0.1124\n",
            "\n",
            "Average test loss: 0.1416  Accuracy: 9525/10000 (95.25%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0801\n",
            "[12800/60000 ( 21%)]  Loss: 0.1865\n",
            "[25600/60000 ( 43%)]  Loss: 0.0665\n",
            "[38400/60000 ( 64%)]  Loss: 0.1197\n",
            "[51200/60000 ( 85%)]  Loss: 0.1002\n",
            "\n",
            "Average test loss: 0.1221  Accuracy: 9620/10000 (96.20%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1306\n",
            "[12800/60000 ( 21%)]  Loss: 0.0624\n",
            "[25600/60000 ( 43%)]  Loss: 0.1265\n",
            "[38400/60000 ( 64%)]  Loss: 0.0281\n",
            "[51200/60000 ( 85%)]  Loss: 0.1376\n",
            "\n",
            "Average test loss: 0.1022  Accuracy: 9689/10000 (96.89%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1074\n",
            "[12800/60000 ( 21%)]  Loss: 0.1526\n",
            "[25600/60000 ( 43%)]  Loss: 0.0989\n",
            "[38400/60000 ( 64%)]  Loss: 0.0379\n",
            "[51200/60000 ( 85%)]  Loss: 0.0564\n",
            "\n",
            "Average test loss: 0.0879  Accuracy: 9718/10000 (97.18%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.1128\n",
            "[12800/60000 ( 21%)]  Loss: 0.0279\n",
            "[25600/60000 ( 43%)]  Loss: 0.0449\n",
            "[38400/60000 ( 64%)]  Loss: 0.0477\n",
            "[51200/60000 ( 85%)]  Loss: 0.0981\n",
            "\n",
            "Average test loss: 0.0715  Accuracy: 9772/10000 (97.72%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0534\n",
            "[12800/60000 ( 21%)]  Loss: 0.0807\n",
            "[25600/60000 ( 43%)]  Loss: 0.0384\n",
            "[38400/60000 ( 64%)]  Loss: 0.0144\n",
            "[51200/60000 ( 85%)]  Loss: 0.0826\n",
            "\n",
            "Average test loss: 0.0767  Accuracy: 9769/10000 (97.69%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0796\n",
            "[12800/60000 ( 21%)]  Loss: 0.0694\n",
            "[25600/60000 ( 43%)]  Loss: 0.0388\n",
            "[38400/60000 ( 64%)]  Loss: 0.0527\n",
            "[51200/60000 ( 85%)]  Loss: 0.0217\n",
            "\n",
            "Average test loss: 0.0739  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0796\n",
            "[12800/60000 ( 21%)]  Loss: 0.0335\n",
            "[25600/60000 ( 43%)]  Loss: 0.0391\n",
            "[38400/60000 ( 64%)]  Loss: 0.0323\n",
            "[51200/60000 ( 85%)]  Loss: 0.0529\n",
            "\n",
            "Average test loss: 0.0730  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0330\n",
            "[12800/60000 ( 21%)]  Loss: 0.0346\n",
            "[25600/60000 ( 43%)]  Loss: 0.0045\n",
            "[38400/60000 ( 64%)]  Loss: 0.0081\n",
            "[51200/60000 ( 85%)]  Loss: 0.0291\n",
            "\n",
            "Average test loss: 0.0740  Accuracy: 9784/10000 (97.84%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0122\n",
            "[12800/60000 ( 21%)]  Loss: 0.0661\n",
            "[25600/60000 ( 43%)]  Loss: 0.0261\n",
            "[38400/60000 ( 64%)]  Loss: 0.0639\n",
            "[51200/60000 ( 85%)]  Loss: 0.0239\n",
            "\n",
            "Average test loss: 0.0685  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0115\n",
            "[12800/60000 ( 21%)]  Loss: 0.0063\n",
            "[25600/60000 ( 43%)]  Loss: 0.0084\n",
            "[38400/60000 ( 64%)]  Loss: 0.0513\n",
            "[51200/60000 ( 85%)]  Loss: 0.0264\n",
            "\n",
            "Average test loss: 0.0876  Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0321\n",
            "[12800/60000 ( 21%)]  Loss: 0.0113\n",
            "[25600/60000 ( 43%)]  Loss: 0.0146\n",
            "[38400/60000 ( 64%)]  Loss: 0.0342\n",
            "[51200/60000 ( 85%)]  Loss: 0.0177\n",
            "\n",
            "Average test loss: 0.0645  Accuracy: 9818/10000 (98.18%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0463\n",
            "[12800/60000 ( 21%)]  Loss: 0.0151\n",
            "[25600/60000 ( 43%)]  Loss: 0.0292\n",
            "[38400/60000 ( 64%)]  Loss: 0.0024\n",
            "[51200/60000 ( 85%)]  Loss: 0.0715\n",
            "\n",
            "Average test loss: 0.0665  Accuracy: 9819/10000 (98.19%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0019\n",
            "[12800/60000 ( 21%)]  Loss: 0.0082\n",
            "[25600/60000 ( 43%)]  Loss: 0.0555\n",
            "[38400/60000 ( 64%)]  Loss: 0.0038\n",
            "[51200/60000 ( 85%)]  Loss: 0.0147\n",
            "\n",
            "Average test loss: 0.0722  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0039\n",
            "[12800/60000 ( 21%)]  Loss: 0.0211\n",
            "[25600/60000 ( 43%)]  Loss: 0.0235\n",
            "[38400/60000 ( 64%)]  Loss: 0.0521\n",
            "[51200/60000 ( 85%)]  Loss: 0.0153\n",
            "\n",
            "Average test loss: 0.0679  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0232\n",
            "[12800/60000 ( 21%)]  Loss: 0.0033\n",
            "[25600/60000 ( 43%)]  Loss: 0.0114\n",
            "[38400/60000 ( 64%)]  Loss: 0.0237\n",
            "[51200/60000 ( 85%)]  Loss: 0.0344\n",
            "\n",
            "Average test loss: 0.0661  Accuracy: 9841/10000 (98.41%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0085\n",
            "[12800/60000 ( 21%)]  Loss: 0.0109\n",
            "[25600/60000 ( 43%)]  Loss: 0.0042\n",
            "[38400/60000 ( 64%)]  Loss: 0.0273\n",
            "[51200/60000 ( 85%)]  Loss: 0.0232\n",
            "\n",
            "Average test loss: 0.0691  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0069\n",
            "[12800/60000 ( 21%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0010\n",
            "[38400/60000 ( 64%)]  Loss: 0.0031\n",
            "[51200/60000 ( 85%)]  Loss: 0.0103\n",
            "\n",
            "Average test loss: 0.0670  Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0221\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0057\n",
            "[38400/60000 ( 64%)]  Loss: 0.0081\n",
            "[51200/60000 ( 85%)]  Loss: 0.0061\n",
            "\n",
            "Average test loss: 0.0725  Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "Testing configuration: {'dim': 128, 'depth': 8, 'heads': 8, 'mlp_dim': 256}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3248\n",
            "[12800/60000 ( 21%)]  Loss: 0.5891\n",
            "[25600/60000 ( 43%)]  Loss: 0.3039\n",
            "[38400/60000 ( 64%)]  Loss: 0.1914\n",
            "[51200/60000 ( 85%)]  Loss: 0.1376\n",
            "\n",
            "Average test loss: 0.1494  Accuracy: 9536/10000 (95.36%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.0943\n",
            "[12800/60000 ( 21%)]  Loss: 0.2217\n",
            "[25600/60000 ( 43%)]  Loss: 0.1553\n",
            "[38400/60000 ( 64%)]  Loss: 0.0894\n",
            "[51200/60000 ( 85%)]  Loss: 0.0637\n",
            "\n",
            "Average test loss: 0.1042  Accuracy: 9678/10000 (96.78%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0614\n",
            "[12800/60000 ( 21%)]  Loss: 0.0347\n",
            "[25600/60000 ( 43%)]  Loss: 0.0648\n",
            "[38400/60000 ( 64%)]  Loss: 0.1278\n",
            "[51200/60000 ( 85%)]  Loss: 0.1441\n",
            "\n",
            "Average test loss: 0.0806  Accuracy: 9747/10000 (97.47%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0980\n",
            "[12800/60000 ( 21%)]  Loss: 0.0698\n",
            "[25600/60000 ( 43%)]  Loss: 0.1349\n",
            "[38400/60000 ( 64%)]  Loss: 0.0603\n",
            "[51200/60000 ( 85%)]  Loss: 0.0251\n",
            "\n",
            "Average test loss: 0.0695  Accuracy: 9786/10000 (97.86%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0575\n",
            "[12800/60000 ( 21%)]  Loss: 0.0154\n",
            "[25600/60000 ( 43%)]  Loss: 0.0363\n",
            "[38400/60000 ( 64%)]  Loss: 0.0701\n",
            "[51200/60000 ( 85%)]  Loss: 0.0493\n",
            "\n",
            "Average test loss: 0.0621  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0530\n",
            "[12800/60000 ( 21%)]  Loss: 0.0397\n",
            "[25600/60000 ( 43%)]  Loss: 0.0363\n",
            "[38400/60000 ( 64%)]  Loss: 0.0613\n",
            "[51200/60000 ( 85%)]  Loss: 0.0129\n",
            "\n",
            "Average test loss: 0.0641  Accuracy: 9807/10000 (98.07%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0104\n",
            "[12800/60000 ( 21%)]  Loss: 0.0310\n",
            "[25600/60000 ( 43%)]  Loss: 0.0177\n",
            "[38400/60000 ( 64%)]  Loss: 0.0117\n",
            "[51200/60000 ( 85%)]  Loss: 0.0049\n",
            "\n",
            "Average test loss: 0.0629  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0096\n",
            "[12800/60000 ( 21%)]  Loss: 0.0136\n",
            "[25600/60000 ( 43%)]  Loss: 0.0310\n",
            "[38400/60000 ( 64%)]  Loss: 0.0083\n",
            "[51200/60000 ( 85%)]  Loss: 0.0076\n",
            "\n",
            "Average test loss: 0.0643  Accuracy: 9789/10000 (97.89%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0155\n",
            "[12800/60000 ( 21%)]  Loss: 0.0090\n",
            "[25600/60000 ( 43%)]  Loss: 0.0348\n",
            "[38400/60000 ( 64%)]  Loss: 0.0089\n",
            "[51200/60000 ( 85%)]  Loss: 0.0036\n",
            "\n",
            "Average test loss: 0.0611  Accuracy: 9833/10000 (98.33%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0085\n",
            "[12800/60000 ( 21%)]  Loss: 0.0020\n",
            "[25600/60000 ( 43%)]  Loss: 0.0878\n",
            "[38400/60000 ( 64%)]  Loss: 0.0337\n",
            "[51200/60000 ( 85%)]  Loss: 0.0226\n",
            "\n",
            "Average test loss: 0.0800  Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0113\n",
            "[12800/60000 ( 21%)]  Loss: 0.0189\n",
            "[25600/60000 ( 43%)]  Loss: 0.0426\n",
            "[38400/60000 ( 64%)]  Loss: 0.0500\n",
            "[51200/60000 ( 85%)]  Loss: 0.0528\n",
            "\n",
            "Average test loss: 0.0768  Accuracy: 9793/10000 (97.93%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0481\n",
            "[12800/60000 ( 21%)]  Loss: 0.0060\n",
            "[25600/60000 ( 43%)]  Loss: 0.0108\n",
            "[38400/60000 ( 64%)]  Loss: 0.0138\n",
            "[51200/60000 ( 85%)]  Loss: 0.0098\n",
            "\n",
            "Average test loss: 0.0592  Accuracy: 9861/10000 (98.61%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0152\n",
            "[12800/60000 ( 21%)]  Loss: 0.0060\n",
            "[25600/60000 ( 43%)]  Loss: 0.0274\n",
            "[38400/60000 ( 64%)]  Loss: 0.0189\n",
            "[51200/60000 ( 85%)]  Loss: 0.0240\n",
            "\n",
            "Average test loss: 0.0843  Accuracy: 9800/10000 (98.00%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0236\n",
            "[12800/60000 ( 21%)]  Loss: 0.0026\n",
            "[25600/60000 ( 43%)]  Loss: 0.0017\n",
            "[38400/60000 ( 64%)]  Loss: 0.0115\n",
            "[51200/60000 ( 85%)]  Loss: 0.0151\n",
            "\n",
            "Average test loss: 0.0715  Accuracy: 9818/10000 (98.18%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0048\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0010\n",
            "[51200/60000 ( 85%)]  Loss: 0.0039\n",
            "\n",
            "Average test loss: 0.0602  Accuracy: 9860/10000 (98.60%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0136\n",
            "[12800/60000 ( 21%)]  Loss: 0.0034\n",
            "[25600/60000 ( 43%)]  Loss: 0.0048\n",
            "[38400/60000 ( 64%)]  Loss: 0.0027\n",
            "[51200/60000 ( 85%)]  Loss: 0.0259\n",
            "\n",
            "Average test loss: 0.0754  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0045\n",
            "[12800/60000 ( 21%)]  Loss: 0.0038\n",
            "[25600/60000 ( 43%)]  Loss: 0.0000\n",
            "[38400/60000 ( 64%)]  Loss: 0.0265\n",
            "[51200/60000 ( 85%)]  Loss: 0.0020\n",
            "\n",
            "Average test loss: 0.0717  Accuracy: 9835/10000 (98.35%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0119\n",
            "[12800/60000 ( 21%)]  Loss: 0.0144\n",
            "[25600/60000 ( 43%)]  Loss: 0.0085\n",
            "[38400/60000 ( 64%)]  Loss: 0.0832\n",
            "[51200/60000 ( 85%)]  Loss: 0.0006\n",
            "\n",
            "Average test loss: 0.0672  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0022\n",
            "[12800/60000 ( 21%)]  Loss: 0.0067\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0021\n",
            "[51200/60000 ( 85%)]  Loss: 0.0023\n",
            "\n",
            "Average test loss: 0.0657  Accuracy: 9856/10000 (98.56%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0055\n",
            "[12800/60000 ( 21%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0049\n",
            "[38400/60000 ( 64%)]  Loss: 0.0000\n",
            "[51200/60000 ( 85%)]  Loss: 0.0009\n",
            "\n",
            "Average test loss: 0.0675  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Testing configuration: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256}\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3045\n",
            "[12800/60000 ( 21%)]  Loss: 0.6935\n",
            "[25600/60000 ( 43%)]  Loss: 0.3278\n",
            "[38400/60000 ( 64%)]  Loss: 0.2758\n",
            "[51200/60000 ( 85%)]  Loss: 0.2628\n",
            "\n",
            "Average test loss: 0.1500  Accuracy: 9502/10000 (95.02%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1178\n",
            "[12800/60000 ( 21%)]  Loss: 0.0952\n",
            "[25600/60000 ( 43%)]  Loss: 0.1811\n",
            "[38400/60000 ( 64%)]  Loss: 0.1183\n",
            "[51200/60000 ( 85%)]  Loss: 0.1587\n",
            "\n",
            "Average test loss: 0.1231  Accuracy: 9621/10000 (96.21%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1285\n",
            "[12800/60000 ( 21%)]  Loss: 0.0333\n",
            "[25600/60000 ( 43%)]  Loss: 0.0927\n",
            "[38400/60000 ( 64%)]  Loss: 0.0727\n",
            "[51200/60000 ( 85%)]  Loss: 0.0930\n",
            "\n",
            "Average test loss: 0.1110  Accuracy: 9654/10000 (96.54%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0147\n",
            "[12800/60000 ( 21%)]  Loss: 0.0161\n",
            "[25600/60000 ( 43%)]  Loss: 0.1370\n",
            "[38400/60000 ( 64%)]  Loss: 0.0488\n",
            "[51200/60000 ( 85%)]  Loss: 0.0735\n",
            "\n",
            "Average test loss: 0.1074  Accuracy: 9676/10000 (96.76%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0739\n",
            "[12800/60000 ( 21%)]  Loss: 0.0333\n",
            "[25600/60000 ( 43%)]  Loss: 0.1062\n",
            "[38400/60000 ( 64%)]  Loss: 0.1747\n",
            "[51200/60000 ( 85%)]  Loss: 0.0347\n",
            "\n",
            "Average test loss: 0.0722  Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0385\n",
            "[12800/60000 ( 21%)]  Loss: 0.1009\n",
            "[25600/60000 ( 43%)]  Loss: 0.0186\n",
            "[38400/60000 ( 64%)]  Loss: 0.0482\n",
            "[51200/60000 ( 85%)]  Loss: 0.0709\n",
            "\n",
            "Average test loss: 0.0615  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0027\n",
            "[12800/60000 ( 21%)]  Loss: 0.0288\n",
            "[25600/60000 ( 43%)]  Loss: 0.0835\n",
            "[38400/60000 ( 64%)]  Loss: 0.0443\n",
            "[51200/60000 ( 85%)]  Loss: 0.0600\n",
            "\n",
            "Average test loss: 0.0814  Accuracy: 9759/10000 (97.59%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0424\n",
            "[12800/60000 ( 21%)]  Loss: 0.0462\n",
            "[25600/60000 ( 43%)]  Loss: 0.0356\n",
            "[38400/60000 ( 64%)]  Loss: 0.0982\n",
            "[51200/60000 ( 85%)]  Loss: 0.0114\n",
            "\n",
            "Average test loss: 0.0593  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0380\n",
            "[12800/60000 ( 21%)]  Loss: 0.0586\n",
            "[25600/60000 ( 43%)]  Loss: 0.0323\n",
            "[38400/60000 ( 64%)]  Loss: 0.0093\n",
            "[51200/60000 ( 85%)]  Loss: 0.0045\n",
            "\n",
            "Average test loss: 0.0720  Accuracy: 9792/10000 (97.92%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0262\n",
            "[12800/60000 ( 21%)]  Loss: 0.0341\n",
            "[25600/60000 ( 43%)]  Loss: 0.0885\n",
            "[38400/60000 ( 64%)]  Loss: 0.0298\n",
            "[51200/60000 ( 85%)]  Loss: 0.0023\n",
            "\n",
            "Average test loss: 0.0566  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0134\n",
            "[12800/60000 ( 21%)]  Loss: 0.0178\n",
            "[25600/60000 ( 43%)]  Loss: 0.0112\n",
            "[38400/60000 ( 64%)]  Loss: 0.0373\n",
            "[51200/60000 ( 85%)]  Loss: 0.0048\n",
            "\n",
            "Average test loss: 0.0673  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0106\n",
            "[12800/60000 ( 21%)]  Loss: 0.0296\n",
            "[25600/60000 ( 43%)]  Loss: 0.0129\n",
            "[38400/60000 ( 64%)]  Loss: 0.0055\n",
            "[51200/60000 ( 85%)]  Loss: 0.0151\n",
            "\n",
            "Average test loss: 0.0824  Accuracy: 9791/10000 (97.91%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0405\n",
            "[12800/60000 ( 21%)]  Loss: 0.0013\n",
            "[25600/60000 ( 43%)]  Loss: 0.0028\n",
            "[38400/60000 ( 64%)]  Loss: 0.0008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0227\n",
            "\n",
            "Average test loss: 0.0718  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0994\n",
            "[12800/60000 ( 21%)]  Loss: 0.0124\n",
            "[25600/60000 ( 43%)]  Loss: 0.0007\n",
            "[38400/60000 ( 64%)]  Loss: 0.0026\n",
            "[51200/60000 ( 85%)]  Loss: 0.0484\n",
            "\n",
            "Average test loss: 0.0769  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0177\n",
            "[12800/60000 ( 21%)]  Loss: 0.0046\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[38400/60000 ( 64%)]  Loss: 0.0114\n",
            "[51200/60000 ( 85%)]  Loss: 0.0053\n",
            "\n",
            "Average test loss: 0.0624  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0019\n",
            "[12800/60000 ( 21%)]  Loss: 0.0014\n",
            "[25600/60000 ( 43%)]  Loss: 0.0092\n",
            "[38400/60000 ( 64%)]  Loss: 0.0093\n",
            "[51200/60000 ( 85%)]  Loss: 0.0030\n",
            "\n",
            "Average test loss: 0.0682  Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0179\n",
            "[12800/60000 ( 21%)]  Loss: 0.0017\n",
            "[25600/60000 ( 43%)]  Loss: 0.0011\n",
            "[38400/60000 ( 64%)]  Loss: 0.0005\n",
            "[51200/60000 ( 85%)]  Loss: 0.0584\n",
            "\n",
            "Average test loss: 0.0678  Accuracy: 9850/10000 (98.50%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0049\n",
            "[12800/60000 ( 21%)]  Loss: 0.0006\n",
            "[25600/60000 ( 43%)]  Loss: 0.0039\n",
            "[38400/60000 ( 64%)]  Loss: 0.0292\n",
            "[51200/60000 ( 85%)]  Loss: 0.0001\n",
            "\n",
            "Average test loss: 0.0695  Accuracy: 9847/10000 (98.47%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0001\n",
            "[12800/60000 ( 21%)]  Loss: 0.0003\n",
            "[25600/60000 ( 43%)]  Loss: 0.0191\n",
            "[38400/60000 ( 64%)]  Loss: 0.0344\n",
            "[51200/60000 ( 85%)]  Loss: 0.0168\n",
            "\n",
            "Average test loss: 0.0647  Accuracy: 9861/10000 (98.61%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0010\n",
            "[12800/60000 ( 21%)]  Loss: 0.0003\n",
            "[25600/60000 ( 43%)]  Loss: 0.0123\n",
            "[38400/60000 ( 64%)]  Loss: 0.0002\n",
            "[51200/60000 ( 85%)]  Loss: 0.0014\n",
            "\n",
            "Average test loss: 0.0582  Accuracy: 9869/10000 (98.69%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ct7GEoMS5V9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}