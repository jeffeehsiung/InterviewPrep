{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G28ynkon-QY1"
      },
      "source": [
        "#Artificial Neural Networks and Deep Learning    \n",
        "##Assignment 3.1 - Autoencoders and Stacked Autoencoders\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we will implement two network architectures from scratch: (1) Autoencoders; (2) Stacked Autoencoders.\n",
        "\n",
        "We will train both networks on the MNIST dataset under reconstruction learning task. All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZHATEBH-INR",
        "outputId": "dfefb0bb-4ad7-48e1-ddaf-77c1316a4555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnZZO_oOFpJP",
        "outputId": "761a17d8-dad6-42a8-9d08-96c45a200f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr  6 18:56:43 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504PW8Go_Bna"
      },
      "source": [
        "# Autoencoder\n",
        "In this section, we implement an Autoencoder from scratch and trainit on the MNIST dataset. With this autoencoder, we pass input data through an encoder making a compressed representation of the input, and then pass this representation to the following decoder to reconstruct the input data.\n",
        "Note that in this exercise session we will be using  ``PyTorch`` instead of ``Keras`` for building and training our neural networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGwR7aL5_FSD",
        "outputId": "01985944-6951-4da4-be8e-2509e603d9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 89801201.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 67111187.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 115295602.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8679056.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set the seed for reproducibility. Don't forget to comment out this line when averaging over different runs\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "# Select only a part of the dataset to speed up training times\n",
        "num_train_samples = 10000\n",
        "num_test_samples = 1000\n",
        "\n",
        "# Randomly select a subset of samples\n",
        "train_indices = torch.randperm(len(train_data))[:num_train_samples]\n",
        "test_indices = torch.randperm(len(test_data))[:num_test_samples]\n",
        "\n",
        "# Create subset samplers to be used in the dataloader\n",
        "train_subset_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "test_subset_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OxiS9KN9oQA"
      },
      "source": [
        "## Create training and test dataloaders\n",
        "\n",
        "\n",
        "Dataloaders are used for efficiently loading, batching, and managing datasets in PyTorch.\n",
        "During training dataloaders feed the data into the network batch by batch instead of the whole dataset at once. This allows us to handle larger datasets without running out of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lig-NUzmAuiZ"
      },
      "outputs": [],
      "source": [
        "# Choose how many samples per batch to load\n",
        "# You can tune the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler = train_subset_sampler,\n",
        "                                           num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          sampler = test_subset_sampler,\n",
        "                                          num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk4UCmCdBjjZ"
      },
      "source": [
        "## Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "3BeTdD4yBcl9",
        "outputId": "5002d454-6ac1-4a32-9542-27f9ea5b65c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff7aa9842e0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAGsCAYAAAC8WvLKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbGklEQVR4nO3db2yV9f3/8dfhTw+o7cFS2tNKwfJHWOSPkUnXqAylg9aFiLL4jxuwGYmuuGF1mi4q6ua6scUZTYd3FpiZqCMRiNwgwWJL3FocCCNkW0e7Tmpoy6zpOaVIYfTzveHP8/PQ0nIuz+nVN30+kiuh51yfXu9duZLnrvZ4NeCccwIAwJhRfg8AAIAXBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmDTG7wEu1NvbqxMnTig9PV2BQMDvcQAAQ8g5p66uLuXl5WnUqIHvsYZdwE6cOKH8/Hy/xwAA+KilpUWTJ08ecJ9h9yPE9PR0v0cAAPjsUlow7ALGjw0BAJfSgpQFrKqqStdee63GjRunwsJCffjhh6k6FABgBEpJwN5++22Vl5drw4YN+uijjzR//nwtW7ZMJ0+eTMXhAAAjkUuBhQsXurKystjX58+fd3l5ea6ysnLQtZFIxEliY2NjYxvBWyQSGbQXSb8DO3v2rA4ePKji4uLYa6NGjVJxcbHq6ur67N/T06NoNBq3AQAwmKQH7NNPP9X58+eVk5MT93pOTo7a2tr67F9ZWalQKBTb+Ag9AOBS+P4pxIqKCkUikdjW0tLi90gAAAOS/h8yZ2VlafTo0Wpvb497vb29XeFwuM/+wWBQwWAw2WMAAC5zSb8DS0tL04IFC1RdXR17rbe3V9XV1SoqKkr24QAAI1RKHiVVXl6u1atX65vf/KYWLlyol19+Wd3d3fr+97+fisMBAEaglATs3nvv1X//+189++yzamtr0w033KDdu3f3+WAHAABeBZxzzu8hvioajSoUCvk9BgDAR5FIRBkZGQPu4/unEAEA8IKAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAExKesCee+45BQKBuG327NnJPgwAYIQbk4pvev311+u99977/wcZk5LDAABGsJSUZcyYMQqHw6n41gAASErR78COHTumvLw8TZs2TatWrdLx48cvum9PT4+i0WjcBgDAYJIesMLCQm3ZskW7d+/Wpk2b1NzcrFtvvVVdXV397l9ZWalQKBTb8vPzkz0SAOAyFHDOuVQeoLOzU1OnTtVLL72kBx98sM/7PT096unpiX0djUaJGACMcJFIRBkZGQPuk/JPV0yYMEHXXXedGhsb+30/GAwqGAymegwAwGUm5f8d2KlTp9TU1KTc3NxUHwoAMIIkPWBPPPGEamtr9Z///Ed/+ctfdNddd2n06NG6//77k30oAMAIlvQfIX7yySe6//771dHRoUmTJumWW25RfX29Jk2alOxDAQBGsJR/iCNR0WhUoVDI7zFwmXr88cc9rSsrK/O0btq0aZ7WWTBjxoyE1/ztb3/zdKw77rjD07ra2lpP6+C/S/kQB89CBACYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYlPK/yAwMJy+++KKndWfOnEnyJPbdfvvtCa8ZP368p2PNmTPH0zqeRn954w4MAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGAST6MH4MnMmTOH7Fgff/zxkB0LdnAHBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiafRw6TJkyd7Wjd69GhP6/bs2eNpnQVXXHGFp3W33XZbkie5uK6uriE7FuzgDgwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJPMwXJn3ve9/ztM7rw3ydc57WWTBv3jxP62688cYkTwIkhjswAIBJBAwAYBIBAwCYlHDA9u3bp+XLlysvL0+BQEA7duyIe985p2effVa5ubkaP368iouLdezYsWTNCwCAJA8B6+7u1vz581VVVdXv+xs3btQrr7yi1157Tfv379eVV16pZcuW6cyZM197WAAAvpTwpxBLS0tVWlra73vOOb388st6+umndeedd0qSXn/9deXk5GjHjh267777vt60AAD8P0n9HVhzc7Pa2tpUXFwcey0UCqmwsFB1dXX9runp6VE0Go3bAAAYTFID1tbWJknKycmJez0nJyf23oUqKysVCoViW35+fjJHAgBcpnz/FGJFRYUikUhsa2lp8XskAIABSQ1YOByWJLW3t8e93t7eHnvvQsFgUBkZGXEbAACDSWrACgoKFA6HVV1dHXstGo1q//79KioqSuahAAAjXMKfQjx16pQaGxtjXzc3N+vw4cPKzMzUlClTtH79ev385z/XzJkzVVBQoGeeeUZ5eXlasWJFMucGAIxwCQfswIEDuu2222Jfl5eXS5JWr16tLVu26Mknn1R3d7fWrl2rzs5O3XLLLdq9e7fGjRuXvKkBACNewgFbvHjxgE/mDgQCeuGFF/TCCy98rcGAgaxatcrTukAg4GndzJkzPa2zoLOz09O68+fPJ7xmzBhvfwAjMzPT0zpc3nz/FCIAAF4QMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACZ5ezQ0YNRAf0lhIDNmzPC0buLEiQmv6ejo8HQsr778k0iJGj16dMJrPvvsM0/H2rlzp6d1uLxxBwYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImn0cOkvXv3elq3YMECT+uuvPJKT+v27NmT8Jp77rnH07FaWlo8rZs7d66ndV588MEHntb19vYmeRJcDrgDAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIP84VJL774oqd1y5cv97Ru9uzZntbdcMMNCa/517/+5elYx48f97RuypQpntZ58de//nXIjoXLH3dgAACTCBgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTAs455/cQXxWNRhUKhfweA5epGTNmeFq3atUqT+t+9KMfJbzm6quv9nSsoXbq1KmE18yaNcvTsVpbWz2tg12RSEQZGRkD7sMdGADAJAIGADCJgAEATEo4YPv27dPy5cuVl5enQCCgHTt2xL2/Zs0aBQKBuK2kpCRZ8wIAIMlDwLq7uzV//nxVVVVddJ+SkhK1trbGtjfffPNrDQkAwIXGJLqgtLRUpaWlA+4TDAYVDocv6fv19PSop6cn9nU0Gk10JADACJSS34HV1NQoOztbs2bN0iOPPKKOjo6L7ltZWalQKBTb8vPzUzESAOAyk/SAlZSU6PXXX1d1dbV+9atfqba2VqWlpTp//ny/+1dUVCgSicS2lpaWZI8EALgMJfwjxMHcd999sX/PnTtX8+bN0/Tp01VTU6MlS5b02T8YDCoYDCZ7DADAZS7lH6OfNm2asrKy1NjYmOpDAQBGkJQH7JNPPlFHR4dyc3NTfSgAwAiS8I8QT506FXc31dzcrMOHDyszM1OZmZl6/vnntXLlSoXDYTU1NenJJ5/UjBkztGzZsqQODgAY2RIO2IEDB3TbbbfFvi4vL5ckrV69Wps2bdKRI0f0hz/8QZ2dncrLy9PSpUv1s5/9jN9zAQCSiqfRAymUmZmZ8BqvT6Pftm2bp3U33HCDp3X//ve/E17j9a8BYOThafQAgMsWAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGBSwn9OBcCl++yzzxJec/r0aU/HmjRpkqd1Xv3mN78Z0uMBF+IODABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEk8zBcYZr773e96WnfNNdckeZKBbd26dUiPB1yIOzAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEk8jR4YZlasWDGkxzt06JCndadOnUryJEBiuAMDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJjE0+iBFJozZ07Ca0pKSlIwycW9+uqrntb19vYmeRIgMdyBAQBMImAAAJMSClhlZaVuuukmpaenKzs7WytWrFBDQ0PcPmfOnFFZWZkmTpyoq666SitXrlR7e3tShwYAIKGA1dbWqqysTPX19dqzZ4/OnTunpUuXqru7O7bPY489pnfffVfbtm1TbW2tTpw4obvvvjvpgwMARraEPsSxe/fuuK+3bNmi7OxsHTx4UIsWLVIkEtHvf/97bd26VbfffrskafPmzfrGN76h+vp6fetb30re5ACAEe1r/Q4sEolIkjIzMyVJBw8e1Llz51RcXBzbZ/bs2ZoyZYrq6ur6/R49PT2KRqNxGwAAg/EcsN7eXq1fv14333xz7KPCbW1tSktL04QJE+L2zcnJUVtbW7/fp7KyUqFQKLbl5+d7HQkAMIJ4DlhZWZmOHj2qt95662sNUFFRoUgkEttaWlq+1vcDAIwMnv5D5nXr1mnXrl3at2+fJk+eHHs9HA7r7Nmz6uzsjLsLa29vVzgc7vd7BYNBBYNBL2MAAEawhO7AnHNat26dtm/frr1796qgoCDu/QULFmjs2LGqrq6OvdbQ0KDjx4+rqKgoORMDAKAE78DKysq0detW7dy5U+np6bHfa4VCIY0fP16hUEgPPvigysvLlZmZqYyMDD366KMqKiriE4gAgKRKKGCbNm2SJC1evDju9c2bN2vNmjWSpN/+9rcaNWqUVq5cqZ6eHi1btky/+93vkjIsAABfSihgzrlB9xk3bpyqqqpUVVXleSgAAAbD0+iBFLr++usTXjNx4kRPx2psbPS07o9//KOndYDfeJgvAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCQCBgAwiYABAEwiYAAAk3iYL3AJxo4d62ndD37wgyRPcnG/+MUvPK373//+l+RJgKHBHRgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCSeRg9cgpycHE/rvvOd7yS8pqWlxdOx3n33XU/rAKu4AwMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmMTT6IFLsGHDhiE71q5duzyt6+joSPIkwPDGHRgAwCQCBgAwiYABAEwiYAAAkwgYAMAkAgYAMImAAQBMImAAAJMIGADAJAIGADCJgAEATCJgAACTCBgAwCSeRo8RJTs729O6e+65J8mTXFx9ff2QHQuwjDswAIBJBAwAYFJCAausrNRNN92k9PR0ZWdna8WKFWpoaIjbZ/HixQoEAnHbww8/nNShAQBIKGC1tbUqKytTfX299uzZo3Pnzmnp0qXq7u6O2++hhx5Sa2trbNu4cWNShwYAIKEPcezevTvu6y1btig7O1sHDx7UokWLYq9fccUVCofDyZkQAIB+fK3fgUUiEUlSZmZm3OtvvPGGsrKyNGfOHFVUVOj06dMX/R49PT2KRqNxGwAAg/H8Mfre3l6tX79eN998s+bMmRN7/YEHHtDUqVOVl5enI0eO6KmnnlJDQ4Peeeedfr9PZWWlnn/+ea9jAABGKM8BKysr09GjR/XBBx/Evb527drYv+fOnavc3FwtWbJETU1Nmj59ep/vU1FRofLy8tjX0WhU+fn5XscCAIwQngK2bt067dq1S/v27dPkyZMH3LewsFCS1NjY2G/AgsGggsGglzEAACNYQgFzzunRRx/V9u3bVVNTo4KCgkHXHD58WJKUm5vraUAAAPqTUMDKysq0detW7dy5U+np6Wpra5MkhUIhjR8/Xk1NTdq6davuuOMOTZw4UUeOHNFjjz2mRYsWad68eSn5HwAAGJkSCtimTZskffEfK3/V5s2btWbNGqWlpem9997Tyy+/rO7ubuXn52vlypV6+umnkzYwAACSFHDOOb+H+KpoNKpQKOT3GAAAH0UiEWVkZAy4D89CBACYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmETAAAAmETAAgEkEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYNu4A55/weAQDgs0tpwbALWFdXl98jAAB8diktCLhhdsvT29urEydOKD09XYFAIO69aDSq/Px8tbS0KCMjw6cJhxfOSV+ck3icj744J30Nl3PinFNXV5fy8vI0atTA91hjhmimSzZq1ChNnjx5wH0yMjK46C7AOemLcxKP89EX56Sv4XBOQqHQJe037H6ECADApSBgAACTTAUsGAxqw4YNCgaDfo8ybHBO+uKcxON89MU56cviORl2H+IAAOBSmLoDAwDgSwQMAGASAQMAmETAAAAmETAAgEmmAlZVVaVrr71W48aNU2FhoT788EO/R/LNc889p0AgELfNnj3b77GGzL59+7R8+XLl5eUpEAhox44dce875/Tss88qNzdX48ePV3FxsY4dO+bPsENksHOyZs2aPtdMSUmJP8MOgcrKSt10001KT09Xdna2VqxYoYaGhrh9zpw5o7KyMk2cOFFXXXWVVq5cqfb2dp8mTr1LOSeLFy/uc508/PDDPk08MDMBe/vtt1VeXq4NGzboo48+0vz587Vs2TKdPHnS79F8c/3116u1tTW2ffDBB36PNGS6u7s1f/58VVVV9fv+xo0b9corr+i1117T/v37deWVV2rZsmU6c+bMEE86dAY7J5JUUlISd828+eabQzjh0KqtrVVZWZnq6+u1Z88enTt3TkuXLlV3d3dsn8cee0zvvvuutm3bptraWp04cUJ33323j1On1qWcE0l66KGH4q6TjRs3+jTxIJwRCxcudGVlZbGvz58/7/Ly8lxlZaWPU/lnw4YNbv78+X6PMSxIctu3b4993dvb68LhsPv1r38de62zs9MFg0H35ptv+jDh0LvwnDjn3OrVq92dd97pyzzDwcmTJ50kV1tb65z74poYO3as27ZtW2yff/zjH06Sq6ur82vMIXXhOXHOuW9/+9vuxz/+sX9DJcDEHdjZs2d18OBBFRcXx14bNWqUiouLVVdX5+Nk/jp27Jjy8vI0bdo0rVq1SsePH/d7pGGhublZbW1tcddLKBRSYWHhiL5eJKmmpkbZ2dmaNWuWHnnkEXV0dPg90pCJRCKSpMzMTEnSwYMHde7cubjrZPbs2ZoyZcqIuU4uPCdfeuONN5SVlaU5c+aooqJCp0+f9mO8QQ27p9H359NPP9X58+eVk5MT93pOTo7++c9/+jSVvwoLC7VlyxbNmjVLra2tev7553Xrrbfq6NGjSk9P93s8X7W1tUlSv9fLl++NRCUlJbr77rtVUFCgpqYm/fSnP1Vpaanq6uo0evRov8dLqd7eXq1fv14333yz5syZI+mL6yQtLU0TJkyI23ekXCf9nRNJeuCBBzR16lTl5eXpyJEjeuqpp9TQ0KB33nnHx2n7ZyJg6Ku0tDT273nz5qmwsFBTp07Vn/70Jz344IM+Tobh6r777ov9e+7cuZo3b56mT5+umpoaLVmyxMfJUq+srExHjx4dUb8nHszFzsnatWtj/547d65yc3O1ZMkSNTU1afr06UM95oBM/AgxKytLo0eP7vPpoPb2doXDYZ+mGl4mTJig6667To2NjX6P4rsvrwmul4FNmzZNWVlZl/01s27dOu3atUvvv/9+3N8aDIfDOnv2rDo7O+P2HwnXycXOSX8KCwslaVheJyYClpaWpgULFqi6ujr2Wm9vr6qrq1VUVOTjZMPHqVOn1NTUpNzcXL9H8V1BQYHC4XDc9RKNRrV//36ul6/45JNP1NHRcdleM845rVu3Ttu3b9fevXtVUFAQ9/6CBQs0duzYuOukoaFBx48fv2yvk8HOSX8OHz4sScPzOvH7UySX6q233nLBYNBt2bLF/f3vf3dr1651EyZMcG1tbX6P5ovHH3/c1dTUuObmZvfnP//ZFRcXu6ysLHfy5Em/RxsSXV1d7tChQ+7QoUNOknvppZfcoUOH3Mcff+ycc+6Xv/ylmzBhgtu5c6c7cuSIu/POO11BQYH7/PPPfZ48dQY6J11dXe6JJ55wdXV1rrm52b333nvuxhtvdDNnznRnzpzxe/SUeOSRR1woFHI1NTWutbU1tp0+fTq2z8MPP+ymTJni9u7d6w4cOOCKiopcUVGRj1On1mDnpLGx0b3wwgvuwIEDrrm52e3cudNNmzbNLVq0yOfJ+2cmYM459+qrr7opU6a4tLQ0t3DhQldfX+/3SL659957XW5urktLS3PXXHONu/fee11jY6PfYw2Z999/30nqs61evdo598VH6Z955hmXk5PjgsGgW7JkiWtoaPB36BQb6JycPn3aLV261E2aNMmNHTvWTZ061T300EOX9f8B7O9cSHKbN2+O7fP555+7H/7wh+7qq692V1xxhbvrrrtca2urf0On2GDn5Pjx427RokUuMzPTBYNBN2PGDPeTn/zERSIRfwe/CP4eGADAJBO/AwMA4EIEDABgEgEDAJhEwAAAJhEwAIBJBAwAYBIBAwCYRMAAACYRMACASQQMAGASAQMAmPR/8powFPqHltgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Obtain one batch of training images\n",
        "images, labels = next(iter(train_loader)) # iter() creates an iterator over the batches in the dataloader and next() selects the next batch from this iterator.\n",
        "images = images.numpy() #.numpy() turns the torch.tensor into a numpy array\n",
        "\n",
        "# Get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (5,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzWxOoXYCUiz"
      },
      "source": [
        "## Build the Linear Autoencoder\n",
        "We now train a linear autoencoder on MINIST dataset.\n",
        "Images of original size 28$\\times$28 will be flattened into 784-dimensional vectors.\n",
        "Note that images from this dataset are already normalized so that the values are between 0 and 1.\n",
        "Since the images are normalized between 0 and 1, we need to use a sigmoid activation on the output layer to get values that match this input value range.\n",
        "\n",
        "The encoder and decoder in the Autoencoder are built with one linear layer where you can tune the dimension of the hidden representation, i.e., ``encoding_dim``, to obtain models with different size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "05JucJsoB6E-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the NN architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Linear(784, encoding_dim)\n",
        "        # Decoder\n",
        "        self.decoder = nn.Linear(encoding_dim, 784)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define feedforward behavior\n",
        "        # and scale the *output* layer with a sigmoid activation function\n",
        "\n",
        "        # Pass x into encoder\n",
        "        out = F.relu(self.encoder(x))\n",
        "        # Pass out into decoder\n",
        "        out = torch.sigmoid(self.decoder(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtBYi-kACg1A",
        "outputId": "090da340-fd3f-4a9a-cc93-0c5003e48376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autoencoder(\n",
            "  (encoder): Linear(in_features=784, out_features=32, bias=True)\n",
            "  (decoder): Linear(in_features=32, out_features=784, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the NN\n",
        "# You can change the encoding_dim to obtain models with different size\n",
        "encoding_dim = 32\n",
        "model = Autoencoder(encoding_dim)\n",
        "\n",
        "# Send model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oAhmIwEDgo"
      },
      "source": [
        "## Training on MNIST\n",
        "Training a neural network in PyTorch involves manually defining the training loop.\n",
        "Here data processing, forward pass, loss computation, and optimization are explicitly specified.\n",
        "\n",
        "Since we work on reconstruction learning tasks, we do not need the labels here but only the images.\n",
        "The loss function should choose the MSE loss for reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssUxyRJSEUzV",
        "outputId": "bf9ca326-777f-47ed-a87b-e905dcd7d7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tTraining Loss: 2.386593\n",
            "Epoch: 1 \tTraining Loss: 1.315528\n",
            "Epoch: 2 \tTraining Loss: 1.040679\n",
            "Epoch: 3 \tTraining Loss: 0.878057\n",
            "Epoch: 4 \tTraining Loss: 0.759522\n",
            "Epoch: 5 \tTraining Loss: 0.688439\n",
            "Epoch: 6 \tTraining Loss: 0.636693\n",
            "Epoch: 7 \tTraining Loss: 0.597742\n",
            "Epoch: 8 \tTraining Loss: 0.568476\n",
            "Epoch: 9 \tTraining Loss: 0.546332\n",
            "Epoch: 10 \tTraining Loss: 0.530176\n",
            "Epoch: 11 \tTraining Loss: 0.518377\n",
            "Epoch: 12 \tTraining Loss: 0.509132\n",
            "Epoch: 13 \tTraining Loss: 0.501971\n",
            "Epoch: 14 \tTraining Loss: 0.495893\n",
            "Epoch: 15 \tTraining Loss: 0.491787\n",
            "Epoch: 16 \tTraining Loss: 0.488524\n",
            "Epoch: 17 \tTraining Loss: 0.485783\n",
            "Epoch: 18 \tTraining Loss: 0.483460\n",
            "Epoch: 19 \tTraining Loss: 0.480828\n"
          ]
        }
      ],
      "source": [
        "# Specify the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# We use Adam as the optimizer with a fixed learning rate of 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Choose the number of Epochs to train the network\n",
        "n_epochs = 20\n",
        "# Set model to training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Monitor training loss\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Train the model #\n",
        "    # Feed the data into the network batch by batch using the dataloader\n",
        "    for batch_data in train_loader:\n",
        "        # _ stands in for labels\n",
        "        # we do not need labels when conducting reconstruction\n",
        "        images, _ = batch_data\n",
        "        # Flatten images and send images to GPU\n",
        "        images = images.view(images.size(0), -1)\n",
        "        # Send images to GPU if possible\n",
        "        if torch.cuda.is_available():\n",
        "          images = images.cuda()\n",
        "        # Clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(images)\n",
        "        # Calculate the loss between output and input images\n",
        "        loss = criterion(outputs, images)\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # Update running training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "\n",
        "    # Print avg training statistics\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch,\n",
        "        train_loss\n",
        "        ))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeN_f28fJ1lq"
      },
      "source": [
        "## Evaluation on test set\n",
        "We now evaluate the reconstruction results on the unseen test set.\n",
        "We plot the original test images and their corresponding reconstruction ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daly1tTfbbXk",
        "outputId": "a9787109-7a2d-47ce-afcc-98caee549b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.001470\n"
          ]
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Monitor test loss\n",
        "test_loss = 0.0\n",
        "\n",
        "# Disable gradient computation\n",
        "with torch.no_grad():\n",
        "    # Iterate over the test data\n",
        "    for batch_data in test_loader:\n",
        "        # Extract images from the batch\n",
        "        images, _ = batch_data\n",
        "        # Flatten images and send them to GPU\n",
        "        images = images.view(images.size(0), -1)\n",
        "        # Send to GPU\n",
        "        if torch.cuda.is_available():\n",
        "          images = images.cuda()\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(images)\n",
        "        # Calculate the loss between output and input images\n",
        "        loss = criterion(outputs, images)\n",
        "        # Update test loss\n",
        "        test_loss += loss.item()*images.size(0)\n",
        "\n",
        "# Compute average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "\n",
        "# Print test loss\n",
        "print('Test Loss: {:.6f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "gemBn0ihG2Yo",
        "outputId": "466045af-788e-42b2-b8a1-c28d04649366"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2500x400 with 20 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Obtain one batch of test images\n",
        "test_images, test_labels = next(iter(test_loader))\n",
        "\n",
        "test_images_flatten = test_images.view(test_images.size(0), -1)\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "# Send model back to CPU\n",
        "model.cpu()\n",
        "# Get sample outputs\n",
        "output = model(test_images_flatten)\n",
        "# Prep images for display\n",
        "test_images = test_images.numpy()\n",
        "\n",
        "# Output is resized into a batch of images\n",
        "output = output.view(batch_size, 1, 28, 28)\n",
        "# Use detach when it's an output that requires_grad\n",
        "output = output.detach().numpy()\n",
        "\n",
        "# Plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# Input images on top row, reconstructions on bottom\n",
        "for test_images, row in zip([test_images, output], axes):\n",
        "    for img, ax in zip(test_images, row):\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h38xuHpmBoIW"
      },
      "source": [
        "## Grid Search for HyperParameters Tuning with Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIIayQlGBmPG",
        "outputId": "1021dfdf-76ad-40ec-d471-ed83f98b2418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 82512446.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 32191255.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 23471786.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4558633.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "batch size under test 16\n",
            "encoding dimension under test 32\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.035330\n",
            "Epoch: 1 \tTraining Loss: 0.020787\n",
            "Epoch: 2 \tTraining Loss: 0.019291\n",
            "Epoch: 3 \tTraining Loss: 0.018910\n",
            "Epoch: 4 \tTraining Loss: 0.018727\n",
            "Epoch: 5 \tTraining Loss: 0.018607\n",
            "Epoch: 6 \tTraining Loss: 0.018512\n",
            "Epoch: 7 \tTraining Loss: 0.018434\n",
            "Epoch: 8 \tTraining Loss: 0.018368\n",
            "Epoch: 9 \tTraining Loss: 0.018313\n",
            "Epoch: 10 \tTraining Loss: 0.018269\n",
            "Epoch: 11 \tTraining Loss: 0.018225\n",
            "Epoch: 12 \tTraining Loss: 0.018184\n",
            "Epoch: 13 \tTraining Loss: 0.018152\n",
            "Epoch: 14 \tTraining Loss: 0.018117\n",
            "Epoch: 15 \tTraining Loss: 0.018088\n",
            "Epoch: 16 \tTraining Loss: 0.018065\n",
            "Epoch: 17 \tTraining Loss: 0.018044\n",
            "Epoch: 18 \tTraining Loss: 0.018013\n",
            "Epoch: 19 \tTraining Loss: 0.017990\n",
            "Epoch: 20 \tTraining Loss: 0.017976\n",
            "Epoch: 21 \tTraining Loss: 0.017955\n",
            "Epoch: 22 \tTraining Loss: 0.017944\n",
            "Epoch: 23 \tTraining Loss: 0.017929\n",
            "Epoch: 24 \tTraining Loss: 0.017910\n",
            "Epoch: 25 \tTraining Loss: 0.017899\n",
            "Epoch: 26 \tTraining Loss: 0.017890\n",
            "Epoch: 27 \tTraining Loss: 0.017876\n",
            "Epoch: 28 \tTraining Loss: 0.017865\n",
            "Epoch: 29 \tTraining Loss: 0.017857\n",
            "Epoch: 30 \tTraining Loss: 0.017849\n",
            "Epoch: 31 \tTraining Loss: 0.017836\n",
            "Epoch: 32 \tTraining Loss: 0.017829\n",
            "Epoch: 33 \tTraining Loss: 0.017819\n",
            "Epoch: 34 \tTraining Loss: 0.017808\n",
            "Epoch: 35 \tTraining Loss: 0.017798\n",
            "Epoch: 36 \tTraining Loss: 0.017793\n",
            "Epoch: 37 \tTraining Loss: 0.017785\n",
            "Epoch: 38 \tTraining Loss: 0.017774\n",
            "Epoch: 39 \tTraining Loss: 0.017770\n",
            "average validation loss: 0.017948554020375013\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.032153\n",
            "Epoch: 1 \tTraining Loss: 0.016390\n",
            "Epoch: 2 \tTraining Loss: 0.014000\n",
            "Epoch: 3 \tTraining Loss: 0.013611\n",
            "Epoch: 4 \tTraining Loss: 0.013462\n",
            "Epoch: 5 \tTraining Loss: 0.013374\n",
            "Epoch: 6 \tTraining Loss: 0.013303\n",
            "Epoch: 7 \tTraining Loss: 0.013255\n",
            "Epoch: 8 \tTraining Loss: 0.013210\n",
            "Epoch: 9 \tTraining Loss: 0.013175\n",
            "Epoch: 10 \tTraining Loss: 0.013140\n",
            "Epoch: 11 \tTraining Loss: 0.013114\n",
            "Epoch: 12 \tTraining Loss: 0.013089\n",
            "Epoch: 13 \tTraining Loss: 0.013067\n",
            "Epoch: 14 \tTraining Loss: 0.013051\n",
            "Epoch: 15 \tTraining Loss: 0.013030\n",
            "Epoch: 16 \tTraining Loss: 0.013008\n",
            "Epoch: 17 \tTraining Loss: 0.012992\n",
            "Epoch: 18 \tTraining Loss: 0.012975\n",
            "Epoch: 19 \tTraining Loss: 0.012951\n",
            "Epoch: 20 \tTraining Loss: 0.012940\n",
            "Epoch: 21 \tTraining Loss: 0.012926\n",
            "Epoch: 22 \tTraining Loss: 0.012910\n",
            "Epoch: 23 \tTraining Loss: 0.012890\n",
            "Epoch: 24 \tTraining Loss: 0.012879\n",
            "Epoch: 25 \tTraining Loss: 0.012863\n",
            "Epoch: 26 \tTraining Loss: 0.012849\n",
            "Epoch: 27 \tTraining Loss: 0.012835\n",
            "Epoch: 28 \tTraining Loss: 0.012825\n",
            "Epoch: 29 \tTraining Loss: 0.012811\n",
            "Epoch: 30 \tTraining Loss: 0.012800\n",
            "Epoch: 31 \tTraining Loss: 0.012793\n",
            "Epoch: 32 \tTraining Loss: 0.012783\n",
            "Epoch: 33 \tTraining Loss: 0.012766\n",
            "Epoch: 34 \tTraining Loss: 0.012762\n",
            "Epoch: 35 \tTraining Loss: 0.012754\n",
            "Epoch: 36 \tTraining Loss: 0.012746\n",
            "Epoch: 37 \tTraining Loss: 0.012732\n",
            "Epoch: 38 \tTraining Loss: 0.012724\n",
            "Epoch: 39 \tTraining Loss: 0.012715\n",
            "Epoch: 40 \tTraining Loss: 0.012713\n",
            "Epoch: 41 \tTraining Loss: 0.012700\n",
            "Epoch: 42 \tTraining Loss: 0.012696\n",
            "Epoch: 43 \tTraining Loss: 0.012688\n",
            "Epoch: 44 \tTraining Loss: 0.012681\n",
            "Epoch: 45 \tTraining Loss: 0.012675\n",
            "Epoch: 46 \tTraining Loss: 0.012668\n",
            "Epoch: 47 \tTraining Loss: 0.012664\n",
            "Epoch: 48 \tTraining Loss: 0.012657\n",
            "Epoch: 49 \tTraining Loss: 0.012651\n",
            "Epoch: 50 \tTraining Loss: 0.012647\n",
            "Epoch: 51 \tTraining Loss: 0.012639\n",
            "Epoch: 52 \tTraining Loss: 0.012634\n",
            "Epoch: 53 \tTraining Loss: 0.012635\n",
            "Epoch: 54 \tTraining Loss: 0.012624\n",
            "Epoch: 55 \tTraining Loss: 0.012618\n",
            "Epoch: 56 \tTraining Loss: 0.012615\n",
            "Epoch: 57 \tTraining Loss: 0.012611\n",
            "Epoch: 58 \tTraining Loss: 0.012608\n",
            "Epoch: 59 \tTraining Loss: 0.012600\n",
            "average validation loss: 0.012706649725635847\n",
            "encoding dimension under test 64\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.022077\n",
            "Epoch: 1 \tTraining Loss: 0.007112\n",
            "Epoch: 2 \tTraining Loss: 0.005796\n",
            "Epoch: 3 \tTraining Loss: 0.005357\n",
            "Epoch: 4 \tTraining Loss: 0.005185\n",
            "Epoch: 5 \tTraining Loss: 0.005079\n",
            "Epoch: 6 \tTraining Loss: 0.005005\n",
            "Epoch: 7 \tTraining Loss: 0.004952\n",
            "Epoch: 8 \tTraining Loss: 0.004916\n",
            "Epoch: 9 \tTraining Loss: 0.004879\n",
            "Epoch: 10 \tTraining Loss: 0.004856\n",
            "Epoch: 11 \tTraining Loss: 0.004833\n",
            "Epoch: 12 \tTraining Loss: 0.004813\n",
            "Epoch: 13 \tTraining Loss: 0.004798\n",
            "Epoch: 14 \tTraining Loss: 0.004787\n",
            "Epoch: 15 \tTraining Loss: 0.004772\n",
            "Epoch: 16 \tTraining Loss: 0.004766\n",
            "Epoch: 17 \tTraining Loss: 0.004747\n",
            "Epoch: 18 \tTraining Loss: 0.004742\n",
            "Epoch: 19 \tTraining Loss: 0.004733\n",
            "Epoch: 20 \tTraining Loss: 0.004725\n",
            "Epoch: 21 \tTraining Loss: 0.004716\n",
            "Epoch: 22 \tTraining Loss: 0.004710\n",
            "Epoch: 23 \tTraining Loss: 0.004704\n",
            "Epoch: 24 \tTraining Loss: 0.004701\n",
            "Epoch: 25 \tTraining Loss: 0.004693\n",
            "Epoch: 26 \tTraining Loss: 0.004689\n",
            "Epoch: 27 \tTraining Loss: 0.004684\n",
            "Epoch: 28 \tTraining Loss: 0.004679\n",
            "Epoch: 29 \tTraining Loss: 0.004677\n",
            "Epoch: 30 \tTraining Loss: 0.004672\n",
            "Epoch: 31 \tTraining Loss: 0.004667\n",
            "Epoch: 32 \tTraining Loss: 0.004663\n",
            "Epoch: 33 \tTraining Loss: 0.004661\n",
            "Epoch: 34 \tTraining Loss: 0.004660\n",
            "Epoch: 35 \tTraining Loss: 0.004655\n",
            "Epoch: 36 \tTraining Loss: 0.004652\n",
            "Epoch: 37 \tTraining Loss: 0.004649\n",
            "Epoch: 38 \tTraining Loss: 0.004645\n",
            "Epoch: 39 \tTraining Loss: 0.004644\n",
            "average validation loss: 0.004797367513800661\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.022512\n",
            "Epoch: 1 \tTraining Loss: 0.007566\n",
            "Epoch: 2 \tTraining Loss: 0.006402\n",
            "Epoch: 3 \tTraining Loss: 0.006083\n",
            "Epoch: 4 \tTraining Loss: 0.005923\n",
            "Epoch: 5 \tTraining Loss: 0.005796\n",
            "Epoch: 6 \tTraining Loss: 0.005717\n",
            "Epoch: 7 \tTraining Loss: 0.005651\n",
            "Epoch: 8 \tTraining Loss: 0.005602\n",
            "Epoch: 9 \tTraining Loss: 0.005556\n",
            "Epoch: 10 \tTraining Loss: 0.005531\n",
            "Epoch: 11 \tTraining Loss: 0.005501\n",
            "Epoch: 12 \tTraining Loss: 0.005482\n",
            "Epoch: 13 \tTraining Loss: 0.005464\n",
            "Epoch: 14 \tTraining Loss: 0.005444\n",
            "Epoch: 15 \tTraining Loss: 0.005432\n",
            "Epoch: 16 \tTraining Loss: 0.005415\n",
            "Epoch: 17 \tTraining Loss: 0.005407\n",
            "Epoch: 18 \tTraining Loss: 0.005395\n",
            "Epoch: 19 \tTraining Loss: 0.005388\n",
            "Epoch: 20 \tTraining Loss: 0.005379\n",
            "Epoch: 21 \tTraining Loss: 0.005370\n",
            "Epoch: 22 \tTraining Loss: 0.005366\n",
            "Epoch: 23 \tTraining Loss: 0.005359\n",
            "Epoch: 24 \tTraining Loss: 0.005352\n",
            "Epoch: 25 \tTraining Loss: 0.005346\n",
            "Epoch: 26 \tTraining Loss: 0.005340\n",
            "Epoch: 27 \tTraining Loss: 0.005332\n",
            "Epoch: 28 \tTraining Loss: 0.005329\n",
            "Epoch: 29 \tTraining Loss: 0.005327\n",
            "Epoch: 30 \tTraining Loss: 0.005323\n",
            "Epoch: 31 \tTraining Loss: 0.005322\n",
            "Epoch: 32 \tTraining Loss: 0.005316\n",
            "Epoch: 33 \tTraining Loss: 0.005310\n",
            "Epoch: 34 \tTraining Loss: 0.005308\n",
            "Epoch: 35 \tTraining Loss: 0.005304\n",
            "Epoch: 36 \tTraining Loss: 0.005303\n",
            "Epoch: 37 \tTraining Loss: 0.005298\n",
            "Epoch: 38 \tTraining Loss: 0.005293\n",
            "Epoch: 39 \tTraining Loss: 0.005295\n",
            "Epoch: 40 \tTraining Loss: 0.005291\n",
            "Epoch: 41 \tTraining Loss: 0.005288\n",
            "Epoch: 42 \tTraining Loss: 0.005287\n",
            "Epoch: 43 \tTraining Loss: 0.005283\n",
            "Epoch: 44 \tTraining Loss: 0.005279\n",
            "Epoch: 45 \tTraining Loss: 0.005275\n",
            "Epoch: 46 \tTraining Loss: 0.005273\n",
            "Epoch: 47 \tTraining Loss: 0.005271\n",
            "Epoch: 48 \tTraining Loss: 0.005271\n",
            "Epoch: 49 \tTraining Loss: 0.005269\n",
            "Epoch: 50 \tTraining Loss: 0.005266\n",
            "Epoch: 51 \tTraining Loss: 0.005262\n",
            "Epoch: 52 \tTraining Loss: 0.005262\n",
            "Epoch: 53 \tTraining Loss: 0.005259\n",
            "Epoch: 54 \tTraining Loss: 0.005260\n",
            "Epoch: 55 \tTraining Loss: 0.005255\n",
            "Epoch: 56 \tTraining Loss: 0.005255\n",
            "Epoch: 57 \tTraining Loss: 0.005252\n",
            "Epoch: 58 \tTraining Loss: 0.005251\n",
            "Epoch: 59 \tTraining Loss: 0.005251\n",
            "average validation loss: 0.005407857708943387\n",
            "encoding dimension under test 128\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.015765\n",
            "Epoch: 1 \tTraining Loss: 0.003883\n",
            "Epoch: 2 \tTraining Loss: 0.002875\n",
            "Epoch: 3 \tTraining Loss: 0.002495\n",
            "Epoch: 4 \tTraining Loss: 0.002282\n",
            "Epoch: 5 \tTraining Loss: 0.002148\n",
            "Epoch: 6 \tTraining Loss: 0.002058\n",
            "Epoch: 7 \tTraining Loss: 0.001995\n",
            "Epoch: 8 \tTraining Loss: 0.001949\n",
            "Epoch: 9 \tTraining Loss: 0.001917\n",
            "Epoch: 10 \tTraining Loss: 0.001887\n",
            "Epoch: 11 \tTraining Loss: 0.001865\n",
            "Epoch: 12 \tTraining Loss: 0.001851\n",
            "Epoch: 13 \tTraining Loss: 0.001832\n",
            "Epoch: 14 \tTraining Loss: 0.001817\n",
            "Epoch: 15 \tTraining Loss: 0.001805\n",
            "Epoch: 16 \tTraining Loss: 0.001796\n",
            "Epoch: 17 \tTraining Loss: 0.001784\n",
            "Epoch: 18 \tTraining Loss: 0.001776\n",
            "Epoch: 19 \tTraining Loss: 0.001767\n",
            "Epoch: 20 \tTraining Loss: 0.001765\n",
            "Epoch: 21 \tTraining Loss: 0.001755\n",
            "Epoch: 22 \tTraining Loss: 0.001748\n",
            "Epoch: 23 \tTraining Loss: 0.001746\n",
            "Epoch: 24 \tTraining Loss: 0.001739\n",
            "Epoch: 25 \tTraining Loss: 0.001735\n",
            "Epoch: 26 \tTraining Loss: 0.001731\n",
            "Epoch: 27 \tTraining Loss: 0.001729\n",
            "Epoch: 28 \tTraining Loss: 0.001726\n",
            "Epoch: 29 \tTraining Loss: 0.001720\n",
            "Epoch: 30 \tTraining Loss: 0.001718\n",
            "Epoch: 31 \tTraining Loss: 0.001714\n",
            "Epoch: 32 \tTraining Loss: 0.001713\n",
            "Epoch: 33 \tTraining Loss: 0.001712\n",
            "Epoch: 34 \tTraining Loss: 0.001709\n",
            "Epoch: 35 \tTraining Loss: 0.001708\n",
            "Epoch: 36 \tTraining Loss: 0.001703\n",
            "Epoch: 37 \tTraining Loss: 0.001701\n",
            "Epoch: 38 \tTraining Loss: 0.001699\n",
            "Epoch: 39 \tTraining Loss: 0.001697\n",
            "average validation loss: 0.001803275318040202\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.016251\n",
            "Epoch: 1 \tTraining Loss: 0.004017\n",
            "Epoch: 2 \tTraining Loss: 0.002957\n",
            "Epoch: 3 \tTraining Loss: 0.002548\n",
            "Epoch: 4 \tTraining Loss: 0.002331\n",
            "Epoch: 5 \tTraining Loss: 0.002188\n",
            "Epoch: 6 \tTraining Loss: 0.002108\n",
            "Epoch: 7 \tTraining Loss: 0.002054\n",
            "Epoch: 8 \tTraining Loss: 0.002013\n",
            "Epoch: 9 \tTraining Loss: 0.001978\n",
            "Epoch: 10 \tTraining Loss: 0.001954\n",
            "Epoch: 11 \tTraining Loss: 0.001935\n",
            "Epoch: 12 \tTraining Loss: 0.001913\n",
            "Epoch: 13 \tTraining Loss: 0.001900\n",
            "Epoch: 14 \tTraining Loss: 0.001885\n",
            "Epoch: 15 \tTraining Loss: 0.001872\n",
            "Epoch: 16 \tTraining Loss: 0.001866\n",
            "Epoch: 17 \tTraining Loss: 0.001852\n",
            "Epoch: 18 \tTraining Loss: 0.001845\n",
            "Epoch: 19 \tTraining Loss: 0.001837\n",
            "Epoch: 20 \tTraining Loss: 0.001832\n",
            "Epoch: 21 \tTraining Loss: 0.001825\n",
            "Epoch: 22 \tTraining Loss: 0.001820\n",
            "Epoch: 23 \tTraining Loss: 0.001815\n",
            "Epoch: 24 \tTraining Loss: 0.001808\n",
            "Epoch: 25 \tTraining Loss: 0.001804\n",
            "Epoch: 26 \tTraining Loss: 0.001802\n",
            "Epoch: 27 \tTraining Loss: 0.001796\n",
            "Epoch: 28 \tTraining Loss: 0.001792\n",
            "Epoch: 29 \tTraining Loss: 0.001790\n",
            "Epoch: 30 \tTraining Loss: 0.001789\n",
            "Epoch: 31 \tTraining Loss: 0.001782\n",
            "Epoch: 32 \tTraining Loss: 0.001782\n",
            "Epoch: 33 \tTraining Loss: 0.001776\n",
            "Epoch: 34 \tTraining Loss: 0.001775\n",
            "Epoch: 35 \tTraining Loss: 0.001774\n",
            "Epoch: 36 \tTraining Loss: 0.001771\n",
            "Epoch: 37 \tTraining Loss: 0.001770\n",
            "Epoch: 38 \tTraining Loss: 0.001767\n",
            "Epoch: 39 \tTraining Loss: 0.001766\n",
            "Epoch: 40 \tTraining Loss: 0.001763\n",
            "Epoch: 41 \tTraining Loss: 0.001762\n",
            "Epoch: 42 \tTraining Loss: 0.001760\n",
            "Epoch: 43 \tTraining Loss: 0.001759\n",
            "Epoch: 44 \tTraining Loss: 0.001758\n",
            "Epoch: 45 \tTraining Loss: 0.001756\n",
            "Epoch: 46 \tTraining Loss: 0.001756\n",
            "Epoch: 47 \tTraining Loss: 0.001753\n",
            "Epoch: 48 \tTraining Loss: 0.001750\n",
            "Epoch: 49 \tTraining Loss: 0.001752\n",
            "Epoch: 50 \tTraining Loss: 0.001751\n",
            "Epoch: 51 \tTraining Loss: 0.001748\n",
            "Epoch: 52 \tTraining Loss: 0.001748\n",
            "Epoch: 53 \tTraining Loss: 0.001746\n",
            "Epoch: 54 \tTraining Loss: 0.001745\n",
            "Epoch: 55 \tTraining Loss: 0.001746\n",
            "Epoch: 56 \tTraining Loss: 0.001745\n",
            "Epoch: 57 \tTraining Loss: 0.001742\n",
            "Epoch: 58 \tTraining Loss: 0.001742\n",
            "Epoch: 59 \tTraining Loss: 0.001743\n",
            "average validation loss: 0.001841558615056177\n",
            "batch size under test 32\n",
            "encoding dimension under test 32\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.040810\n",
            "Epoch: 1 \tTraining Loss: 0.019564\n",
            "Epoch: 2 \tTraining Loss: 0.016248\n",
            "Epoch: 3 \tTraining Loss: 0.015271\n",
            "Epoch: 4 \tTraining Loss: 0.014890\n",
            "Epoch: 5 \tTraining Loss: 0.014704\n",
            "Epoch: 6 \tTraining Loss: 0.014579\n",
            "Epoch: 7 \tTraining Loss: 0.014501\n",
            "Epoch: 8 \tTraining Loss: 0.014443\n",
            "Epoch: 9 \tTraining Loss: 0.014400\n",
            "Epoch: 10 \tTraining Loss: 0.014360\n",
            "Epoch: 11 \tTraining Loss: 0.014338\n",
            "Epoch: 12 \tTraining Loss: 0.014309\n",
            "Epoch: 13 \tTraining Loss: 0.014291\n",
            "Epoch: 14 \tTraining Loss: 0.014264\n",
            "Epoch: 15 \tTraining Loss: 0.014249\n",
            "Epoch: 16 \tTraining Loss: 0.014225\n",
            "Epoch: 17 \tTraining Loss: 0.014211\n",
            "Epoch: 18 \tTraining Loss: 0.014192\n",
            "Epoch: 19 \tTraining Loss: 0.014170\n",
            "Epoch: 20 \tTraining Loss: 0.014147\n",
            "Epoch: 21 \tTraining Loss: 0.014130\n",
            "Epoch: 22 \tTraining Loss: 0.014122\n",
            "Epoch: 23 \tTraining Loss: 0.014102\n",
            "Epoch: 24 \tTraining Loss: 0.014091\n",
            "Epoch: 25 \tTraining Loss: 0.014076\n",
            "Epoch: 26 \tTraining Loss: 0.014071\n",
            "Epoch: 27 \tTraining Loss: 0.014064\n",
            "Epoch: 28 \tTraining Loss: 0.014052\n",
            "Epoch: 29 \tTraining Loss: 0.014040\n",
            "Epoch: 30 \tTraining Loss: 0.014032\n",
            "Epoch: 31 \tTraining Loss: 0.014021\n",
            "Epoch: 32 \tTraining Loss: 0.014010\n",
            "Epoch: 33 \tTraining Loss: 0.014002\n",
            "Epoch: 34 \tTraining Loss: 0.013998\n",
            "Epoch: 35 \tTraining Loss: 0.013986\n",
            "Epoch: 36 \tTraining Loss: 0.013975\n",
            "Epoch: 37 \tTraining Loss: 0.013970\n",
            "Epoch: 38 \tTraining Loss: 0.013963\n",
            "Epoch: 39 \tTraining Loss: 0.013954\n",
            "average validation loss: 0.01410618870705366\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.040461\n",
            "Epoch: 1 \tTraining Loss: 0.019621\n",
            "Epoch: 2 \tTraining Loss: 0.016179\n",
            "Epoch: 3 \tTraining Loss: 0.015291\n",
            "Epoch: 4 \tTraining Loss: 0.014963\n",
            "Epoch: 5 \tTraining Loss: 0.014786\n",
            "Epoch: 6 \tTraining Loss: 0.014669\n",
            "Epoch: 7 \tTraining Loss: 0.014569\n",
            "Epoch: 8 \tTraining Loss: 0.014494\n",
            "Epoch: 9 \tTraining Loss: 0.014440\n",
            "Epoch: 10 \tTraining Loss: 0.014401\n",
            "Epoch: 11 \tTraining Loss: 0.014371\n",
            "Epoch: 12 \tTraining Loss: 0.014343\n",
            "Epoch: 13 \tTraining Loss: 0.014309\n",
            "Epoch: 14 \tTraining Loss: 0.014268\n",
            "Epoch: 15 \tTraining Loss: 0.014241\n",
            "Epoch: 16 \tTraining Loss: 0.014224\n",
            "Epoch: 17 \tTraining Loss: 0.014195\n",
            "Epoch: 18 \tTraining Loss: 0.014167\n",
            "Epoch: 19 \tTraining Loss: 0.014152\n",
            "Epoch: 20 \tTraining Loss: 0.014132\n",
            "Epoch: 21 \tTraining Loss: 0.014117\n",
            "Epoch: 22 \tTraining Loss: 0.014101\n",
            "Epoch: 23 \tTraining Loss: 0.014083\n",
            "Epoch: 24 \tTraining Loss: 0.014059\n",
            "Epoch: 25 \tTraining Loss: 0.014040\n",
            "Epoch: 26 \tTraining Loss: 0.014021\n",
            "Epoch: 27 \tTraining Loss: 0.014017\n",
            "Epoch: 28 \tTraining Loss: 0.013998\n",
            "Epoch: 29 \tTraining Loss: 0.013984\n",
            "Epoch: 30 \tTraining Loss: 0.013974\n",
            "Epoch: 31 \tTraining Loss: 0.013965\n",
            "Epoch: 32 \tTraining Loss: 0.013947\n",
            "Epoch: 33 \tTraining Loss: 0.013940\n",
            "Epoch: 34 \tTraining Loss: 0.013926\n",
            "Epoch: 35 \tTraining Loss: 0.013914\n",
            "Epoch: 36 \tTraining Loss: 0.013903\n",
            "Epoch: 37 \tTraining Loss: 0.013890\n",
            "Epoch: 38 \tTraining Loss: 0.013881\n",
            "Epoch: 39 \tTraining Loss: 0.013872\n",
            "Epoch: 40 \tTraining Loss: 0.013862\n",
            "Epoch: 41 \tTraining Loss: 0.013848\n",
            "Epoch: 42 \tTraining Loss: 0.013838\n",
            "Epoch: 43 \tTraining Loss: 0.013825\n",
            "Epoch: 44 \tTraining Loss: 0.013818\n",
            "Epoch: 45 \tTraining Loss: 0.013814\n",
            "Epoch: 46 \tTraining Loss: 0.013802\n",
            "Epoch: 47 \tTraining Loss: 0.013790\n",
            "Epoch: 48 \tTraining Loss: 0.013791\n",
            "Epoch: 49 \tTraining Loss: 0.013780\n",
            "Epoch: 50 \tTraining Loss: 0.013774\n",
            "Epoch: 51 \tTraining Loss: 0.013762\n",
            "Epoch: 52 \tTraining Loss: 0.013758\n",
            "Epoch: 53 \tTraining Loss: 0.013748\n",
            "Epoch: 54 \tTraining Loss: 0.013746\n",
            "Epoch: 55 \tTraining Loss: 0.013741\n",
            "Epoch: 56 \tTraining Loss: 0.013732\n",
            "Epoch: 57 \tTraining Loss: 0.013727\n",
            "Epoch: 58 \tTraining Loss: 0.013722\n",
            "Epoch: 59 \tTraining Loss: 0.013717\n",
            "average validation loss: 0.013888560553391775\n",
            "encoding dimension under test 64\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.030134\n",
            "Epoch: 1 \tTraining Loss: 0.009366\n",
            "Epoch: 2 \tTraining Loss: 0.006603\n",
            "Epoch: 3 \tTraining Loss: 0.005772\n",
            "Epoch: 4 \tTraining Loss: 0.005400\n",
            "Epoch: 5 \tTraining Loss: 0.005229\n",
            "Epoch: 6 \tTraining Loss: 0.005123\n",
            "Epoch: 7 \tTraining Loss: 0.005046\n",
            "Epoch: 8 \tTraining Loss: 0.004979\n",
            "Epoch: 9 \tTraining Loss: 0.004929\n",
            "Epoch: 10 \tTraining Loss: 0.004885\n",
            "Epoch: 11 \tTraining Loss: 0.004851\n",
            "Epoch: 12 \tTraining Loss: 0.004827\n",
            "Epoch: 13 \tTraining Loss: 0.004794\n",
            "Epoch: 14 \tTraining Loss: 0.004773\n",
            "Epoch: 15 \tTraining Loss: 0.004752\n",
            "Epoch: 16 \tTraining Loss: 0.004737\n",
            "Epoch: 17 \tTraining Loss: 0.004719\n",
            "Epoch: 18 \tTraining Loss: 0.004701\n",
            "Epoch: 19 \tTraining Loss: 0.004687\n",
            "Epoch: 20 \tTraining Loss: 0.004669\n",
            "Epoch: 21 \tTraining Loss: 0.004662\n",
            "Epoch: 22 \tTraining Loss: 0.004653\n",
            "Epoch: 23 \tTraining Loss: 0.004647\n",
            "Epoch: 24 \tTraining Loss: 0.004634\n",
            "Epoch: 25 \tTraining Loss: 0.004627\n",
            "Epoch: 26 \tTraining Loss: 0.004623\n",
            "Epoch: 27 \tTraining Loss: 0.004618\n",
            "Epoch: 28 \tTraining Loss: 0.004613\n",
            "Epoch: 29 \tTraining Loss: 0.004605\n",
            "Epoch: 30 \tTraining Loss: 0.004595\n",
            "Epoch: 31 \tTraining Loss: 0.004591\n",
            "Epoch: 32 \tTraining Loss: 0.004589\n",
            "Epoch: 33 \tTraining Loss: 0.004579\n",
            "Epoch: 34 \tTraining Loss: 0.004580\n",
            "Epoch: 35 \tTraining Loss: 0.004574\n",
            "Epoch: 36 \tTraining Loss: 0.004568\n",
            "Epoch: 37 \tTraining Loss: 0.004566\n",
            "Epoch: 38 \tTraining Loss: 0.004566\n",
            "Epoch: 39 \tTraining Loss: 0.004560\n",
            "average validation loss: 0.004698568860068917\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.031132\n",
            "Epoch: 1 \tTraining Loss: 0.010476\n",
            "Epoch: 2 \tTraining Loss: 0.007339\n",
            "Epoch: 3 \tTraining Loss: 0.006546\n",
            "Epoch: 4 \tTraining Loss: 0.006224\n",
            "Epoch: 5 \tTraining Loss: 0.006041\n",
            "Epoch: 6 \tTraining Loss: 0.005933\n",
            "Epoch: 7 \tTraining Loss: 0.005845\n",
            "Epoch: 8 \tTraining Loss: 0.005791\n",
            "Epoch: 9 \tTraining Loss: 0.005732\n",
            "Epoch: 10 \tTraining Loss: 0.005685\n",
            "Epoch: 11 \tTraining Loss: 0.005643\n",
            "Epoch: 12 \tTraining Loss: 0.005613\n",
            "Epoch: 13 \tTraining Loss: 0.005579\n",
            "Epoch: 14 \tTraining Loss: 0.005564\n",
            "Epoch: 15 \tTraining Loss: 0.005541\n",
            "Epoch: 16 \tTraining Loss: 0.005521\n",
            "Epoch: 17 \tTraining Loss: 0.005505\n",
            "Epoch: 18 \tTraining Loss: 0.005484\n",
            "Epoch: 19 \tTraining Loss: 0.005470\n",
            "Epoch: 20 \tTraining Loss: 0.005462\n",
            "Epoch: 21 \tTraining Loss: 0.005452\n",
            "Epoch: 22 \tTraining Loss: 0.005440\n",
            "Epoch: 23 \tTraining Loss: 0.005438\n",
            "Epoch: 24 \tTraining Loss: 0.005424\n",
            "Epoch: 25 \tTraining Loss: 0.005422\n",
            "Epoch: 26 \tTraining Loss: 0.005411\n",
            "Epoch: 27 \tTraining Loss: 0.005408\n",
            "Epoch: 28 \tTraining Loss: 0.005402\n",
            "Epoch: 29 \tTraining Loss: 0.005397\n",
            "Epoch: 30 \tTraining Loss: 0.005388\n",
            "Epoch: 31 \tTraining Loss: 0.005384\n",
            "Epoch: 32 \tTraining Loss: 0.005376\n",
            "Epoch: 33 \tTraining Loss: 0.005375\n",
            "Epoch: 34 \tTraining Loss: 0.005367\n",
            "Epoch: 35 \tTraining Loss: 0.005364\n",
            "Epoch: 36 \tTraining Loss: 0.005360\n",
            "Epoch: 37 \tTraining Loss: 0.005357\n",
            "Epoch: 38 \tTraining Loss: 0.005355\n",
            "Epoch: 39 \tTraining Loss: 0.005350\n",
            "Epoch: 40 \tTraining Loss: 0.005347\n",
            "Epoch: 41 \tTraining Loss: 0.005344\n",
            "Epoch: 42 \tTraining Loss: 0.005343\n",
            "Epoch: 43 \tTraining Loss: 0.005340\n",
            "Epoch: 44 \tTraining Loss: 0.005334\n",
            "Epoch: 45 \tTraining Loss: 0.005335\n",
            "Epoch: 46 \tTraining Loss: 0.005331\n",
            "Epoch: 47 \tTraining Loss: 0.005327\n",
            "Epoch: 48 \tTraining Loss: 0.005323\n",
            "Epoch: 49 \tTraining Loss: 0.005324\n",
            "Epoch: 50 \tTraining Loss: 0.005323\n",
            "Epoch: 51 \tTraining Loss: 0.005317\n",
            "Epoch: 52 \tTraining Loss: 0.005313\n",
            "Epoch: 53 \tTraining Loss: 0.005314\n",
            "Epoch: 54 \tTraining Loss: 0.005312\n",
            "Epoch: 55 \tTraining Loss: 0.005308\n",
            "Epoch: 56 \tTraining Loss: 0.005306\n",
            "Epoch: 57 \tTraining Loss: 0.005306\n",
            "Epoch: 58 \tTraining Loss: 0.005302\n",
            "Epoch: 59 \tTraining Loss: 0.005304\n",
            "average validation loss: 0.005495807031790415\n",
            "encoding dimension under test 128\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.022631\n",
            "Epoch: 1 \tTraining Loss: 0.005440\n",
            "Epoch: 2 \tTraining Loss: 0.003558\n",
            "Epoch: 3 \tTraining Loss: 0.002895\n",
            "Epoch: 4 \tTraining Loss: 0.002554\n",
            "Epoch: 5 \tTraining Loss: 0.002360\n",
            "Epoch: 6 \tTraining Loss: 0.002228\n",
            "Epoch: 7 \tTraining Loss: 0.002143\n",
            "Epoch: 8 \tTraining Loss: 0.002069\n",
            "Epoch: 9 \tTraining Loss: 0.002020\n",
            "Epoch: 10 \tTraining Loss: 0.001978\n",
            "Epoch: 11 \tTraining Loss: 0.001943\n",
            "Epoch: 12 \tTraining Loss: 0.001916\n",
            "Epoch: 13 \tTraining Loss: 0.001891\n",
            "Epoch: 14 \tTraining Loss: 0.001871\n",
            "Epoch: 15 \tTraining Loss: 0.001854\n",
            "Epoch: 16 \tTraining Loss: 0.001841\n",
            "Epoch: 17 \tTraining Loss: 0.001827\n",
            "Epoch: 18 \tTraining Loss: 0.001812\n",
            "Epoch: 19 \tTraining Loss: 0.001801\n",
            "Epoch: 20 \tTraining Loss: 0.001793\n",
            "Epoch: 21 \tTraining Loss: 0.001784\n",
            "Epoch: 22 \tTraining Loss: 0.001774\n",
            "Epoch: 23 \tTraining Loss: 0.001767\n",
            "Epoch: 24 \tTraining Loss: 0.001762\n",
            "Epoch: 25 \tTraining Loss: 0.001757\n",
            "Epoch: 26 \tTraining Loss: 0.001752\n",
            "Epoch: 27 \tTraining Loss: 0.001745\n",
            "Epoch: 28 \tTraining Loss: 0.001740\n",
            "Epoch: 29 \tTraining Loss: 0.001733\n",
            "Epoch: 30 \tTraining Loss: 0.001732\n",
            "Epoch: 31 \tTraining Loss: 0.001728\n",
            "Epoch: 32 \tTraining Loss: 0.001725\n",
            "Epoch: 33 \tTraining Loss: 0.001719\n",
            "Epoch: 34 \tTraining Loss: 0.001713\n",
            "Epoch: 35 \tTraining Loss: 0.001711\n",
            "Epoch: 36 \tTraining Loss: 0.001709\n",
            "Epoch: 37 \tTraining Loss: 0.001704\n",
            "Epoch: 38 \tTraining Loss: 0.001701\n",
            "Epoch: 39 \tTraining Loss: 0.001701\n",
            "average validation loss: 0.0017972165880103905\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.022776\n",
            "Epoch: 1 \tTraining Loss: 0.005559\n",
            "Epoch: 2 \tTraining Loss: 0.003606\n",
            "Epoch: 3 \tTraining Loss: 0.002927\n",
            "Epoch: 4 \tTraining Loss: 0.002599\n",
            "Epoch: 5 \tTraining Loss: 0.002409\n",
            "Epoch: 6 \tTraining Loss: 0.002279\n",
            "Epoch: 7 \tTraining Loss: 0.002178\n",
            "Epoch: 8 \tTraining Loss: 0.002100\n",
            "Epoch: 9 \tTraining Loss: 0.002043\n",
            "Epoch: 10 \tTraining Loss: 0.002006\n",
            "Epoch: 11 \tTraining Loss: 0.001968\n",
            "Epoch: 12 \tTraining Loss: 0.001928\n",
            "Epoch: 13 \tTraining Loss: 0.001890\n",
            "Epoch: 14 \tTraining Loss: 0.001866\n",
            "Epoch: 15 \tTraining Loss: 0.001842\n",
            "Epoch: 16 \tTraining Loss: 0.001826\n",
            "Epoch: 17 \tTraining Loss: 0.001812\n",
            "Epoch: 18 \tTraining Loss: 0.001799\n",
            "Epoch: 19 \tTraining Loss: 0.001787\n",
            "Epoch: 20 \tTraining Loss: 0.001775\n",
            "Epoch: 21 \tTraining Loss: 0.001765\n",
            "Epoch: 22 \tTraining Loss: 0.001759\n",
            "Epoch: 23 \tTraining Loss: 0.001750\n",
            "Epoch: 24 \tTraining Loss: 0.001742\n",
            "Epoch: 25 \tTraining Loss: 0.001736\n",
            "Epoch: 26 \tTraining Loss: 0.001730\n",
            "Epoch: 27 \tTraining Loss: 0.001726\n",
            "Epoch: 28 \tTraining Loss: 0.001720\n",
            "Epoch: 29 \tTraining Loss: 0.001715\n",
            "Epoch: 30 \tTraining Loss: 0.001710\n",
            "Epoch: 31 \tTraining Loss: 0.001709\n",
            "Epoch: 32 \tTraining Loss: 0.001703\n",
            "Epoch: 33 \tTraining Loss: 0.001699\n",
            "Epoch: 34 \tTraining Loss: 0.001696\n",
            "Epoch: 35 \tTraining Loss: 0.001693\n",
            "Epoch: 36 \tTraining Loss: 0.001689\n",
            "Epoch: 37 \tTraining Loss: 0.001688\n",
            "Epoch: 38 \tTraining Loss: 0.001684\n",
            "Epoch: 39 \tTraining Loss: 0.001680\n",
            "Epoch: 40 \tTraining Loss: 0.001679\n",
            "Epoch: 41 \tTraining Loss: 0.001679\n",
            "Epoch: 42 \tTraining Loss: 0.001675\n",
            "Epoch: 43 \tTraining Loss: 0.001672\n",
            "Epoch: 44 \tTraining Loss: 0.001671\n",
            "Epoch: 45 \tTraining Loss: 0.001671\n",
            "Epoch: 46 \tTraining Loss: 0.001666\n",
            "Epoch: 47 \tTraining Loss: 0.001666\n",
            "Epoch: 48 \tTraining Loss: 0.001661\n",
            "Epoch: 49 \tTraining Loss: 0.001660\n",
            "Epoch: 50 \tTraining Loss: 0.001659\n",
            "Epoch: 51 \tTraining Loss: 0.001656\n",
            "Epoch: 52 \tTraining Loss: 0.001654\n",
            "Epoch: 53 \tTraining Loss: 0.001652\n",
            "Epoch: 54 \tTraining Loss: 0.001652\n",
            "Epoch: 55 \tTraining Loss: 0.001650\n",
            "Epoch: 56 \tTraining Loss: 0.001649\n",
            "Epoch: 57 \tTraining Loss: 0.001649\n",
            "Epoch: 58 \tTraining Loss: 0.001646\n",
            "Epoch: 59 \tTraining Loss: 0.001645\n",
            "average validation loss: 0.0017599450452253221\n",
            "batch size under test 64\n",
            "encoding dimension under test 32\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.052780\n",
            "Epoch: 1 \tTraining Loss: 0.026597\n",
            "Epoch: 2 \tTraining Loss: 0.021454\n",
            "Epoch: 3 \tTraining Loss: 0.019296\n",
            "Epoch: 4 \tTraining Loss: 0.018207\n",
            "Epoch: 5 \tTraining Loss: 0.016971\n",
            "Epoch: 6 \tTraining Loss: 0.016537\n",
            "Epoch: 7 \tTraining Loss: 0.016309\n",
            "Epoch: 8 \tTraining Loss: 0.016167\n",
            "Epoch: 9 \tTraining Loss: 0.016063\n",
            "Epoch: 10 \tTraining Loss: 0.015984\n",
            "Epoch: 11 \tTraining Loss: 0.015928\n",
            "Epoch: 12 \tTraining Loss: 0.015880\n",
            "Epoch: 13 \tTraining Loss: 0.015837\n",
            "Epoch: 14 \tTraining Loss: 0.015798\n",
            "Epoch: 15 \tTraining Loss: 0.015767\n",
            "Epoch: 16 \tTraining Loss: 0.015730\n",
            "Epoch: 17 \tTraining Loss: 0.015702\n",
            "Epoch: 18 \tTraining Loss: 0.015679\n",
            "Epoch: 19 \tTraining Loss: 0.015655\n",
            "Epoch: 20 \tTraining Loss: 0.015631\n",
            "Epoch: 21 \tTraining Loss: 0.015612\n",
            "Epoch: 22 \tTraining Loss: 0.015584\n",
            "Epoch: 23 \tTraining Loss: 0.015558\n",
            "Epoch: 24 \tTraining Loss: 0.015534\n",
            "Epoch: 25 \tTraining Loss: 0.015516\n",
            "Epoch: 26 \tTraining Loss: 0.015491\n",
            "Epoch: 27 \tTraining Loss: 0.015476\n",
            "Epoch: 28 \tTraining Loss: 0.015455\n",
            "Epoch: 29 \tTraining Loss: 0.015434\n",
            "Epoch: 30 \tTraining Loss: 0.015418\n",
            "Epoch: 31 \tTraining Loss: 0.015404\n",
            "Epoch: 32 \tTraining Loss: 0.015384\n",
            "Epoch: 33 \tTraining Loss: 0.015371\n",
            "Epoch: 34 \tTraining Loss: 0.015359\n",
            "Epoch: 35 \tTraining Loss: 0.015346\n",
            "Epoch: 36 \tTraining Loss: 0.015331\n",
            "Epoch: 37 \tTraining Loss: 0.015320\n",
            "Epoch: 38 \tTraining Loss: 0.015309\n",
            "Epoch: 39 \tTraining Loss: 0.015295\n",
            "average validation loss: 0.015392365475495657\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.050823\n",
            "Epoch: 1 \tTraining Loss: 0.024662\n",
            "Epoch: 2 \tTraining Loss: 0.019370\n",
            "Epoch: 3 \tTraining Loss: 0.016766\n",
            "Epoch: 4 \tTraining Loss: 0.015599\n",
            "Epoch: 5 \tTraining Loss: 0.015097\n",
            "Epoch: 6 \tTraining Loss: 0.014863\n",
            "Epoch: 7 \tTraining Loss: 0.014725\n",
            "Epoch: 8 \tTraining Loss: 0.014629\n",
            "Epoch: 9 \tTraining Loss: 0.014553\n",
            "Epoch: 10 \tTraining Loss: 0.014501\n",
            "Epoch: 11 \tTraining Loss: 0.014453\n",
            "Epoch: 12 \tTraining Loss: 0.014361\n",
            "Epoch: 13 \tTraining Loss: 0.014311\n",
            "Epoch: 14 \tTraining Loss: 0.014282\n",
            "Epoch: 15 \tTraining Loss: 0.014264\n",
            "Epoch: 16 \tTraining Loss: 0.014250\n",
            "Epoch: 17 \tTraining Loss: 0.014234\n",
            "Epoch: 18 \tTraining Loss: 0.014210\n",
            "Epoch: 19 \tTraining Loss: 0.014197\n",
            "Epoch: 20 \tTraining Loss: 0.014177\n",
            "Epoch: 21 \tTraining Loss: 0.014168\n",
            "Epoch: 22 \tTraining Loss: 0.014158\n",
            "Epoch: 23 \tTraining Loss: 0.014149\n",
            "Epoch: 24 \tTraining Loss: 0.014133\n",
            "Epoch: 25 \tTraining Loss: 0.014128\n",
            "Epoch: 26 \tTraining Loss: 0.014121\n",
            "Epoch: 27 \tTraining Loss: 0.014113\n",
            "Epoch: 28 \tTraining Loss: 0.014099\n",
            "Epoch: 29 \tTraining Loss: 0.014097\n",
            "Epoch: 30 \tTraining Loss: 0.014080\n",
            "Epoch: 31 \tTraining Loss: 0.014081\n",
            "Epoch: 32 \tTraining Loss: 0.014070\n",
            "Epoch: 33 \tTraining Loss: 0.014052\n",
            "Epoch: 34 \tTraining Loss: 0.014031\n",
            "Epoch: 35 \tTraining Loss: 0.014014\n",
            "Epoch: 36 \tTraining Loss: 0.014013\n",
            "Epoch: 37 \tTraining Loss: 0.013998\n",
            "Epoch: 38 \tTraining Loss: 0.013993\n",
            "Epoch: 39 \tTraining Loss: 0.013989\n",
            "Epoch: 40 \tTraining Loss: 0.013984\n",
            "Epoch: 41 \tTraining Loss: 0.013976\n",
            "Epoch: 42 \tTraining Loss: 0.013965\n",
            "Epoch: 43 \tTraining Loss: 0.013961\n",
            "Epoch: 44 \tTraining Loss: 0.013949\n",
            "Epoch: 45 \tTraining Loss: 0.013946\n",
            "Epoch: 46 \tTraining Loss: 0.013943\n",
            "Epoch: 47 \tTraining Loss: 0.013935\n",
            "Epoch: 48 \tTraining Loss: 0.013924\n",
            "Epoch: 49 \tTraining Loss: 0.013922\n",
            "Epoch: 50 \tTraining Loss: 0.013919\n",
            "Epoch: 51 \tTraining Loss: 0.013910\n",
            "Epoch: 52 \tTraining Loss: 0.013905\n",
            "Epoch: 53 \tTraining Loss: 0.013896\n",
            "Epoch: 54 \tTraining Loss: 0.013892\n",
            "Epoch: 55 \tTraining Loss: 0.013888\n",
            "Epoch: 56 \tTraining Loss: 0.013884\n",
            "Epoch: 57 \tTraining Loss: 0.013873\n",
            "Epoch: 58 \tTraining Loss: 0.013871\n",
            "Epoch: 59 \tTraining Loss: 0.013869\n",
            "average validation loss: 0.014030455440282821\n",
            "encoding dimension under test 64\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.041261\n",
            "Epoch: 1 \tTraining Loss: 0.016294\n",
            "Epoch: 2 \tTraining Loss: 0.010031\n",
            "Epoch: 3 \tTraining Loss: 0.007647\n",
            "Epoch: 4 \tTraining Loss: 0.006768\n",
            "Epoch: 5 \tTraining Loss: 0.006374\n",
            "Epoch: 6 \tTraining Loss: 0.006160\n",
            "Epoch: 7 \tTraining Loss: 0.006024\n",
            "Epoch: 8 \tTraining Loss: 0.005917\n",
            "Epoch: 9 \tTraining Loss: 0.005838\n",
            "Epoch: 10 \tTraining Loss: 0.005775\n",
            "Epoch: 11 \tTraining Loss: 0.005714\n",
            "Epoch: 12 \tTraining Loss: 0.005663\n",
            "Epoch: 13 \tTraining Loss: 0.005625\n",
            "Epoch: 14 \tTraining Loss: 0.005597\n",
            "Epoch: 15 \tTraining Loss: 0.005569\n",
            "Epoch: 16 \tTraining Loss: 0.005547\n",
            "Epoch: 17 \tTraining Loss: 0.005524\n",
            "Epoch: 18 \tTraining Loss: 0.005508\n",
            "Epoch: 19 \tTraining Loss: 0.005492\n",
            "Epoch: 20 \tTraining Loss: 0.005468\n",
            "Epoch: 21 \tTraining Loss: 0.005458\n",
            "Epoch: 22 \tTraining Loss: 0.005441\n",
            "Epoch: 23 \tTraining Loss: 0.005430\n",
            "Epoch: 24 \tTraining Loss: 0.005422\n",
            "Epoch: 25 \tTraining Loss: 0.005410\n",
            "Epoch: 26 \tTraining Loss: 0.005396\n",
            "Epoch: 27 \tTraining Loss: 0.005384\n",
            "Epoch: 28 \tTraining Loss: 0.005369\n",
            "Epoch: 29 \tTraining Loss: 0.005365\n",
            "Epoch: 30 \tTraining Loss: 0.005356\n",
            "Epoch: 31 \tTraining Loss: 0.005351\n",
            "Epoch: 32 \tTraining Loss: 0.005345\n",
            "Epoch: 33 \tTraining Loss: 0.005335\n",
            "Epoch: 34 \tTraining Loss: 0.005332\n",
            "Epoch: 35 \tTraining Loss: 0.005327\n",
            "Epoch: 36 \tTraining Loss: 0.005318\n",
            "Epoch: 37 \tTraining Loss: 0.005316\n",
            "Epoch: 38 \tTraining Loss: 0.005309\n",
            "Epoch: 39 \tTraining Loss: 0.005306\n",
            "average validation loss: 0.005479766167700291\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.041197\n",
            "Epoch: 1 \tTraining Loss: 0.015519\n",
            "Epoch: 2 \tTraining Loss: 0.009879\n",
            "Epoch: 3 \tTraining Loss: 0.007699\n",
            "Epoch: 4 \tTraining Loss: 0.006812\n",
            "Epoch: 5 \tTraining Loss: 0.006431\n",
            "Epoch: 6 \tTraining Loss: 0.006191\n",
            "Epoch: 7 \tTraining Loss: 0.005928\n",
            "Epoch: 8 \tTraining Loss: 0.005814\n",
            "Epoch: 9 \tTraining Loss: 0.005726\n",
            "Epoch: 10 \tTraining Loss: 0.005669\n",
            "Epoch: 11 \tTraining Loss: 0.005618\n",
            "Epoch: 12 \tTraining Loss: 0.005565\n",
            "Epoch: 13 \tTraining Loss: 0.005519\n",
            "Epoch: 14 \tTraining Loss: 0.005485\n",
            "Epoch: 15 \tTraining Loss: 0.005459\n",
            "Epoch: 16 \tTraining Loss: 0.005440\n",
            "Epoch: 17 \tTraining Loss: 0.005416\n",
            "Epoch: 18 \tTraining Loss: 0.005400\n",
            "Epoch: 19 \tTraining Loss: 0.005391\n",
            "Epoch: 20 \tTraining Loss: 0.005378\n",
            "Epoch: 21 \tTraining Loss: 0.005359\n",
            "Epoch: 22 \tTraining Loss: 0.005350\n",
            "Epoch: 23 \tTraining Loss: 0.005337\n",
            "Epoch: 24 \tTraining Loss: 0.005321\n",
            "Epoch: 25 \tTraining Loss: 0.005312\n",
            "Epoch: 26 \tTraining Loss: 0.005302\n",
            "Epoch: 27 \tTraining Loss: 0.005290\n",
            "Epoch: 28 \tTraining Loss: 0.005274\n",
            "Epoch: 29 \tTraining Loss: 0.005267\n",
            "Epoch: 30 \tTraining Loss: 0.005257\n",
            "Epoch: 31 \tTraining Loss: 0.005251\n",
            "Epoch: 32 \tTraining Loss: 0.005240\n",
            "Epoch: 33 \tTraining Loss: 0.005238\n",
            "Epoch: 34 \tTraining Loss: 0.005233\n",
            "Epoch: 35 \tTraining Loss: 0.005228\n",
            "Epoch: 36 \tTraining Loss: 0.005222\n",
            "Epoch: 37 \tTraining Loss: 0.005213\n",
            "Epoch: 38 \tTraining Loss: 0.005209\n",
            "Epoch: 39 \tTraining Loss: 0.005199\n",
            "Epoch: 40 \tTraining Loss: 0.005195\n",
            "Epoch: 41 \tTraining Loss: 0.005191\n",
            "Epoch: 42 \tTraining Loss: 0.005185\n",
            "Epoch: 43 \tTraining Loss: 0.005175\n",
            "Epoch: 44 \tTraining Loss: 0.005174\n",
            "Epoch: 45 \tTraining Loss: 0.005167\n",
            "Epoch: 46 \tTraining Loss: 0.005168\n",
            "Epoch: 47 \tTraining Loss: 0.005166\n",
            "Epoch: 48 \tTraining Loss: 0.005160\n",
            "Epoch: 49 \tTraining Loss: 0.005159\n",
            "Epoch: 50 \tTraining Loss: 0.005151\n",
            "Epoch: 51 \tTraining Loss: 0.005150\n",
            "Epoch: 52 \tTraining Loss: 0.005146\n",
            "Epoch: 53 \tTraining Loss: 0.005144\n",
            "Epoch: 54 \tTraining Loss: 0.005144\n",
            "Epoch: 55 \tTraining Loss: 0.005140\n",
            "Epoch: 56 \tTraining Loss: 0.005133\n",
            "Epoch: 57 \tTraining Loss: 0.005133\n",
            "Epoch: 58 \tTraining Loss: 0.005130\n",
            "Epoch: 59 \tTraining Loss: 0.005130\n",
            "average validation loss: 0.005293305454154809\n",
            "encoding dimension under test 128\n",
            "number of epoch under test 40\n",
            "Epoch: 0 \tTraining Loss: 0.032270\n",
            "Epoch: 1 \tTraining Loss: 0.009414\n",
            "Epoch: 2 \tTraining Loss: 0.005362\n",
            "Epoch: 3 \tTraining Loss: 0.003966\n",
            "Epoch: 4 \tTraining Loss: 0.003259\n",
            "Epoch: 5 \tTraining Loss: 0.002827\n",
            "Epoch: 6 \tTraining Loss: 0.002552\n",
            "Epoch: 7 \tTraining Loss: 0.002365\n",
            "Epoch: 8 \tTraining Loss: 0.002247\n",
            "Epoch: 9 \tTraining Loss: 0.002162\n",
            "Epoch: 10 \tTraining Loss: 0.002100\n",
            "Epoch: 11 \tTraining Loss: 0.002052\n",
            "Epoch: 12 \tTraining Loss: 0.002011\n",
            "Epoch: 13 \tTraining Loss: 0.001975\n",
            "Epoch: 14 \tTraining Loss: 0.001943\n",
            "Epoch: 15 \tTraining Loss: 0.001899\n",
            "Epoch: 16 \tTraining Loss: 0.001874\n",
            "Epoch: 17 \tTraining Loss: 0.001854\n",
            "Epoch: 18 \tTraining Loss: 0.001835\n",
            "Epoch: 19 \tTraining Loss: 0.001820\n",
            "Epoch: 20 \tTraining Loss: 0.001806\n",
            "Epoch: 21 \tTraining Loss: 0.001793\n",
            "Epoch: 22 \tTraining Loss: 0.001783\n",
            "Epoch: 23 \tTraining Loss: 0.001771\n",
            "Epoch: 24 \tTraining Loss: 0.001762\n",
            "Epoch: 25 \tTraining Loss: 0.001749\n",
            "Epoch: 26 \tTraining Loss: 0.001744\n",
            "Epoch: 27 \tTraining Loss: 0.001734\n",
            "Epoch: 28 \tTraining Loss: 0.001724\n",
            "Epoch: 29 \tTraining Loss: 0.001719\n",
            "Epoch: 30 \tTraining Loss: 0.001712\n",
            "Epoch: 31 \tTraining Loss: 0.001707\n",
            "Epoch: 32 \tTraining Loss: 0.001702\n",
            "Epoch: 33 \tTraining Loss: 0.001697\n",
            "Epoch: 34 \tTraining Loss: 0.001691\n",
            "Epoch: 35 \tTraining Loss: 0.001690\n",
            "Epoch: 36 \tTraining Loss: 0.001684\n",
            "Epoch: 37 \tTraining Loss: 0.001681\n",
            "Epoch: 38 \tTraining Loss: 0.001677\n",
            "Epoch: 39 \tTraining Loss: 0.001673\n",
            "average validation loss: 0.0017778251422569155\n",
            "number of epoch under test 60\n",
            "Epoch: 0 \tTraining Loss: 0.032250\n",
            "Epoch: 1 \tTraining Loss: 0.009489\n",
            "Epoch: 2 \tTraining Loss: 0.005375\n",
            "Epoch: 3 \tTraining Loss: 0.003959\n",
            "Epoch: 4 \tTraining Loss: 0.003276\n",
            "Epoch: 5 \tTraining Loss: 0.002895\n",
            "Epoch: 6 \tTraining Loss: 0.002648\n",
            "Epoch: 7 \tTraining Loss: 0.002482\n",
            "Epoch: 8 \tTraining Loss: 0.002339\n",
            "Epoch: 9 \tTraining Loss: 0.002233\n",
            "Epoch: 10 \tTraining Loss: 0.002164\n",
            "Epoch: 11 \tTraining Loss: 0.002111\n",
            "Epoch: 12 \tTraining Loss: 0.002068\n",
            "Epoch: 13 \tTraining Loss: 0.002032\n",
            "Epoch: 14 \tTraining Loss: 0.002005\n",
            "Epoch: 15 \tTraining Loss: 0.001978\n",
            "Epoch: 16 \tTraining Loss: 0.001954\n",
            "Epoch: 17 \tTraining Loss: 0.001922\n",
            "Epoch: 18 \tTraining Loss: 0.001903\n",
            "Epoch: 19 \tTraining Loss: 0.001880\n",
            "Epoch: 20 \tTraining Loss: 0.001863\n",
            "Epoch: 21 \tTraining Loss: 0.001850\n",
            "Epoch: 22 \tTraining Loss: 0.001834\n",
            "Epoch: 23 \tTraining Loss: 0.001815\n",
            "Epoch: 24 \tTraining Loss: 0.001799\n",
            "Epoch: 25 \tTraining Loss: 0.001788\n",
            "Epoch: 26 \tTraining Loss: 0.001774\n",
            "Epoch: 27 \tTraining Loss: 0.001763\n",
            "Epoch: 28 \tTraining Loss: 0.001755\n",
            "Epoch: 29 \tTraining Loss: 0.001746\n",
            "Epoch: 30 \tTraining Loss: 0.001738\n",
            "Epoch: 31 \tTraining Loss: 0.001734\n",
            "Epoch: 32 \tTraining Loss: 0.001722\n",
            "Epoch: 33 \tTraining Loss: 0.001711\n",
            "Epoch: 34 \tTraining Loss: 0.001703\n",
            "Epoch: 35 \tTraining Loss: 0.001699\n",
            "Epoch: 36 \tTraining Loss: 0.001694\n",
            "Epoch: 37 \tTraining Loss: 0.001689\n",
            "Epoch: 38 \tTraining Loss: 0.001685\n",
            "Epoch: 39 \tTraining Loss: 0.001677\n",
            "Epoch: 40 \tTraining Loss: 0.001677\n",
            "Epoch: 41 \tTraining Loss: 0.001670\n",
            "Epoch: 42 \tTraining Loss: 0.001667\n",
            "Epoch: 43 \tTraining Loss: 0.001666\n",
            "Epoch: 44 \tTraining Loss: 0.001662\n",
            "Epoch: 45 \tTraining Loss: 0.001659\n",
            "Epoch: 46 \tTraining Loss: 0.001655\n",
            "Epoch: 47 \tTraining Loss: 0.001654\n",
            "Epoch: 48 \tTraining Loss: 0.001652\n",
            "Epoch: 49 \tTraining Loss: 0.001649\n",
            "Epoch: 50 \tTraining Loss: 0.001647\n",
            "Epoch: 51 \tTraining Loss: 0.001642\n",
            "Epoch: 52 \tTraining Loss: 0.001639\n",
            "Epoch: 53 \tTraining Loss: 0.001640\n",
            "Epoch: 54 \tTraining Loss: 0.001637\n",
            "Epoch: 55 \tTraining Loss: 0.001633\n",
            "Epoch: 56 \tTraining Loss: 0.001635\n",
            "Epoch: 57 \tTraining Loss: 0.001630\n",
            "Epoch: 58 \tTraining Loss: 0.001630\n",
            "Epoch: 59 \tTraining Loss: 0.001630\n",
            "average validation loss: 0.0017278431582575043\n",
            "Best Validation Loss: 0.0017278431582575043\n",
            "Best Parameters: Batch Size = 64, Encoding Dim = 128, Epochs = 60\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load the dataset\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "# Splitting the training data into train and validation sets\n",
        "num_train = int(len(train_data) * 0.8)  # Using 80% for training\n",
        "num_val = len(train_data) - num_train   # Remaining 20% for validation\n",
        "train_data, val_data = torch.utils.data.random_split(train_data, [num_train, num_val], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Grid search parameters\n",
        "batch_sizes = [16, 32, 64]\n",
        "encoding_dims = [32, 64, 128]\n",
        "epochs_list = [40, 60]\n",
        "\n",
        "# Store best model info\n",
        "best_loss = np.inf\n",
        "best_params = {}\n",
        "patience = 5  # Number of epochs to wait for improvement before stopping\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"batch size under test {batch_size}\")\n",
        "    for encoding_dim in encoding_dims:\n",
        "        print(f\"encoding dimension under test {encoding_dim}\")\n",
        "        for n_epochs in epochs_list:\n",
        "            print(f\"number of epoch under test {n_epochs}\")\n",
        "            # Prepare data loaders\n",
        "            train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            # Initialize the NN\n",
        "            model = Autoencoder(encoding_dim)\n",
        "            if torch.cuda.is_available():\n",
        "                model.cuda()\n",
        "\n",
        "            # Specify loss function and optimizer\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "            # Implementing early stopping\n",
        "            early_stopping_counter = 0  # Tracks epochs without improvement\n",
        "\n",
        "            # Train the model with current parameter set\n",
        "            model.train()\n",
        "            for epoch in range(n_epochs):\n",
        "                train_loss = 0.0\n",
        "                for data in train_loader:\n",
        "                    images, _ = data\n",
        "                    images = images.view(images.size(0), -1)\n",
        "                    if torch.cuda.is_available():\n",
        "                        images = images.cuda()\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, images)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    train_loss += loss.item()*images.size(0)\n",
        "                train_loss = train_loss/len(train_loader.dataset)\n",
        "                print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch,train_loss))\n",
        "\n",
        "            # Validate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                total_val_loss = 0.0\n",
        "                for data in val_loader:\n",
        "                    images, _ = data\n",
        "                    images = images.view(images.size(0), -1)\n",
        "                    if torch.cuda.is_available():\n",
        "                        images = images.cuda()\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, images)\n",
        "                    total_val_loss += loss.item()*images.size(0)\n",
        "                avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
        "                print(f\"average validation loss: {avg_val_loss}\")\n",
        "\n",
        "                # Early stopping logic\n",
        "                if avg_val_loss < best_loss:\n",
        "                    best_loss = avg_val_loss\n",
        "                    best_params = {'batch_size': batch_size, 'encoding_dim': encoding_dim, 'epochs': n_epochs}\n",
        "                    early_stopping_counter = 0  # Reset counter\n",
        "                else:\n",
        "                    early_stopping_counter += 1  # Increment counter\n",
        "\n",
        "                if early_stopping_counter > patience:\n",
        "                    print(\"Stopping early due to lack of improvement in validation loss.\")\n",
        "                    break  # Exit the epoch loop\n",
        "print(f\"Best Validation Loss: {best_loss}\")\n",
        "print(f\"Best Parameters: Batch Size = {best_params['batch_size']}, Encoding Dim = {best_params['encoding_dim']}, Epochs = {best_params['epochs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f27LdJ6e6h8S"
      },
      "source": [
        "## Train the selected model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xuvubRjp6oT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a67d6c-c9f9-48f9-9eb4-83af16e33d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 103974369.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 41598795.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27629526.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21429166.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Epoch: 59 \tTraining Loss: 0.000335\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "# Convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load the dataset\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Select only a part of the dataset to speed up training times\n",
        "num_train_samples = 10000\n",
        "num_test_samples = 1000\n",
        "\n",
        "# Randomly select a subset of samples\n",
        "train_indices = torch.randperm(len(train_data))[:num_train_samples]\n",
        "test_indices = torch.randperm(len(test_data))[:num_test_samples]\n",
        "\n",
        "# Create subset samplers to be used in the dataloader\n",
        "train_subset_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "test_subset_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
        "\n",
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# parameters\n",
        "batch_size = 64\n",
        "encoding_dim = 128\n",
        "n_epochs = 60\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           sampler = train_subset_sampler,\n",
        "                                           num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          sampler = test_subset_sampler,\n",
        "                                          num_workers=0)\n",
        "\n",
        "# Initialize the NN\n",
        "model = Autoencoder(encoding_dim)\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "# Specify loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model with current parameter set\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "  train_loss = 0.0\n",
        "  for data in train_loader:\n",
        "    images, _ = data\n",
        "    images = images.view(images.size(0), -1)\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, images)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*images.size(0)\n",
        "  train_loss = train_loss/len(train_loader.dataset)\n",
        "print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch,train_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Setg-PyJ6HzV"
      },
      "source": [
        "## Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gkn7HOxz6WO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6692154e-2dbc-4ead-b392-e8c0c5f01f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.000227\n"
          ]
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Monitor test loss\n",
        "test_loss = 0.0\n",
        "\n",
        "# Disable gradient computation\n",
        "with torch.no_grad():\n",
        "    # Iterate over the test data\n",
        "    for batch_data in test_loader:\n",
        "        # Extract images from the batch\n",
        "        images, _ = batch_data\n",
        "        # Flatten images and send them to GPU\n",
        "        images = images.view(images.size(0), -1)\n",
        "        # Send to GPU\n",
        "        if torch.cuda.is_available():\n",
        "          images = images.cuda()\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(images)\n",
        "        # Calculate the loss between output and input images\n",
        "        loss = criterion(outputs, images)\n",
        "        # Update test loss\n",
        "        test_loss += loss.item()*images.size(0)\n",
        "\n",
        "# Compute average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "\n",
        "# Print test loss\n",
        "print('Test Loss: {:.6f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVYXhsPrLBrV"
      },
      "source": [
        "# Stacked Autoencoder\n",
        "In this section, we implement the Stacked Autoencoder from scratch and train it on the MNIST dataset.\n",
        "The training process is with two steps: (1) each layer is trained separately; (2) the network is trained as a whole.\n",
        "\n",
        "We first load the dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8163o-y3SLfw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim, functional, utils\n",
        "\n",
        "from torch.nn import BCELoss, init\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import time, os\n",
        "\n",
        "# We automate the dataloading process for future use\n",
        "def get_mnist_loader(batch_size=32, num_train_samples = 6000, num_test_samples = 1000):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
        "    transforms.Normalize((0.0,), (1.0,))  # Normalize the tensor with mean 0 and standard deviation 1\n",
        "    ])\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=transform,\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=transform,\n",
        "                         download=True)\n",
        "\n",
        "    # Randomly select a subset of samples\n",
        "    train_indices = torch.randperm(len(train_dataset))[:num_train_samples]\n",
        "    test_indices = torch.randperm(len(test_dataset))[:num_test_samples]\n",
        "\n",
        "    # Create subset samplers to be used in the dataloader\n",
        "    train_subset_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    test_subset_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler = train_subset_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sampler = test_subset_sampler)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GimciNoQI73"
      },
      "source": [
        "## Build the Stacked Autoencoder\n",
        "Before building the Stacked Autoencoder, we need to build the Autoencoder layer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TNjUD_jNLDN3"
      },
      "outputs": [],
      "source": [
        "class AutoEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    fully-connected linear layers for stacked autoencoders.\n",
        "    This module can automatically be trained when training each layer is enabled\n",
        "    Yes, this is much like the simplest auto-encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=None, hidden_dim=None, SelfTraining=False):\n",
        "        super(AutoEncoderLayer, self).__init__()\n",
        "        # If input_dim is None or hidden_dim is None:\n",
        "        # raise ValueError\n",
        "        self.in_features = input_dim\n",
        "        self.out_features = hidden_dim\n",
        "        # Whether to conduct layer-by-layer pre-training, or train the entire network\n",
        "        self.is_training_self = SelfTraining\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.out_features, bias=True),\n",
        "            nn.Sigmoid()  # use Sigmoid activation function\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.out_features, self.in_features, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        #self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        if self.is_training_self:\n",
        "            return self.decoder(out) # If the layer is not in training mode, it will just encode the data and pass it through\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def lock_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def acquire_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    @property\n",
        "    def input_dim(self):\n",
        "        return self.in_features\n",
        "\n",
        "    @property\n",
        "    def output_dim(self):\n",
        "        return self.out_features\n",
        "\n",
        "    @property\n",
        "    def is_training_layer(self):\n",
        "        return self.is_training_self\n",
        "\n",
        "    @is_training_layer.setter\n",
        "    def is_training_layer(self, other: bool):\n",
        "        self.is_training_self = other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxT2upS1Aee6"
      },
      "source": [
        "## Build the stacked autoencoder\n",
        "\n",
        "Here we stack the autoencoder layers together and add a classification layer at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dw1r8BdBpO5f"
      },
      "outputs": [],
      "source": [
        "class StackedAutoEncoderClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack the trained autoencoder layers and add a classification layer at the end\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, autoencoder_list=None, num_classes=10):\n",
        "        super(StackedAutoEncoderClassifier, self).__init__()\n",
        "        # Use only the encoder parts of the autoencoders\n",
        "        self.encoder_layers = nn.ModuleList([autoencoder.encoder for autoencoder in autoencoder_list])\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Classification layer\n",
        "        self.classification_layer = nn.Linear(autoencoder_list[-1].out_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_representation = x\n",
        "        for layer in self.encoder_layers:\n",
        "            encoded_representation = layer(encoded_representation)\n",
        "\n",
        "        # Classification layer\n",
        "        output = self.classification_layer(encoded_representation)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UQL0TzMRxc3"
      },
      "source": [
        "## The training function of each layer\n",
        "We need to freeze the parameters in the previous layers when training the current layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pvZYBFsGRsmg"
      },
      "outputs": [],
      "source": [
        "def train_layer(layers_list=None, layer=None, epochs=None, validate=True, batch_size = 128):\n",
        "    \"\"\"\n",
        "    Greedy layer-wise training: when training the i-th layer, freeze the i-1 layer\n",
        "    :param layers_list:\n",
        "    :param layer:\n",
        "    :param epoch:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for model in layers_list:\n",
        "            model.cuda()\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=batch_size)\n",
        "    optimizer = optim.SGD(layers_list[layer].parameters(), lr=0.1)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Train\n",
        "    for epoch_index in range(epochs):\n",
        "        sum_loss = 0.\n",
        "\n",
        "        # Freeze the parameters of all layers before the current layer\n",
        "        # Layer 0 has no previous layer\n",
        "        if layer != 0:\n",
        "            for index in range(layer):\n",
        "                # In addition to freezing parameters\n",
        "                # the output return method of the frozen layer must also be set.\n",
        "                layers_list[index].lock_grad()\n",
        "                layers_list[index].is_training_layer = False\n",
        "\n",
        "        for batch_index, (train_data, _) in enumerate(train_loader):\n",
        "            # Generate input data\n",
        "            if torch.cuda.is_available():\n",
        "                train_data = train_data.cuda()  # Put data onto GPU\n",
        "            out = train_data.view(train_data.size(0), -1)\n",
        "\n",
        "            # Perform forward calculation on the frozen layers before (layer-1)-th layer\n",
        "            if layer != 0:\n",
        "                for l in range(layer):\n",
        "                    out = layers_list[l](out)\n",
        "\n",
        "            # Train the layer-th layer\n",
        "            pred = layers_list[layer](out)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(pred, out)\n",
        "            sum_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #if (batch_index + 1) % 20 == 0:\n",
        "        print(\"Train Layer: {}, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}\".format(\n",
        "            layer, (epoch_index + 1), epochs, (batch_index + 1), len(train_loader), loss))\n",
        "\n",
        "        if validate:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzOYc3BWx1WY"
      },
      "source": [
        "## The training function of the whole network\n",
        "Now we unfreeze all the parameters in the network that are previously frozen for the layer training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LaK2zTZd2Z5Z"
      },
      "outputs": [],
      "source": [
        "def train_classifier(model=None, epochs=20, fine_tuning = False, batch_size = 128):\n",
        "\n",
        "    print(\">> start training whole model\")\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    # unfreeze the parameters frozen in pre-training if needed\n",
        "    if fine_tuning:\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = True\n",
        "      training_phase = 'Fine-Tuning'\n",
        "    else:\n",
        "      training_phase = 'Classifier'\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=batch_size)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # train\n",
        "    for epoch_index in range(epochs):\n",
        "        sum_loss = 0.\n",
        "        for batch_index, (train_data, train_labels) in enumerate(train_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                train_data, train_labels = train_data.cuda(), train_labels.cuda()\n",
        "            x = train_data.view(train_data.size(0), -1)\n",
        "\n",
        "            predicted_labels = model(x)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(predicted_labels, train_labels)\n",
        "            sum_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train {}, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}\".format(training_phase, (epoch_index + 1), epochs, (batch_index + 1), len(train_loader), loss))\n",
        "\n",
        "    print(\"<< end training whole model\")\n",
        "    print(\"Calculating accuracy on the whole test set\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate accuracy on the test set\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_loader:\n",
        "            images, labels = batch_data\n",
        "            images = images.view(images.size(0), -1)\n",
        "            if torch.cuda.is_available():\n",
        "              images, labels = images.cuda(), labels.cuda()\n",
        "            outputs = model(images)\n",
        "            # Get the highest value for each point to get the label\n",
        "            _, predicted_labels = torch.max(outputs.data, 1)\n",
        "            # Count number of correct predictions in the batch\n",
        "            total_correct += (predicted_labels == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "\n",
        "    # Compute average test loss\n",
        "    accuracy = total_correct / total_samples\n",
        "    print('Accuracy on the test set: {:.6f}'.format(accuracy))\n",
        "    return model, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkccSfRJx4ah"
      },
      "source": [
        "## Let's start training!\n",
        "The default network is trained with layer epochs 10, classifier epochs 10, fine-tuning epochs 20, batch size of 128. You can try to change the hyper-parameters to obtain better reconstruction performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zm44Cap4T7oS"
      },
      "outputs": [],
      "source": [
        "# You can change the hyper-parameters here\n",
        "# Be sure to try out different combinations of epochs for the pretraining and fine-tuning\n",
        "num_layer_wise_epochs = 10\n",
        "num_classifier_epochs = 10\n",
        "num_finetuning_epochs = 20\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "M6N2XhTqT_-X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5688a62-1641-48cb-bfd9-7fb14fccdafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 110899319.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 91699995.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31549735.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16408724.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Train Layer: 0, Epoch: 1/10, Iter: 47/47, Loss: 0.2213\n",
            "Train Layer: 0, Epoch: 2/10, Iter: 47/47, Loss: 0.2073\n",
            "Train Layer: 0, Epoch: 3/10, Iter: 47/47, Loss: 0.1948\n",
            "Train Layer: 0, Epoch: 4/10, Iter: 47/47, Loss: 0.1827\n",
            "Train Layer: 0, Epoch: 5/10, Iter: 47/47, Loss: 0.1720\n",
            "Train Layer: 0, Epoch: 6/10, Iter: 47/47, Loss: 0.1622\n",
            "Train Layer: 0, Epoch: 7/10, Iter: 47/47, Loss: 0.1541\n",
            "Train Layer: 0, Epoch: 8/10, Iter: 47/47, Loss: 0.1456\n",
            "Train Layer: 0, Epoch: 9/10, Iter: 47/47, Loss: 0.1374\n",
            "Train Layer: 0, Epoch: 10/10, Iter: 47/47, Loss: 0.1315\n",
            "Train Layer: 1, Epoch: 1/10, Iter: 47/47, Loss: 0.0087\n",
            "Train Layer: 1, Epoch: 2/10, Iter: 47/47, Loss: 0.0081\n",
            "Train Layer: 1, Epoch: 3/10, Iter: 47/47, Loss: 0.0073\n",
            "Train Layer: 1, Epoch: 4/10, Iter: 47/47, Loss: 0.0070\n",
            "Train Layer: 1, Epoch: 5/10, Iter: 47/47, Loss: 0.0063\n",
            "Train Layer: 1, Epoch: 6/10, Iter: 47/47, Loss: 0.0061\n",
            "Train Layer: 1, Epoch: 7/10, Iter: 47/47, Loss: 0.0056\n",
            "Train Layer: 1, Epoch: 8/10, Iter: 47/47, Loss: 0.0053\n",
            "Train Layer: 1, Epoch: 9/10, Iter: 47/47, Loss: 0.0049\n",
            "Train Layer: 1, Epoch: 10/10, Iter: 47/47, Loss: 0.0048\n",
            "StackedAutoEncoderClassifier(\n",
            "  (encoder_layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (classification_layer): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            ">> start training whole model\n",
            "Train Classifier, Epoch: 1/10, Iter: 47/47, Loss: 2.3147\n",
            "Train Classifier, Epoch: 2/10, Iter: 47/47, Loss: 2.2940\n",
            "Train Classifier, Epoch: 3/10, Iter: 47/47, Loss: 2.2845\n",
            "Train Classifier, Epoch: 4/10, Iter: 47/47, Loss: 2.3051\n",
            "Train Classifier, Epoch: 5/10, Iter: 47/47, Loss: 2.2743\n",
            "Train Classifier, Epoch: 6/10, Iter: 47/47, Loss: 2.2763\n",
            "Train Classifier, Epoch: 7/10, Iter: 47/47, Loss: 2.2165\n",
            "Train Classifier, Epoch: 8/10, Iter: 47/47, Loss: 2.2358\n",
            "Train Classifier, Epoch: 9/10, Iter: 47/47, Loss: 2.1949\n",
            "Train Classifier, Epoch: 10/10, Iter: 47/47, Loss: 2.1333\n",
            "<< end training whole model\n",
            "Calculating accuracy on the whole test set\n",
            "Accuracy on the test set: 0.182000\n",
            ">> start training whole model\n",
            "Train Fine-Tuning, Epoch: 1/20, Iter: 47/47, Loss: 0.9458\n",
            "Train Fine-Tuning, Epoch: 2/20, Iter: 47/47, Loss: 0.7613\n",
            "Train Fine-Tuning, Epoch: 3/20, Iter: 47/47, Loss: 0.5902\n",
            "Train Fine-Tuning, Epoch: 4/20, Iter: 47/47, Loss: 0.3548\n",
            "Train Fine-Tuning, Epoch: 5/20, Iter: 47/47, Loss: 0.4324\n",
            "Train Fine-Tuning, Epoch: 6/20, Iter: 47/47, Loss: 0.2495\n",
            "Train Fine-Tuning, Epoch: 7/20, Iter: 47/47, Loss: 0.4429\n",
            "Train Fine-Tuning, Epoch: 8/20, Iter: 47/47, Loss: 0.2968\n",
            "Train Fine-Tuning, Epoch: 9/20, Iter: 47/47, Loss: 0.4125\n",
            "Train Fine-Tuning, Epoch: 10/20, Iter: 47/47, Loss: 0.3718\n",
            "Train Fine-Tuning, Epoch: 11/20, Iter: 47/47, Loss: 0.3412\n",
            "Train Fine-Tuning, Epoch: 12/20, Iter: 47/47, Loss: 0.3335\n",
            "Train Fine-Tuning, Epoch: 13/20, Iter: 47/47, Loss: 0.3175\n",
            "Train Fine-Tuning, Epoch: 14/20, Iter: 47/47, Loss: 0.1343\n",
            "Train Fine-Tuning, Epoch: 15/20, Iter: 47/47, Loss: 0.1399\n",
            "Train Fine-Tuning, Epoch: 16/20, Iter: 47/47, Loss: 0.1748\n",
            "Train Fine-Tuning, Epoch: 17/20, Iter: 47/47, Loss: 0.1813\n",
            "Train Fine-Tuning, Epoch: 18/20, Iter: 47/47, Loss: 0.1285\n",
            "Train Fine-Tuning, Epoch: 19/20, Iter: 47/47, Loss: 0.2275\n",
            "Train Fine-Tuning, Epoch: 20/20, Iter: 47/47, Loss: 0.1978\n",
            "<< end training whole model\n",
            "Calculating accuracy on the whole test set\n",
            "Accuracy on the test set: 0.926000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory ./models does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-47255f1e8565>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Save the model (refer: https://pytorch.org/docs/master/notes/serialization.html)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAE_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./models/sae_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./models does not exist."
          ]
        }
      ],
      "source": [
        "input_dim = 784\n",
        "# Define the autoencoder layers\n",
        "# Try different values for the dimensions of the hidden layers and the number of layers\n",
        "encoder_1 = AutoEncoderLayer(input_dim = input_dim, hidden_dim = 256, SelfTraining=True)\n",
        "encoder_2 = AutoEncoderLayer(input_dim = 256, hidden_dim = 64, SelfTraining=True)\n",
        "\n",
        "encoders_list = [encoder_1, encoder_2]\n",
        "num_layers = len(encoders_list)\n",
        "\n",
        "\n",
        "# Pre-train each layer\n",
        "for level in range(num_layers):\n",
        "   train_layer(layers_list=encoders_list, layer=level, epochs=num_layer_wise_epochs, validate=True, batch_size = batch_size)\n",
        "\n",
        "# Build the stacked autoencoder\n",
        "SAE_model = StackedAutoEncoderClassifier(autoencoder_list=encoders_list, num_classes = 10)\n",
        "# Print the model\n",
        "print(SAE_model)\n",
        "\n",
        "# First train the classification layer\n",
        "train_classifier(model=SAE_model, epochs=num_classifier_epochs, fine_tuning = False, batch_size = batch_size)\n",
        "\n",
        "# Train the whole model and perform fine-tuning\n",
        "train_classifier(model=SAE_model, epochs=num_finetuning_epochs, fine_tuning = True, batch_size = batch_size)\n",
        "\n",
        "\n",
        "# # Save the model (refer: https://pytorch.org/docs/master/notes/serialization.html)\n",
        "# torch.save(SAE_model, './models/sae_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters fine tuning"
      ],
      "metadata": {
        "id": "6oT14Ckr5iOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layer_wise_epochs = 60\n",
        "num_classifier_epochs = 60\n",
        "num_finetuning_epochs = 60\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "6ryXvCk05lCc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 784\n",
        "# Define the autoencoder layers\n",
        "# Try different values for the dimensions of the hidden layers and the number of layers\n",
        "encoder_1 = AutoEncoderLayer(input_dim = input_dim, hidden_dim = 256, SelfTraining=True)\n",
        "encoder_2 = AutoEncoderLayer(input_dim = 256, hidden_dim = 64, SelfTraining=True)\n",
        "\n",
        "encoders_list = [encoder_1, encoder_2]\n",
        "num_layers = len(encoders_list)\n",
        "\n",
        "\n",
        "# Pre-train each layer\n",
        "for level in range(num_layers):\n",
        "   train_layer(layers_list=encoders_list, layer=level, epochs=num_layer_wise_epochs, validate=True, batch_size = batch_size)\n",
        "\n",
        "# Build the stacked autoencoder\n",
        "SAE_model = StackedAutoEncoderClassifier(autoencoder_list=encoders_list, num_classes = 10)\n",
        "# Print the model\n",
        "print(SAE_model)\n",
        "\n",
        "# First train the classification layer\n",
        "train_classifier(model=SAE_model, epochs=num_classifier_epochs, fine_tuning = False, batch_size = batch_size)\n",
        "\n",
        "# Train the whole model and perform fine-tuning\n",
        "train_classifier(model=SAE_model, epochs=num_finetuning_epochs, fine_tuning = True, batch_size = batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqyG9Scn62MO",
        "outputId": "765da2c8-4b28-42af-eaf8-ced79aa59e7a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Layer: 0, Epoch: 1/60, Iter: 94/94, Loss: 0.2029\n",
            "Train Layer: 0, Epoch: 2/60, Iter: 94/94, Loss: 0.1791\n",
            "Train Layer: 0, Epoch: 3/60, Iter: 94/94, Loss: 0.1586\n",
            "Train Layer: 0, Epoch: 4/60, Iter: 94/94, Loss: 0.1429\n",
            "Train Layer: 0, Epoch: 5/60, Iter: 94/94, Loss: 0.1276\n",
            "Train Layer: 0, Epoch: 6/60, Iter: 94/94, Loss: 0.1195\n",
            "Train Layer: 0, Epoch: 7/60, Iter: 94/94, Loss: 0.1108\n",
            "Train Layer: 0, Epoch: 8/60, Iter: 94/94, Loss: 0.1054\n",
            "Train Layer: 0, Epoch: 9/60, Iter: 94/94, Loss: 0.0972\n",
            "Train Layer: 0, Epoch: 10/60, Iter: 94/94, Loss: 0.0941\n",
            "Train Layer: 0, Epoch: 11/60, Iter: 94/94, Loss: 0.0898\n",
            "Train Layer: 0, Epoch: 12/60, Iter: 94/94, Loss: 0.0844\n",
            "Train Layer: 0, Epoch: 13/60, Iter: 94/94, Loss: 0.0846\n",
            "Train Layer: 0, Epoch: 14/60, Iter: 94/94, Loss: 0.0826\n",
            "Train Layer: 0, Epoch: 15/60, Iter: 94/94, Loss: 0.0815\n",
            "Train Layer: 0, Epoch: 16/60, Iter: 94/94, Loss: 0.0764\n",
            "Train Layer: 0, Epoch: 17/60, Iter: 94/94, Loss: 0.0777\n",
            "Train Layer: 0, Epoch: 18/60, Iter: 94/94, Loss: 0.0772\n",
            "Train Layer: 0, Epoch: 19/60, Iter: 94/94, Loss: 0.0741\n",
            "Train Layer: 0, Epoch: 20/60, Iter: 94/94, Loss: 0.0789\n",
            "Train Layer: 0, Epoch: 21/60, Iter: 94/94, Loss: 0.0781\n",
            "Train Layer: 0, Epoch: 22/60, Iter: 94/94, Loss: 0.0739\n",
            "Train Layer: 0, Epoch: 23/60, Iter: 94/94, Loss: 0.0738\n",
            "Train Layer: 0, Epoch: 24/60, Iter: 94/94, Loss: 0.0739\n",
            "Train Layer: 0, Epoch: 25/60, Iter: 94/94, Loss: 0.0755\n",
            "Train Layer: 0, Epoch: 26/60, Iter: 94/94, Loss: 0.0739\n",
            "Train Layer: 0, Epoch: 27/60, Iter: 94/94, Loss: 0.0718\n",
            "Train Layer: 0, Epoch: 28/60, Iter: 94/94, Loss: 0.0707\n",
            "Train Layer: 0, Epoch: 29/60, Iter: 94/94, Loss: 0.0701\n",
            "Train Layer: 0, Epoch: 30/60, Iter: 94/94, Loss: 0.0743\n",
            "Train Layer: 0, Epoch: 31/60, Iter: 94/94, Loss: 0.0735\n",
            "Train Layer: 0, Epoch: 32/60, Iter: 94/94, Loss: 0.0721\n",
            "Train Layer: 0, Epoch: 33/60, Iter: 94/94, Loss: 0.0707\n",
            "Train Layer: 0, Epoch: 34/60, Iter: 94/94, Loss: 0.0678\n",
            "Train Layer: 0, Epoch: 35/60, Iter: 94/94, Loss: 0.0731\n",
            "Train Layer: 0, Epoch: 36/60, Iter: 94/94, Loss: 0.0742\n",
            "Train Layer: 0, Epoch: 37/60, Iter: 94/94, Loss: 0.0713\n",
            "Train Layer: 0, Epoch: 38/60, Iter: 94/94, Loss: 0.0722\n",
            "Train Layer: 0, Epoch: 39/60, Iter: 94/94, Loss: 0.0713\n",
            "Train Layer: 0, Epoch: 40/60, Iter: 94/94, Loss: 0.0694\n",
            "Train Layer: 0, Epoch: 41/60, Iter: 94/94, Loss: 0.0685\n",
            "Train Layer: 0, Epoch: 42/60, Iter: 94/94, Loss: 0.0684\n",
            "Train Layer: 0, Epoch: 43/60, Iter: 94/94, Loss: 0.0689\n",
            "Train Layer: 0, Epoch: 44/60, Iter: 94/94, Loss: 0.0689\n",
            "Train Layer: 0, Epoch: 45/60, Iter: 94/94, Loss: 0.0667\n",
            "Train Layer: 0, Epoch: 46/60, Iter: 94/94, Loss: 0.0748\n",
            "Train Layer: 0, Epoch: 47/60, Iter: 94/94, Loss: 0.0719\n",
            "Train Layer: 0, Epoch: 48/60, Iter: 94/94, Loss: 0.0666\n",
            "Train Layer: 0, Epoch: 49/60, Iter: 94/94, Loss: 0.0649\n",
            "Train Layer: 0, Epoch: 50/60, Iter: 94/94, Loss: 0.0740\n",
            "Train Layer: 0, Epoch: 51/60, Iter: 94/94, Loss: 0.0659\n",
            "Train Layer: 0, Epoch: 52/60, Iter: 94/94, Loss: 0.0672\n",
            "Train Layer: 0, Epoch: 53/60, Iter: 94/94, Loss: 0.0692\n",
            "Train Layer: 0, Epoch: 54/60, Iter: 94/94, Loss: 0.0656\n",
            "Train Layer: 0, Epoch: 55/60, Iter: 94/94, Loss: 0.0713\n",
            "Train Layer: 0, Epoch: 56/60, Iter: 94/94, Loss: 0.0698\n",
            "Train Layer: 0, Epoch: 57/60, Iter: 94/94, Loss: 0.0663\n",
            "Train Layer: 0, Epoch: 58/60, Iter: 94/94, Loss: 0.0709\n",
            "Train Layer: 0, Epoch: 59/60, Iter: 94/94, Loss: 0.0693\n",
            "Train Layer: 0, Epoch: 60/60, Iter: 94/94, Loss: 0.0679\n",
            "Train Layer: 1, Epoch: 1/60, Iter: 94/94, Loss: 0.0319\n",
            "Train Layer: 1, Epoch: 2/60, Iter: 94/94, Loss: 0.0283\n",
            "Train Layer: 1, Epoch: 3/60, Iter: 94/94, Loss: 0.0290\n",
            "Train Layer: 1, Epoch: 4/60, Iter: 94/94, Loss: 0.0210\n",
            "Train Layer: 1, Epoch: 5/60, Iter: 94/94, Loss: 0.0187\n",
            "Train Layer: 1, Epoch: 6/60, Iter: 94/94, Loss: 0.0140\n",
            "Train Layer: 1, Epoch: 7/60, Iter: 94/94, Loss: 0.0112\n",
            "Train Layer: 1, Epoch: 8/60, Iter: 94/94, Loss: 0.0111\n",
            "Train Layer: 1, Epoch: 9/60, Iter: 94/94, Loss: 0.0090\n",
            "Train Layer: 1, Epoch: 10/60, Iter: 94/94, Loss: 0.0070\n",
            "Train Layer: 1, Epoch: 11/60, Iter: 94/94, Loss: 0.0073\n",
            "Train Layer: 1, Epoch: 12/60, Iter: 94/94, Loss: 0.0057\n",
            "Train Layer: 1, Epoch: 13/60, Iter: 94/94, Loss: 0.0052\n",
            "Train Layer: 1, Epoch: 14/60, Iter: 94/94, Loss: 0.0041\n",
            "Train Layer: 1, Epoch: 15/60, Iter: 94/94, Loss: 0.0038\n",
            "Train Layer: 1, Epoch: 16/60, Iter: 94/94, Loss: 0.0036\n",
            "Train Layer: 1, Epoch: 17/60, Iter: 94/94, Loss: 0.0040\n",
            "Train Layer: 1, Epoch: 18/60, Iter: 94/94, Loss: 0.0044\n",
            "Train Layer: 1, Epoch: 19/60, Iter: 94/94, Loss: 0.0034\n",
            "Train Layer: 1, Epoch: 20/60, Iter: 94/94, Loss: 0.0032\n",
            "Train Layer: 1, Epoch: 21/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 22/60, Iter: 94/94, Loss: 0.0027\n",
            "Train Layer: 1, Epoch: 23/60, Iter: 94/94, Loss: 0.0029\n",
            "Train Layer: 1, Epoch: 24/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 25/60, Iter: 94/94, Loss: 0.0024\n",
            "Train Layer: 1, Epoch: 26/60, Iter: 94/94, Loss: 0.0032\n",
            "Train Layer: 1, Epoch: 27/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 28/60, Iter: 94/94, Loss: 0.0029\n",
            "Train Layer: 1, Epoch: 29/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 30/60, Iter: 94/94, Loss: 0.0025\n",
            "Train Layer: 1, Epoch: 31/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 32/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 33/60, Iter: 94/94, Loss: 0.0026\n",
            "Train Layer: 1, Epoch: 34/60, Iter: 94/94, Loss: 0.0024\n",
            "Train Layer: 1, Epoch: 35/60, Iter: 94/94, Loss: 0.0023\n",
            "Train Layer: 1, Epoch: 36/60, Iter: 94/94, Loss: 0.0026\n",
            "Train Layer: 1, Epoch: 37/60, Iter: 94/94, Loss: 0.0026\n",
            "Train Layer: 1, Epoch: 38/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 39/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 40/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 41/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 42/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 43/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 44/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 45/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 46/60, Iter: 94/94, Loss: 0.0025\n",
            "Train Layer: 1, Epoch: 47/60, Iter: 94/94, Loss: 0.0028\n",
            "Train Layer: 1, Epoch: 48/60, Iter: 94/94, Loss: 0.0024\n",
            "Train Layer: 1, Epoch: 49/60, Iter: 94/94, Loss: 0.0029\n",
            "Train Layer: 1, Epoch: 50/60, Iter: 94/94, Loss: 0.0025\n",
            "Train Layer: 1, Epoch: 51/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 52/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 53/60, Iter: 94/94, Loss: 0.0028\n",
            "Train Layer: 1, Epoch: 54/60, Iter: 94/94, Loss: 0.0028\n",
            "Train Layer: 1, Epoch: 55/60, Iter: 94/94, Loss: 0.0023\n",
            "Train Layer: 1, Epoch: 56/60, Iter: 94/94, Loss: 0.0023\n",
            "Train Layer: 1, Epoch: 57/60, Iter: 94/94, Loss: 0.0028\n",
            "Train Layer: 1, Epoch: 58/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 59/60, Iter: 94/94, Loss: 0.0023\n",
            "Train Layer: 1, Epoch: 60/60, Iter: 94/94, Loss: 0.0026\n",
            "StackedAutoEncoderClassifier(\n",
            "  (encoder_layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (classification_layer): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            ">> start training whole model\n",
            "Train Classifier, Epoch: 1/60, Iter: 94/94, Loss: 2.3017\n",
            "Train Classifier, Epoch: 2/60, Iter: 94/94, Loss: 2.2816\n",
            "Train Classifier, Epoch: 3/60, Iter: 94/94, Loss: 2.2804\n",
            "Train Classifier, Epoch: 4/60, Iter: 94/94, Loss: 2.2636\n",
            "Train Classifier, Epoch: 5/60, Iter: 94/94, Loss: 2.4415\n",
            "Train Classifier, Epoch: 6/60, Iter: 94/94, Loss: 2.1401\n",
            "Train Classifier, Epoch: 7/60, Iter: 94/94, Loss: 2.2031\n",
            "Train Classifier, Epoch: 8/60, Iter: 94/94, Loss: 2.2040\n",
            "Train Classifier, Epoch: 9/60, Iter: 94/94, Loss: 2.4307\n",
            "Train Classifier, Epoch: 10/60, Iter: 94/94, Loss: 2.2370\n",
            "Train Classifier, Epoch: 11/60, Iter: 94/94, Loss: 2.2500\n",
            "Train Classifier, Epoch: 12/60, Iter: 94/94, Loss: 2.1983\n",
            "Train Classifier, Epoch: 13/60, Iter: 94/94, Loss: 2.2119\n",
            "Train Classifier, Epoch: 14/60, Iter: 94/94, Loss: 2.3293\n",
            "Train Classifier, Epoch: 15/60, Iter: 94/94, Loss: 2.2075\n",
            "Train Classifier, Epoch: 16/60, Iter: 94/94, Loss: 2.1730\n",
            "Train Classifier, Epoch: 17/60, Iter: 94/94, Loss: 2.2518\n",
            "Train Classifier, Epoch: 18/60, Iter: 94/94, Loss: 2.4246\n",
            "Train Classifier, Epoch: 19/60, Iter: 94/94, Loss: 2.1566\n",
            "Train Classifier, Epoch: 20/60, Iter: 94/94, Loss: 2.1968\n",
            "Train Classifier, Epoch: 21/60, Iter: 94/94, Loss: 2.1531\n",
            "Train Classifier, Epoch: 22/60, Iter: 94/94, Loss: 2.2407\n",
            "Train Classifier, Epoch: 23/60, Iter: 94/94, Loss: 2.2910\n",
            "Train Classifier, Epoch: 24/60, Iter: 94/94, Loss: 2.2931\n",
            "Train Classifier, Epoch: 25/60, Iter: 94/94, Loss: 2.4290\n",
            "Train Classifier, Epoch: 26/60, Iter: 94/94, Loss: 2.1421\n",
            "Train Classifier, Epoch: 27/60, Iter: 94/94, Loss: 2.2859\n",
            "Train Classifier, Epoch: 28/60, Iter: 94/94, Loss: 2.2056\n",
            "Train Classifier, Epoch: 29/60, Iter: 94/94, Loss: 2.2493\n",
            "Train Classifier, Epoch: 30/60, Iter: 94/94, Loss: 2.2608\n",
            "Train Classifier, Epoch: 31/60, Iter: 94/94, Loss: 2.3077\n",
            "Train Classifier, Epoch: 32/60, Iter: 94/94, Loss: 2.1801\n",
            "Train Classifier, Epoch: 33/60, Iter: 94/94, Loss: 2.2885\n",
            "Train Classifier, Epoch: 34/60, Iter: 94/94, Loss: 2.2539\n",
            "Train Classifier, Epoch: 35/60, Iter: 94/94, Loss: 2.4187\n",
            "Train Classifier, Epoch: 36/60, Iter: 94/94, Loss: 2.2490\n",
            "Train Classifier, Epoch: 37/60, Iter: 94/94, Loss: 2.1879\n",
            "Train Classifier, Epoch: 38/60, Iter: 94/94, Loss: 2.2016\n",
            "Train Classifier, Epoch: 39/60, Iter: 94/94, Loss: 2.2729\n",
            "Train Classifier, Epoch: 40/60, Iter: 94/94, Loss: 2.2623\n",
            "Train Classifier, Epoch: 41/60, Iter: 94/94, Loss: 2.2705\n",
            "Train Classifier, Epoch: 42/60, Iter: 94/94, Loss: 2.2496\n",
            "Train Classifier, Epoch: 43/60, Iter: 94/94, Loss: 2.4235\n",
            "Train Classifier, Epoch: 44/60, Iter: 94/94, Loss: 2.1675\n",
            "Train Classifier, Epoch: 45/60, Iter: 94/94, Loss: 2.1575\n",
            "Train Classifier, Epoch: 46/60, Iter: 94/94, Loss: 2.2256\n",
            "Train Classifier, Epoch: 47/60, Iter: 94/94, Loss: 2.3003\n",
            "Train Classifier, Epoch: 48/60, Iter: 94/94, Loss: 2.2388\n",
            "Train Classifier, Epoch: 49/60, Iter: 94/94, Loss: 2.1927\n",
            "Train Classifier, Epoch: 50/60, Iter: 94/94, Loss: 2.3089\n",
            "Train Classifier, Epoch: 51/60, Iter: 94/94, Loss: 2.3242\n",
            "Train Classifier, Epoch: 52/60, Iter: 94/94, Loss: 2.1092\n",
            "Train Classifier, Epoch: 53/60, Iter: 94/94, Loss: 2.1575\n",
            "Train Classifier, Epoch: 54/60, Iter: 94/94, Loss: 2.2346\n",
            "Train Classifier, Epoch: 55/60, Iter: 94/94, Loss: 2.0113\n",
            "Train Classifier, Epoch: 56/60, Iter: 94/94, Loss: 2.2760\n",
            "Train Classifier, Epoch: 57/60, Iter: 94/94, Loss: 2.2186\n",
            "Train Classifier, Epoch: 58/60, Iter: 94/94, Loss: 2.3208\n",
            "Train Classifier, Epoch: 59/60, Iter: 94/94, Loss: 2.1910\n",
            "Train Classifier, Epoch: 60/60, Iter: 94/94, Loss: 2.1336\n",
            "<< end training whole model\n",
            "Calculating accuracy on the whole test set\n",
            "Accuracy on the test set: 0.272000\n",
            ">> start training whole model\n",
            "Train Fine-Tuning, Epoch: 1/60, Iter: 94/94, Loss: 1.2546\n",
            "Train Fine-Tuning, Epoch: 2/60, Iter: 94/94, Loss: 0.8424\n",
            "Train Fine-Tuning, Epoch: 3/60, Iter: 94/94, Loss: 0.5113\n",
            "Train Fine-Tuning, Epoch: 4/60, Iter: 94/94, Loss: 0.4170\n",
            "Train Fine-Tuning, Epoch: 5/60, Iter: 94/94, Loss: 0.3473\n",
            "Train Fine-Tuning, Epoch: 6/60, Iter: 94/94, Loss: 0.2915\n",
            "Train Fine-Tuning, Epoch: 7/60, Iter: 94/94, Loss: 0.2165\n",
            "Train Fine-Tuning, Epoch: 8/60, Iter: 94/94, Loss: 0.2360\n",
            "Train Fine-Tuning, Epoch: 9/60, Iter: 94/94, Loss: 0.3411\n",
            "Train Fine-Tuning, Epoch: 10/60, Iter: 94/94, Loss: 0.3371\n",
            "Train Fine-Tuning, Epoch: 11/60, Iter: 94/94, Loss: 0.1464\n",
            "Train Fine-Tuning, Epoch: 12/60, Iter: 94/94, Loss: 0.1601\n",
            "Train Fine-Tuning, Epoch: 13/60, Iter: 94/94, Loss: 0.0867\n",
            "Train Fine-Tuning, Epoch: 14/60, Iter: 94/94, Loss: 0.1208\n",
            "Train Fine-Tuning, Epoch: 15/60, Iter: 94/94, Loss: 0.0929\n",
            "Train Fine-Tuning, Epoch: 16/60, Iter: 94/94, Loss: 0.1653\n",
            "Train Fine-Tuning, Epoch: 17/60, Iter: 94/94, Loss: 0.1665\n",
            "Train Fine-Tuning, Epoch: 18/60, Iter: 94/94, Loss: 0.1016\n",
            "Train Fine-Tuning, Epoch: 19/60, Iter: 94/94, Loss: 0.0623\n",
            "Train Fine-Tuning, Epoch: 20/60, Iter: 94/94, Loss: 0.1139\n",
            "Train Fine-Tuning, Epoch: 21/60, Iter: 94/94, Loss: 0.0627\n",
            "Train Fine-Tuning, Epoch: 22/60, Iter: 94/94, Loss: 0.1058\n",
            "Train Fine-Tuning, Epoch: 23/60, Iter: 94/94, Loss: 0.0649\n",
            "Train Fine-Tuning, Epoch: 24/60, Iter: 94/94, Loss: 0.0969\n",
            "Train Fine-Tuning, Epoch: 25/60, Iter: 94/94, Loss: 0.0153\n",
            "Train Fine-Tuning, Epoch: 26/60, Iter: 94/94, Loss: 0.0299\n",
            "Train Fine-Tuning, Epoch: 27/60, Iter: 94/94, Loss: 0.0636\n",
            "Train Fine-Tuning, Epoch: 28/60, Iter: 94/94, Loss: 0.0193\n",
            "Train Fine-Tuning, Epoch: 29/60, Iter: 94/94, Loss: 0.0170\n",
            "Train Fine-Tuning, Epoch: 30/60, Iter: 94/94, Loss: 0.1814\n",
            "Train Fine-Tuning, Epoch: 31/60, Iter: 94/94, Loss: 0.0117\n",
            "Train Fine-Tuning, Epoch: 32/60, Iter: 94/94, Loss: 0.0201\n",
            "Train Fine-Tuning, Epoch: 33/60, Iter: 94/94, Loss: 0.0143\n",
            "Train Fine-Tuning, Epoch: 34/60, Iter: 94/94, Loss: 0.0205\n",
            "Train Fine-Tuning, Epoch: 35/60, Iter: 94/94, Loss: 0.0092\n",
            "Train Fine-Tuning, Epoch: 36/60, Iter: 94/94, Loss: 0.0181\n",
            "Train Fine-Tuning, Epoch: 37/60, Iter: 94/94, Loss: 0.0116\n",
            "Train Fine-Tuning, Epoch: 38/60, Iter: 94/94, Loss: 0.0601\n",
            "Train Fine-Tuning, Epoch: 39/60, Iter: 94/94, Loss: 0.0152\n",
            "Train Fine-Tuning, Epoch: 40/60, Iter: 94/94, Loss: 0.0222\n",
            "Train Fine-Tuning, Epoch: 41/60, Iter: 94/94, Loss: 0.0085\n",
            "Train Fine-Tuning, Epoch: 42/60, Iter: 94/94, Loss: 0.0155\n",
            "Train Fine-Tuning, Epoch: 43/60, Iter: 94/94, Loss: 0.0034\n",
            "Train Fine-Tuning, Epoch: 44/60, Iter: 94/94, Loss: 0.0060\n",
            "Train Fine-Tuning, Epoch: 45/60, Iter: 94/94, Loss: 0.0070\n",
            "Train Fine-Tuning, Epoch: 46/60, Iter: 94/94, Loss: 0.0096\n",
            "Train Fine-Tuning, Epoch: 47/60, Iter: 94/94, Loss: 0.0290\n",
            "Train Fine-Tuning, Epoch: 48/60, Iter: 94/94, Loss: 0.0067\n",
            "Train Fine-Tuning, Epoch: 49/60, Iter: 94/94, Loss: 0.0021\n",
            "Train Fine-Tuning, Epoch: 50/60, Iter: 94/94, Loss: 0.0078\n",
            "Train Fine-Tuning, Epoch: 51/60, Iter: 94/94, Loss: 0.0058\n",
            "Train Fine-Tuning, Epoch: 52/60, Iter: 94/94, Loss: 0.0058\n",
            "Train Fine-Tuning, Epoch: 53/60, Iter: 94/94, Loss: 0.0087\n",
            "Train Fine-Tuning, Epoch: 54/60, Iter: 94/94, Loss: 0.0047\n",
            "Train Fine-Tuning, Epoch: 55/60, Iter: 94/94, Loss: 0.0069\n",
            "Train Fine-Tuning, Epoch: 56/60, Iter: 94/94, Loss: 0.0211\n",
            "Train Fine-Tuning, Epoch: 57/60, Iter: 94/94, Loss: 0.0041\n",
            "Train Fine-Tuning, Epoch: 58/60, Iter: 94/94, Loss: 0.0253\n",
            "Train Fine-Tuning, Epoch: 59/60, Iter: 94/94, Loss: 0.0022\n",
            "Train Fine-Tuning, Epoch: 60/60, Iter: 94/94, Loss: 0.0017\n",
            "<< end training whole model\n",
            "Calculating accuracy on the whole test set\n",
            "Accuracy on the test set: 0.949000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(StackedAutoEncoderClassifier(\n",
              "   (encoder_layers): ModuleList(\n",
              "     (0): Sequential(\n",
              "       (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "       (1): Sigmoid()\n",
              "     )\n",
              "     (1): Sequential(\n",
              "       (0): Linear(in_features=256, out_features=64, bias=True)\n",
              "       (1): Sigmoid()\n",
              "     )\n",
              "   )\n",
              "   (classification_layer): Linear(in_features=64, out_features=10, bias=True)\n",
              " ),\n",
              " 0.949)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layer_wise_epochs = 60\n",
        "num_classifier_epochs = 60\n",
        "num_finetuning_epochs = 60\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "SNKnK8gt659-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 784\n",
        "# Define the autoencoder layers\n",
        "# Try different values for the dimensions of the hidden layers and the number of layers\n",
        "encoder_1 = AutoEncoderLayer(input_dim = input_dim, hidden_dim = 128, SelfTraining=True)\n",
        "encoder_2 = AutoEncoderLayer(input_dim = 128, hidden_dim = 64, SelfTraining=True)\n",
        "\n",
        "encoders_list = [encoder_1, encoder_2]\n",
        "num_layers = len(encoders_list)\n",
        "\n",
        "\n",
        "# Pre-train each layer\n",
        "for level in range(num_layers):\n",
        "   train_layer(layers_list=encoders_list, layer=level, epochs=num_layer_wise_epochs, validate=True, batch_size = batch_size)\n",
        "\n",
        "# Build the stacked autoencoder\n",
        "SAE_model = StackedAutoEncoderClassifier(autoencoder_list=encoders_list, num_classes = 10)\n",
        "# Print the model\n",
        "print(SAE_model)\n",
        "\n",
        "# First train the classification layer\n",
        "train_classifier(model=SAE_model, epochs=num_classifier_epochs, fine_tuning = False, batch_size = batch_size)\n",
        "\n",
        "# Train the whole model and perform fine-tuning\n",
        "train_classifier(model=SAE_model, epochs=num_finetuning_epochs, fine_tuning = True, batch_size = batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFH1Orro807f",
        "outputId": "f24fb735-8cf3-40bf-a040-394fb79b7980"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Layer: 0, Epoch: 1/60, Iter: 94/94, Loss: 0.2192\n",
            "Train Layer: 0, Epoch: 2/60, Iter: 94/94, Loss: 0.2048\n",
            "Train Layer: 0, Epoch: 3/60, Iter: 94/94, Loss: 0.1906\n",
            "Train Layer: 0, Epoch: 4/60, Iter: 94/94, Loss: 0.1789\n",
            "Train Layer: 0, Epoch: 5/60, Iter: 94/94, Loss: 0.1661\n",
            "Train Layer: 0, Epoch: 6/60, Iter: 94/94, Loss: 0.1550\n",
            "Train Layer: 0, Epoch: 7/60, Iter: 94/94, Loss: 0.1443\n",
            "Train Layer: 0, Epoch: 8/60, Iter: 94/94, Loss: 0.1357\n",
            "Train Layer: 0, Epoch: 9/60, Iter: 94/94, Loss: 0.1272\n",
            "Train Layer: 0, Epoch: 10/60, Iter: 94/94, Loss: 0.1186\n",
            "Train Layer: 0, Epoch: 11/60, Iter: 94/94, Loss: 0.1119\n",
            "Train Layer: 0, Epoch: 12/60, Iter: 94/94, Loss: 0.1080\n",
            "Train Layer: 0, Epoch: 13/60, Iter: 94/94, Loss: 0.1021\n",
            "Train Layer: 0, Epoch: 14/60, Iter: 94/94, Loss: 0.1004\n",
            "Train Layer: 0, Epoch: 15/60, Iter: 94/94, Loss: 0.0947\n",
            "Train Layer: 0, Epoch: 16/60, Iter: 94/94, Loss: 0.0915\n",
            "Train Layer: 0, Epoch: 17/60, Iter: 94/94, Loss: 0.0907\n",
            "Train Layer: 0, Epoch: 18/60, Iter: 94/94, Loss: 0.0875\n",
            "Train Layer: 0, Epoch: 19/60, Iter: 94/94, Loss: 0.0863\n",
            "Train Layer: 0, Epoch: 20/60, Iter: 94/94, Loss: 0.0824\n",
            "Train Layer: 0, Epoch: 21/60, Iter: 94/94, Loss: 0.0866\n",
            "Train Layer: 0, Epoch: 22/60, Iter: 94/94, Loss: 0.0803\n",
            "Train Layer: 0, Epoch: 23/60, Iter: 94/94, Loss: 0.0816\n",
            "Train Layer: 0, Epoch: 24/60, Iter: 94/94, Loss: 0.0805\n",
            "Train Layer: 0, Epoch: 25/60, Iter: 94/94, Loss: 0.0780\n",
            "Train Layer: 0, Epoch: 26/60, Iter: 94/94, Loss: 0.0786\n",
            "Train Layer: 0, Epoch: 27/60, Iter: 94/94, Loss: 0.0793\n",
            "Train Layer: 0, Epoch: 28/60, Iter: 94/94, Loss: 0.0740\n",
            "Train Layer: 0, Epoch: 29/60, Iter: 94/94, Loss: 0.0743\n",
            "Train Layer: 0, Epoch: 30/60, Iter: 94/94, Loss: 0.0736\n",
            "Train Layer: 0, Epoch: 31/60, Iter: 94/94, Loss: 0.0760\n",
            "Train Layer: 0, Epoch: 32/60, Iter: 94/94, Loss: 0.0762\n",
            "Train Layer: 0, Epoch: 33/60, Iter: 94/94, Loss: 0.0717\n",
            "Train Layer: 0, Epoch: 34/60, Iter: 94/94, Loss: 0.0747\n",
            "Train Layer: 0, Epoch: 35/60, Iter: 94/94, Loss: 0.0727\n",
            "Train Layer: 0, Epoch: 36/60, Iter: 94/94, Loss: 0.0709\n",
            "Train Layer: 0, Epoch: 37/60, Iter: 94/94, Loss: 0.0754\n",
            "Train Layer: 0, Epoch: 38/60, Iter: 94/94, Loss: 0.0734\n",
            "Train Layer: 0, Epoch: 39/60, Iter: 94/94, Loss: 0.0753\n",
            "Train Layer: 0, Epoch: 40/60, Iter: 94/94, Loss: 0.0734\n",
            "Train Layer: 0, Epoch: 41/60, Iter: 94/94, Loss: 0.0684\n",
            "Train Layer: 0, Epoch: 42/60, Iter: 94/94, Loss: 0.0735\n",
            "Train Layer: 0, Epoch: 43/60, Iter: 94/94, Loss: 0.0737\n",
            "Train Layer: 0, Epoch: 44/60, Iter: 94/94, Loss: 0.0730\n",
            "Train Layer: 0, Epoch: 45/60, Iter: 94/94, Loss: 0.0704\n",
            "Train Layer: 0, Epoch: 46/60, Iter: 94/94, Loss: 0.0683\n",
            "Train Layer: 0, Epoch: 47/60, Iter: 94/94, Loss: 0.0745\n",
            "Train Layer: 0, Epoch: 48/60, Iter: 94/94, Loss: 0.0708\n",
            "Train Layer: 0, Epoch: 49/60, Iter: 94/94, Loss: 0.0716\n",
            "Train Layer: 0, Epoch: 50/60, Iter: 94/94, Loss: 0.0678\n",
            "Train Layer: 0, Epoch: 51/60, Iter: 94/94, Loss: 0.0754\n",
            "Train Layer: 0, Epoch: 52/60, Iter: 94/94, Loss: 0.0723\n",
            "Train Layer: 0, Epoch: 53/60, Iter: 94/94, Loss: 0.0676\n",
            "Train Layer: 0, Epoch: 54/60, Iter: 94/94, Loss: 0.0696\n",
            "Train Layer: 0, Epoch: 55/60, Iter: 94/94, Loss: 0.0705\n",
            "Train Layer: 0, Epoch: 56/60, Iter: 94/94, Loss: 0.0727\n",
            "Train Layer: 0, Epoch: 57/60, Iter: 94/94, Loss: 0.0667\n",
            "Train Layer: 0, Epoch: 58/60, Iter: 94/94, Loss: 0.0696\n",
            "Train Layer: 0, Epoch: 59/60, Iter: 94/94, Loss: 0.0719\n",
            "Train Layer: 0, Epoch: 60/60, Iter: 94/94, Loss: 0.0721\n",
            "Train Layer: 1, Epoch: 1/60, Iter: 94/94, Loss: 0.0670\n",
            "Train Layer: 1, Epoch: 2/60, Iter: 94/94, Loss: 0.0455\n",
            "Train Layer: 1, Epoch: 3/60, Iter: 94/94, Loss: 0.0279\n",
            "Train Layer: 1, Epoch: 4/60, Iter: 94/94, Loss: 0.0232\n",
            "Train Layer: 1, Epoch: 5/60, Iter: 94/94, Loss: 0.0141\n",
            "Train Layer: 1, Epoch: 6/60, Iter: 94/94, Loss: 0.0100\n",
            "Train Layer: 1, Epoch: 7/60, Iter: 94/94, Loss: 0.0075\n",
            "Train Layer: 1, Epoch: 8/60, Iter: 94/94, Loss: 0.0062\n",
            "Train Layer: 1, Epoch: 9/60, Iter: 94/94, Loss: 0.0042\n",
            "Train Layer: 1, Epoch: 10/60, Iter: 94/94, Loss: 0.0041\n",
            "Train Layer: 1, Epoch: 11/60, Iter: 94/94, Loss: 0.0045\n",
            "Train Layer: 1, Epoch: 12/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 13/60, Iter: 94/94, Loss: 0.0040\n",
            "Train Layer: 1, Epoch: 14/60, Iter: 94/94, Loss: 0.0039\n",
            "Train Layer: 1, Epoch: 15/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 16/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 17/60, Iter: 94/94, Loss: 0.0037\n",
            "Train Layer: 1, Epoch: 18/60, Iter: 94/94, Loss: 0.0028\n",
            "Train Layer: 1, Epoch: 19/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 20/60, Iter: 94/94, Loss: 0.0032\n",
            "Train Layer: 1, Epoch: 21/60, Iter: 94/94, Loss: 0.0039\n",
            "Train Layer: 1, Epoch: 22/60, Iter: 94/94, Loss: 0.0041\n",
            "Train Layer: 1, Epoch: 23/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 24/60, Iter: 94/94, Loss: 0.0039\n",
            "Train Layer: 1, Epoch: 25/60, Iter: 94/94, Loss: 0.0037\n",
            "Train Layer: 1, Epoch: 26/60, Iter: 94/94, Loss: 0.0032\n",
            "Train Layer: 1, Epoch: 27/60, Iter: 94/94, Loss: 0.0038\n",
            "Train Layer: 1, Epoch: 28/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 29/60, Iter: 94/94, Loss: 0.0039\n",
            "Train Layer: 1, Epoch: 30/60, Iter: 94/94, Loss: 0.0029\n",
            "Train Layer: 1, Epoch: 31/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 32/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 33/60, Iter: 94/94, Loss: 0.0036\n",
            "Train Layer: 1, Epoch: 34/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 35/60, Iter: 94/94, Loss: 0.0028\n",
            "Train Layer: 1, Epoch: 36/60, Iter: 94/94, Loss: 0.0032\n",
            "Train Layer: 1, Epoch: 37/60, Iter: 94/94, Loss: 0.0029\n",
            "Train Layer: 1, Epoch: 38/60, Iter: 94/94, Loss: 0.0034\n",
            "Train Layer: 1, Epoch: 39/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 40/60, Iter: 94/94, Loss: 0.0037\n",
            "Train Layer: 1, Epoch: 41/60, Iter: 94/94, Loss: 0.0027\n",
            "Train Layer: 1, Epoch: 42/60, Iter: 94/94, Loss: 0.0036\n",
            "Train Layer: 1, Epoch: 43/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 44/60, Iter: 94/94, Loss: 0.0023\n",
            "Train Layer: 1, Epoch: 45/60, Iter: 94/94, Loss: 0.0038\n",
            "Train Layer: 1, Epoch: 46/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 47/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 48/60, Iter: 94/94, Loss: 0.0034\n",
            "Train Layer: 1, Epoch: 49/60, Iter: 94/94, Loss: 0.0027\n",
            "Train Layer: 1, Epoch: 50/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 51/60, Iter: 94/94, Loss: 0.0026\n",
            "Train Layer: 1, Epoch: 52/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 53/60, Iter: 94/94, Loss: 0.0033\n",
            "Train Layer: 1, Epoch: 54/60, Iter: 94/94, Loss: 0.0030\n",
            "Train Layer: 1, Epoch: 55/60, Iter: 94/94, Loss: 0.0035\n",
            "Train Layer: 1, Epoch: 56/60, Iter: 94/94, Loss: 0.0027\n",
            "Train Layer: 1, Epoch: 57/60, Iter: 94/94, Loss: 0.0031\n",
            "Train Layer: 1, Epoch: 58/60, Iter: 94/94, Loss: 0.0027\n",
            "Train Layer: 1, Epoch: 59/60, Iter: 94/94, Loss: 0.0041\n",
            "Train Layer: 1, Epoch: 60/60, Iter: 94/94, Loss: 0.0033\n",
            "StackedAutoEncoderClassifier(\n",
            "  (encoder_layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (1): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (classification_layer): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            ">> start training whole model\n",
            "Train Classifier, Epoch: 1/60, Iter: 94/94, Loss: 2.2737\n",
            "Train Classifier, Epoch: 2/60, Iter: 94/94, Loss: 2.2914\n",
            "Train Classifier, Epoch: 3/60, Iter: 94/94, Loss: 2.2814\n",
            "Train Classifier, Epoch: 4/60, Iter: 94/94, Loss: 2.2673\n",
            "Train Classifier, Epoch: 5/60, Iter: 94/94, Loss: 2.2876\n",
            "Train Classifier, Epoch: 6/60, Iter: 94/94, Loss: 2.2776\n",
            "Train Classifier, Epoch: 7/60, Iter: 94/94, Loss: 2.1893\n",
            "Train Classifier, Epoch: 8/60, Iter: 94/94, Loss: 2.2656\n",
            "Train Classifier, Epoch: 9/60, Iter: 94/94, Loss: 2.2477\n",
            "Train Classifier, Epoch: 10/60, Iter: 94/94, Loss: 2.1959\n",
            "Train Classifier, Epoch: 11/60, Iter: 94/94, Loss: 2.1873\n",
            "Train Classifier, Epoch: 12/60, Iter: 94/94, Loss: 2.1854\n",
            "Train Classifier, Epoch: 13/60, Iter: 94/94, Loss: 2.2244\n",
            "Train Classifier, Epoch: 14/60, Iter: 94/94, Loss: 2.1991\n",
            "Train Classifier, Epoch: 15/60, Iter: 94/94, Loss: 2.2415\n",
            "Train Classifier, Epoch: 16/60, Iter: 94/94, Loss: 2.1811\n",
            "Train Classifier, Epoch: 17/60, Iter: 94/94, Loss: 2.2513\n",
            "Train Classifier, Epoch: 18/60, Iter: 94/94, Loss: 2.2459\n",
            "Train Classifier, Epoch: 19/60, Iter: 94/94, Loss: 2.2330\n",
            "Train Classifier, Epoch: 20/60, Iter: 94/94, Loss: 2.1287\n",
            "Train Classifier, Epoch: 21/60, Iter: 94/94, Loss: 2.2622\n",
            "Train Classifier, Epoch: 22/60, Iter: 94/94, Loss: 2.2778\n",
            "Train Classifier, Epoch: 23/60, Iter: 94/94, Loss: 2.3094\n",
            "Train Classifier, Epoch: 24/60, Iter: 94/94, Loss: 2.2589\n",
            "Train Classifier, Epoch: 25/60, Iter: 94/94, Loss: 2.2751\n",
            "Train Classifier, Epoch: 26/60, Iter: 94/94, Loss: 2.2977\n",
            "Train Classifier, Epoch: 27/60, Iter: 94/94, Loss: 2.2637\n",
            "Train Classifier, Epoch: 28/60, Iter: 94/94, Loss: 2.2864\n",
            "Train Classifier, Epoch: 29/60, Iter: 94/94, Loss: 2.4899\n",
            "Train Classifier, Epoch: 30/60, Iter: 94/94, Loss: 2.2546\n",
            "Train Classifier, Epoch: 31/60, Iter: 94/94, Loss: 2.1603\n",
            "Train Classifier, Epoch: 32/60, Iter: 94/94, Loss: 2.3204\n",
            "Train Classifier, Epoch: 33/60, Iter: 94/94, Loss: 2.2624\n",
            "Train Classifier, Epoch: 34/60, Iter: 94/94, Loss: 2.2744\n",
            "Train Classifier, Epoch: 35/60, Iter: 94/94, Loss: 2.1203\n",
            "Train Classifier, Epoch: 36/60, Iter: 94/94, Loss: 2.2775\n",
            "Train Classifier, Epoch: 37/60, Iter: 94/94, Loss: 2.3210\n",
            "Train Classifier, Epoch: 38/60, Iter: 94/94, Loss: 2.2982\n",
            "Train Classifier, Epoch: 39/60, Iter: 94/94, Loss: 2.3159\n",
            "Train Classifier, Epoch: 40/60, Iter: 94/94, Loss: 2.2630\n",
            "Train Classifier, Epoch: 41/60, Iter: 94/94, Loss: 2.2664\n",
            "Train Classifier, Epoch: 42/60, Iter: 94/94, Loss: 2.3152\n",
            "Train Classifier, Epoch: 43/60, Iter: 94/94, Loss: 2.3072\n",
            "Train Classifier, Epoch: 44/60, Iter: 94/94, Loss: 2.2104\n",
            "Train Classifier, Epoch: 45/60, Iter: 94/94, Loss: 2.2918\n",
            "Train Classifier, Epoch: 46/60, Iter: 94/94, Loss: 2.2389\n",
            "Train Classifier, Epoch: 47/60, Iter: 94/94, Loss: 2.2143\n",
            "Train Classifier, Epoch: 48/60, Iter: 94/94, Loss: 2.4644\n",
            "Train Classifier, Epoch: 49/60, Iter: 94/94, Loss: 2.2468\n",
            "Train Classifier, Epoch: 50/60, Iter: 94/94, Loss: 2.3523\n",
            "Train Classifier, Epoch: 51/60, Iter: 94/94, Loss: 2.3015\n",
            "Train Classifier, Epoch: 52/60, Iter: 94/94, Loss: 2.2822\n",
            "Train Classifier, Epoch: 53/60, Iter: 94/94, Loss: 2.2395\n",
            "Train Classifier, Epoch: 54/60, Iter: 94/94, Loss: 2.1970\n",
            "Train Classifier, Epoch: 55/60, Iter: 94/94, Loss: 2.2122\n",
            "Train Classifier, Epoch: 56/60, Iter: 94/94, Loss: 2.2762\n",
            "Train Classifier, Epoch: 57/60, Iter: 94/94, Loss: 2.3243\n",
            "Train Classifier, Epoch: 58/60, Iter: 94/94, Loss: 2.3438\n",
            "Train Classifier, Epoch: 59/60, Iter: 94/94, Loss: 2.2524\n",
            "Train Classifier, Epoch: 60/60, Iter: 94/94, Loss: 2.3389\n",
            "<< end training whole model\n",
            "Calculating accuracy on the whole test set\n",
            "Accuracy on the test set: 0.119000\n",
            ">> start training whole model\n",
            "Train Fine-Tuning, Epoch: 1/60, Iter: 94/94, Loss: 1.2668\n",
            "Train Fine-Tuning, Epoch: 2/60, Iter: 94/94, Loss: 1.2017\n",
            "Train Fine-Tuning, Epoch: 3/60, Iter: 94/94, Loss: 0.8597\n",
            "Train Fine-Tuning, Epoch: 4/60, Iter: 94/94, Loss: 0.6571\n",
            "Train Fine-Tuning, Epoch: 5/60, Iter: 94/94, Loss: 0.6608\n",
            "Train Fine-Tuning, Epoch: 6/60, Iter: 94/94, Loss: 0.4295\n",
            "Train Fine-Tuning, Epoch: 7/60, Iter: 94/94, Loss: 0.5443\n",
            "Train Fine-Tuning, Epoch: 8/60, Iter: 94/94, Loss: 0.2570\n",
            "Train Fine-Tuning, Epoch: 9/60, Iter: 94/94, Loss: 0.3453\n",
            "Train Fine-Tuning, Epoch: 10/60, Iter: 94/94, Loss: 0.2790\n",
            "Train Fine-Tuning, Epoch: 11/60, Iter: 94/94, Loss: 0.2548\n",
            "Train Fine-Tuning, Epoch: 12/60, Iter: 94/94, Loss: 0.1072\n",
            "Train Fine-Tuning, Epoch: 13/60, Iter: 94/94, Loss: 0.1761\n",
            "Train Fine-Tuning, Epoch: 14/60, Iter: 94/94, Loss: 0.1131\n",
            "Train Fine-Tuning, Epoch: 15/60, Iter: 94/94, Loss: 0.1405\n",
            "Train Fine-Tuning, Epoch: 16/60, Iter: 94/94, Loss: 0.1960\n",
            "Train Fine-Tuning, Epoch: 17/60, Iter: 94/94, Loss: 0.1788\n",
            "Train Fine-Tuning, Epoch: 18/60, Iter: 94/94, Loss: 0.0959\n",
            "Train Fine-Tuning, Epoch: 19/60, Iter: 94/94, Loss: 0.1207\n",
            "Train Fine-Tuning, Epoch: 20/60, Iter: 94/94, Loss: 0.1547\n",
            "Train Fine-Tuning, Epoch: 21/60, Iter: 94/94, Loss: 0.0431\n",
            "Train Fine-Tuning, Epoch: 22/60, Iter: 94/94, Loss: 0.0361\n",
            "Train Fine-Tuning, Epoch: 23/60, Iter: 94/94, Loss: 0.0268\n",
            "Train Fine-Tuning, Epoch: 24/60, Iter: 94/94, Loss: 0.0424\n",
            "Train Fine-Tuning, Epoch: 25/60, Iter: 94/94, Loss: 0.0203\n",
            "Train Fine-Tuning, Epoch: 26/60, Iter: 94/94, Loss: 0.0243\n",
            "Train Fine-Tuning, Epoch: 27/60, Iter: 94/94, Loss: 0.0324\n",
            "Train Fine-Tuning, Epoch: 28/60, Iter: 94/94, Loss: 0.0280\n",
            "Train Fine-Tuning, Epoch: 29/60, Iter: 94/94, Loss: 0.0245\n",
            "Train Fine-Tuning, Epoch: 30/60, Iter: 94/94, Loss: 0.0268\n",
            "Train Fine-Tuning, Epoch: 31/60, Iter: 94/94, Loss: 0.0196\n",
            "Train Fine-Tuning, Epoch: 32/60, Iter: 94/94, Loss: 0.0179\n",
            "Train Fine-Tuning, Epoch: 33/60, Iter: 94/94, Loss: 0.0117\n",
            "Train Fine-Tuning, Epoch: 34/60, Iter: 94/94, Loss: 0.1238\n",
            "Train Fine-Tuning, Epoch: 35/60, Iter: 94/94, Loss: 0.0110\n",
            "Train Fine-Tuning, Epoch: 36/60, Iter: 94/94, Loss: 0.0167\n",
            "Train Fine-Tuning, Epoch: 37/60, Iter: 94/94, Loss: 0.0108\n",
            "Train Fine-Tuning, Epoch: 38/60, Iter: 94/94, Loss: 0.0077\n",
            "Train Fine-Tuning, Epoch: 39/60, Iter: 94/94, Loss: 0.0176\n",
            "Train Fine-Tuning, Epoch: 40/60, Iter: 94/94, Loss: 0.0093\n",
            "Train Fine-Tuning, Epoch: 41/60, Iter: 94/94, Loss: 0.0054\n",
            "Train Fine-Tuning, Epoch: 42/60, Iter: 94/94, Loss: 0.0752\n",
            "Train Fine-Tuning, Epoch: 43/60, Iter: 94/94, Loss: 0.0134\n",
            "Train Fine-Tuning, Epoch: 44/60, Iter: 94/94, Loss: 0.0102\n",
            "Train Fine-Tuning, Epoch: 45/60, Iter: 94/94, Loss: 0.0055\n",
            "Train Fine-Tuning, Epoch: 46/60, Iter: 94/94, Loss: 0.0075\n",
            "Train Fine-Tuning, Epoch: 47/60, Iter: 94/94, Loss: 0.0048\n",
            "Train Fine-Tuning, Epoch: 48/60, Iter: 94/94, Loss: 0.0044\n",
            "Train Fine-Tuning, Epoch: 49/60, Iter: 94/94, Loss: 0.0032\n",
            "Train Fine-Tuning, Epoch: 50/60, Iter: 94/94, Loss: 0.0050\n",
            "Train Fine-Tuning, Epoch: 51/60, Iter: 94/94, Loss: 0.0088\n",
            "Train Fine-Tuning, Epoch: 52/60, Iter: 94/94, Loss: 0.0087\n",
            "Train Fine-Tuning, Epoch: 53/60, Iter: 94/94, Loss: 0.0067\n",
            "Train Fine-Tuning, Epoch: 54/60, Iter: 94/94, Loss: 0.0040\n",
            "Train Fine-Tuning, Epoch: 55/60, Iter: 94/94, Loss: 0.0029\n",
            "Train Fine-Tuning, Epoch: 56/60, Iter: 94/94, Loss: 0.0044\n",
            "Train Fine-Tuning, Epoch: 57/60, Iter: 94/94, Loss: 0.0067\n",
            "Train Fine-Tuning, Epoch: 58/60, Iter: 94/94, Loss: 0.0020\n",
            "Train Fine-Tuning, Epoch: 59/60, Iter: 94/94, Loss: 0.0059\n",
            "Train Fine-Tuning, Epoch: 60/60, Iter: 94/94, Loss: 0.0886\n",
            "<< end training whole model\n",
            "Calculating accuracy on the whole test set\n",
            "Accuracy on the test set: 0.954000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(StackedAutoEncoderClassifier(\n",
              "   (encoder_layers): ModuleList(\n",
              "     (0): Sequential(\n",
              "       (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "       (1): Sigmoid()\n",
              "     )\n",
              "     (1): Sequential(\n",
              "       (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "       (1): Sigmoid()\n",
              "     )\n",
              "   )\n",
              "   (classification_layer): Linear(in_features=64, out_features=10, bias=True)\n",
              " ),\n",
              " 0.954)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RXfo0Ln87Ob"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}