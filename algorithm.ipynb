{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Key Techniques and Areas to Master**\n",
    "\n",
    "1. **Image Processing Fundamentals:**\n",
    "   - **Filtering and Transformations:** Understand spatial filtering (e.g., Gaussian, median filtering), frequency domain techniques (Fourier transform), and edge detection (Sobel, Canny).\n",
    "   - **Image Enhancement:** Techniques for improving image quality, including contrast adjustment, histogram equalization, and denoising.\n",
    "   - **Image Restoration:** Knowledge of deblurring, noise reduction, and techniques to reverse the effects of degradation.\n",
    "\n",
    "2. **Computer Vision Algorithms:**\n",
    "   - **Feature Extraction:** Techniques like SIFT, SURF, and ORB for identifying and matching keypoints in images.\n",
    "   - **Object Detection and Recognition:** Learning about algorithms like YOLO, SSD, and R-CNN for identifying and classifying objects.\n",
    "   - **Image Segmentation:** Methods such as thresholding, region-growing, and advanced techniques like U-Net for partitioning images into meaningful segments.\n",
    "\n",
    "3. **Machine Learning & Deep Learning:**\n",
    "   - **Convolutional Neural Networks (CNNs):** Deep learning models designed for image classification, detection, and segmentation tasks.\n",
    "   - **Generative Models:** GANs and VAEs for tasks like image generation and super-resolution.\n",
    "   - **Training and Fine-Tuning Models:** Experience with frameworks like TensorFlow, PyTorch, and using pre-trained models for specific tasks.\n",
    "\n",
    "4. **Remote Sensing and Geospatial Analysis:**\n",
    "   - **Multispectral and Hyperspectral Imaging:** Understanding how different wavelengths of light are captured and processed.\n",
    "   - **Image Fusion:** Techniques for combining data from multiple sources to enhance image quality or information content.\n",
    "   - **Georeferencing and Mapping:** Aligning images to geographical coordinates and working with geospatial data.\n",
    "\n",
    "5. **Optics and Image Formation:**\n",
    "   - **Camera Models:** Understanding pinhole and lens models, distortion correction, and calibration.\n",
    "   - **Point Spread Function (PSF):** Understanding how optical systems blur points of light and how this affects image quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **Point Spread Function (PSF)**\n",
    "\n",
    "The **Point Spread Function (PSF)** is a fundamental concept in optics and imaging systems, describing how a single point of light (a \"point source\") is rendered or spread out in an image. \n",
    "\n",
    "- **Concept:** In an ideal optical system, a point source of light would be imaged as a single point. However, due to imperfections in the lens, diffraction, and other factors, the point is spread out into a small blur, typically with a characteristic pattern.\n",
    "\n",
    "- **Significance:** The PSF characterizes the imaging system's resolution and blur. By understanding or estimating the PSF, you can:\n",
    "  - **Deblur images:** Using techniques like deconvolution to reverse the blurring effect.\n",
    "  - **Improve resolution:** Enhance image quality by compensating for known distortions.\n",
    "  - **Design optical systems:** Optimize the design of lenses and sensors to minimize blur and other aberrations.\n",
    "\n",
    "- **Mathematical Representation:** The PSF is often represented as a 2D function, where the intensity of the blur at each point in the image plane is calculated. It's also closely related to the **Optical Transfer Function (OTF)**, which describes the system's response in the frequency domain.\n",
    "\n",
    "### **1. Image Preprocessing:**\n",
    "- **Image Filtering:** Techniques like Gaussian filtering, median filtering, and bilateral filtering are used to remove noise and enhance important features.\n",
    "- **Histogram Equalization:** Improves the contrast of an image by spreading out the most frequent intensity values.\n",
    "- **Edge Detection:** Algorithms like Sobel, Canny, and Laplacian are used to detect edges, which are critical for understanding object boundaries in images.\n",
    "- **Morphological Operations:** Techniques like dilation, erosion, opening, and closing are used for processing binary images, particularly in tasks like segmentation and feature extraction.\n",
    "\n",
    "### **2. Feature Detection and Description:**\n",
    "- **Corner and Edge Detectors:** Harris, Shi-Tomasi, and Canny detectors are used for identifying key points in an image.\n",
    "- **SIFT (Scale-Invariant Feature Transform):** Extracts distinctive key points from images, useful for object recognition and image stitching.\n",
    "- **SURF (Speeded-Up Robust Features):** Similar to SIFT but optimized for faster computation, used in real-time applications.\n",
    "- **ORB (Oriented FAST and Rotated BRIEF):** A more efficient alternative to SIFT and SURF, often used in real-time systems.\n",
    "\n",
    "### **3. Image Segmentation:**\n",
    "- **Thresholding:** Simple techniques like Otsu’s method to separate objects from the background based on pixel intensity.\n",
    "- **Watershed Algorithm:** Used for separating touching objects in an image, particularly in microscopy and medical imaging.\n",
    "- **Active Contours (Snakes):** Used for detecting object boundaries by evolving contours based on image gradients.\n",
    "- **Region Growing:** Segmentation technique that groups pixels or subregions into larger regions based on predefined criteria.\n",
    "- **Graph Cuts:** Used for energy minimization-based segmentation, often applied in medical imaging and object detection.\n",
    "\n",
    "### **4. Object Detection and Recognition:**\n",
    "- **HOG (Histogram of Oriented Gradients):** A feature descriptor used for object detection, especially in human detection.\n",
    "- **Viola-Jones Algorithm:** A machine learning approach for real-time object detection, particularly faces.\n",
    "- **YOLO (You Only Look Once) and SSD (Single Shot Multibox Detector):** State-of-the-art deep learning models for real-time object detection.\n",
    "- **R-CNN and Variants (Fast R-CNN, Faster R-CNN, Mask R-CNN):** Popular deep learning frameworks for object detection and instance segmentation.\n",
    "\n",
    "### **5. Image Registration and Alignment:**\n",
    "- **Homography:** Used to align images from different viewpoints, critical in applications like image stitching and panoramic creation.\n",
    "- **Optical Flow:** Techniques like Farneback or Lucas-Kanade methods to track motion between frames in a video sequence.\n",
    "- **Image Warping:** Used to correct geometric distortions in images, often applied in remote sensing and medical imaging.\n",
    "\n",
    "### **6. Depth Estimation and 3D Reconstruction:**\n",
    "- **Stereo Vision:** Uses two or more images from different viewpoints to estimate depth by matching corresponding points.\n",
    "- **Structure from Motion (SfM):** A technique that reconstructs 3D structures from a sequence of images, used in photogrammetry and 3D modeling.\n",
    "- **SLAM (Simultaneous Localization and Mapping):** Used in robotics and AR/VR, SLAM algorithms map an environment while tracking the camera's location.\n",
    "\n",
    "### **7. Machine Learning and Deep Learning Techniques:**\n",
    "- **Convolutional Neural Networks (CNNs):** Fundamental in image classification, object detection, and segmentation tasks. Experience with architectures like ResNet, VGG, and Inception is valuable.\n",
    "- **Autoencoders:** Used for unsupervised learning, particularly in denoising images and anomaly detection.\n",
    "- **Generative Adversarial Networks (GANs):** Used for generating new image data and for tasks like super-resolution and inpainting.\n",
    "- **Transfer Learning:** Leveraging pre-trained models for specific tasks, reducing the need for large labeled datasets.\n",
    "- **Reinforcement Learning:** Although less common in vision tasks, it’s increasingly applied in scenarios like robotic vision and control.\n",
    "\n",
    "### **8. Image Fusion and Super-Resolution:**\n",
    "- **Image Fusion:** Combining multiple images from different sensors or viewpoints to produce a single image with enhanced information.\n",
    "- **Super-Resolution:** Techniques that reconstruct high-resolution images from low-resolution inputs, often using deep learning methods like GANs.\n",
    "\n",
    "### **9. Statistical and Mathematical Methods:**\n",
    "- **Principal Component Analysis (PCA):** Used for dimensionality reduction and feature extraction.\n",
    "- **Fourier Transform:** Used in frequency domain analysis, important for filtering and image enhancement.\n",
    "- **Wavelet Transform:** Applied for multi-resolution analysis in image compression and denoising.\n",
    "\n",
    "### **10. Image Understanding and Interpretation:**\n",
    "- **Scene Understanding:** Using image data to interpret complex scenes, often involving semantic segmentation and scene parsing.\n",
    "- **Pose Estimation:** Determining the orientation of objects, particularly in applications like augmented reality and robotics.\n",
    "- **Facial Recognition and Expression Analysis:** Techniques for identifying and analyzing facial features, widely used in security and marketing.\n",
    "\n",
    "### **11. Imaging System Understanding:**\n",
    "- **Calibration:** Techniques to correct for lens distortions and sensor inaccuracies, important in precise measurement tasks.\n",
    "- **Point Spread Function (PSF) Analysis:** Understanding the PSF of an imaging system is crucial for deblurring and image restoration tasks.\n",
    "- **Radiometric Calibration:** Adjusting images to correct for varying sensor responses, particularly in multispectral and hyperspectral imaging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Image preprocessing**\n",
    "### **Edge Detection**\n",
    "Feature detection and engineering involve identifying and extracting key characteristics from images, which can be used for tasks such as object detection, recognition, and image segmentation. Below are key methods, including their steps and mathematical models:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Canny Edge Detection**\n",
    "   * **Canny math**\n",
    "   The Canny edge detection algorithm involves several mathematical steps:\n",
    "\n",
    "   1. **Smoothing (Gaussian Blur):** Convolve the image with a Gaussian filter to reduce noise.\n",
    "      $$ G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-(x^2 + y^2) / (2\\sigma^2)} $$\n",
    "\n",
    "   2. **Intensity Gradient Calculation:** Compute the gradient of the smoothed image to find the intensity changes.\n",
    "      $$ G_x = \\frac{\\partial G}{\\partial x}, \\quad G_y = \\frac{\\partial G}{\\partial y} $$\n",
    "      $$ \\text{Gradient Magnitude (M)} = \\sqrt{G_x^2 + G_y^2} $$\n",
    "      $$ \\text{Gradient Direction (θ)} = \\arctan\\left(\\frac{G_y}{G_x}\\right) $$\n",
    "\n",
    "   3. **Non-maximum Suppression:** Suppress non-maximum values to thin edges.\n",
    "      Compare the gradient magnitude with neighboring pixels along the gradient direction.\n",
    "\n",
    "   4. **Edge Tracking by Hysteresis Dual Thresholds:** Use two thresholds to determine strong, weak, and non-relevant edges.\n",
    "      - Pixels with intensity gradient above a high threshold are considered strong edges.\n",
    "      - Pixels with intensity gradient below a low threshold are ignored.\n",
    "      - Pixels with intensity gradient between the low and high thresholds are considered weak edges if they connect to strong edges.\n",
    "\n",
    "   These mathematical operations help highlight and connect the edges in an image effectively.\n",
    "\n",
    "**Purpose:** The Canny edge detector is a multi-stage algorithm used to detect a wide range of edges in images. It’s particularly known for its ability to detect edges with a low error rate and precise localization.\n",
    "\n",
    "**Steps and Mathematical Models:**\n",
    "\n",
    "1. **Gaussian Blurring:**\n",
    "   - **Purpose:** Reduce noise in the image to prevent false edge detection.\n",
    "   - **Mathematics:**\n",
    "     $$\n",
    "     G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}}\n",
    "     $$\n",
    "     - Convolve the image $ I(x, y) $ with the Gaussian kernel $ G(x, y) $ to produce a smoothed image $ I_s(x, y) $:\n",
    "     $$\n",
    "     I_s(x, y) = I(x, y) * G(x, y)\n",
    "     $$\n",
    "\n",
    "2. **Gradient Calculation:**\n",
    "   - **Purpose:** Identify the intensity gradient of the image.\n",
    "   - **Mathematics:**\n",
    "     - Compute gradients using Sobel filters:\n",
    "     $$\n",
    "     G_x = \\frac{\\partial I_s}{\\partial x} \\quad \\text{and} \\quad G_y = \\frac{\\partial I_s}{\\partial y}\n",
    "     $$\n",
    "     - Magnitude and direction of the gradient:\n",
    "     $$\n",
    "     G = \\sqrt{G_x^2 + G_y^2}\n",
    "     $$\n",
    "     $$\n",
    "     \\theta = \\tan^{-1}\\left(\\frac{G_y}{G_x}\\right)\n",
    "     $$\n",
    "\n",
    "3. **Non-Maximum Suppression:**\n",
    "   - **Purpose:** Thin the edges to produce a binary image where edges are one pixel thick.\n",
    "   - **Mathematics:** Suppress any pixel value that is not a local maximum along the direction of the gradient $ \\theta $.\n",
    "\n",
    "4. **Double Thresholding:**\n",
    "   - **Purpose:** Determine potential edges by applying high and low thresholds.\n",
    "   - **Mathematics:**\n",
    "     - Strong edges: $ G > T_{high} $\n",
    "     - Weak edges: $ T_{low} < G \\leq T_{high} $\n",
    "     - Non-edges: $ G \\leq T_{low} $\n",
    "\n",
    "5. **Edge Tracking by Hysteresis:**\n",
    "   - **Purpose:** Finalize edge detection by suppressing all weak edges that are not connected to strong edges.\n",
    "\n",
    "**Use Case:** Canny is widely used in various applications such as object detection, image segmentation, and video processing, where accurate edge detection is crucial.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Sobel Operator**\n",
    "\n",
    "**Purpose:** The Sobel operator is a discrete differentiation operator used to compute the gradient of the image intensity function, highlighting regions of high spatial frequency that correspond to edges.\n",
    "\n",
    "**Steps and Mathematical Models:**\n",
    "\n",
    "1. **Gradient Calculation:**\n",
    "   - **Purpose:** Detect edges by calculating the gradient of image intensity at each pixel.\n",
    "   - **Mathematics:**\n",
    "     - Sobel kernels for horizontal ($ G_x $) and vertical ($ G_y $) gradients:\n",
    "     $$\n",
    "     G_x = \\begin{pmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{pmatrix}\n",
    "     \\quad \\text{and} \\quad\n",
    "     G_y = \\begin{pmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{pmatrix}\n",
    "     $$\n",
    "     - Convolution with image $ I(x, y) $:\n",
    "     $$\n",
    "     G_x(x, y) = I(x, y) * G_x \\quad \\text{and} \\quad G_y(x, y) = I(x, y) * G_y\n",
    "     $$\n",
    "     - Gradient magnitude and direction:\n",
    "     $$\n",
    "     G(x, y) = \\sqrt{G_x(x, y)^2 + G_y(x, y)^2}\n",
    "     $$\n",
    "     $$\n",
    "     \\theta(x, y) = \\tan^{-1}\\left(\\frac{G_y(x, y)}{G_x(x, y)}\\right)\n",
    "     $$\n",
    "\n",
    "**Use Case:** The Sobel operator is commonly used for edge detection in image processing, particularly as a preliminary step before more advanced techniques like Canny or Hough transformation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Hough Transformation**\n",
    "\n",
    "**Purpose:** The Hough transformation is a feature extraction technique used to detect geometric shapes, such as lines, circles, and ellipses, in images. It is especially effective for detecting shapes in edge-detected images.\n",
    "\n",
    "**Steps and Mathematical Models:**\n",
    "\n",
    "1. **Parameter Space Representation:**\n",
    "   - **Purpose:** Represent possible shapes in a parameter space where each point corresponds to a possible shape in the image space.\n",
    "   - **Mathematics:**\n",
    "     - **For lines:** Use the parametric representation of a line:\n",
    "     $$\n",
    "     \\rho = x \\cos\\theta + y \\sin\\theta\n",
    "     $$\n",
    "     where $ \\rho $ is the perpendicular distance from the origin to the line, and $ \\theta $ is the angle of the normal to the line.\n",
    "     - For every edge point $ (x, y) $, compute $ \\rho $ for a range of $ \\theta $ values and plot $ (\\rho, \\theta) $ in the Hough space.\n",
    "\n",
    "2. **Voting in the Accumulator Array:**\n",
    "   - **Purpose:** For each edge point, increment votes in the parameter space (Hough space) for each potential line passing through that point.\n",
    "   - **Mathematics:** The accumulator array is updated as:\n",
    "     $$\n",
    "     A(\\rho, \\theta) += 1\n",
    "     $$\n",
    "     where $ A(\\rho, \\theta) $ counts how many edge points support the line with parameters $ (\\rho, \\theta) $.\n",
    "\n",
    "3. **Peak Detection:**\n",
    "   - **Purpose:** Identify the most voted $ (\\rho, \\theta) $ pairs in the accumulator array, which correspond to the most likely lines in the image.\n",
    "   - **Mathematics:** Detect peaks in the accumulator array to determine the dominant lines.\n",
    "\n",
    "**Use Case:** The Hough transformation is widely used in applications like lane detection in autonomous driving, detecting circular objects like coins, and recognizing various shapes in industrial vision systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **Canny Edge Detection:** Used for precise edge detection, particularly in scenarios requiring accurate and noise-resistant edge localization.\n",
    "- **Sobel Operator:** Primarily used for detecting edges by calculating the gradient of image intensity, often as a preprocessing step.\n",
    "- **Hough Transformation:** Employed for detecting geometric shapes like lines and circles, particularly after edge detection steps.\n",
    "\n",
    "These techniques are fundamental in feature detection and engineering, forming the backbone of many computer vision applications, from simple edge detection to complex shape recognition. Understanding their mathematical models and how they are applied in practice is crucial for effective implementation in real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **Fitlering**\n",
    "Certainly! The Laplacian filter is a second-order derivative filter used in image processing to highlight regions of rapid intensity change (such as edges). It operates by calculating the Laplacian, which is the sum of second-order derivatives in the image.\n",
    "\n",
    "Below is a Python implementation of the Laplacian filter without using OpenCV or other external libraries, except for NumPy.\n",
    "\n",
    "#### **Laplacian Filter Implementation**\n",
    "\n",
    "1. **Create the Laplacian Kernel:**\n",
    "   - The most common kernel used for the Laplacian filter is a 3x3 kernel that looks like this:\n",
    "\n",
    "   $$\n",
    "   \\text{Laplacian Kernel} = \\begin{pmatrix} \n",
    "   0 & 1 & 0 \\\\\n",
    "   1 & -4 & 1 \\\\\n",
    "   0 & 1 & 0 \n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   - Another common variation of the Laplacian kernel is:\n",
    "\n",
    "   $$\n",
    "   \\text{Laplacian Kernel} = \\begin{pmatrix} \n",
    "   1 & 1 & 1 \\\\\n",
    "   1 & -8 & 1 \\\\\n",
    "   1 & 1 & 1 \n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   Both kernels can be used depending on the desired sensitivity of the filter.\n",
    "\n",
    "2. **Apply the Laplacian Filter to the Image:**\n",
    "   - The Laplacian filter is applied using convolution, where the kernel is slid over the image, and for each position, the pixel values are multiplied by the corresponding kernel values and summed to produce the output pixel.\n",
    "\n",
    "\n",
    "#### **Bilateral filtering**\n",
    "\n",
    "Bilateral filtering is a non-linear filtering technique used in image processing and computer vision to smooth images while preserving edges and important details. It's particularly effective in reducing noise while maintaining the structural characteristics of the image. Bilateral filtering is based on both spatial and intensity information, making it suitable for a variety of applications.\n",
    "\n",
    "Here's a simplified explanation of bilateral filtering:\n",
    "\n",
    "1. **Spatial Domain:**\n",
    "   - The filter considers the spatial distance between pixels. Closer pixels in terms of spatial coordinates are given more weight than those farther away.\n",
    "\n",
    "2. **Intensity Domain:**\n",
    "   - The filter also takes into account the difference in intensity values between pixels. Similar intensity values receive higher weights, preserving edges and details.\n",
    "\n",
    "3. **Filter Function:**\n",
    "   - The bilateral filter is defined by a function that combines the spatial and intensity components. For a pixel $I(x, y)$ in an image $I$, the filtered value $I'(x, y)$ is computed as follows:\n",
    "\n",
    "   $$ I'(x, y) = \\frac{1}{W_p} \\sum_{(i, j) \\in \\text{window}} I(i, j) \\cdot w_s(i, j, x, y) \\cdot w_r(I(i, j), I(x, y)) $$\n",
    "\n",
    "   where:\n",
    "   - $W_p$ is the normalization term,\n",
    "   - $w_s$ is the spatial weight function,\n",
    "   - $w_r$ is the range (intensity) weight function.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - $W_p$ is a normalization factor to ensure that the sum of the weights is equal to 1.\n",
    "\n",
    "5. **Parameters:**\n",
    "   - Bilateral filtering has two key parameters:\n",
    "      - $ \\sigma_s $ (spatial standard deviation): Controls the spatial extent of the filter.\n",
    "      - $ \\sigma_r $ (range standard deviation): Controls the range of intensities considered similar.\n",
    "\n",
    "The bilateral filter is computationally efficient, and it's widely used in various applications, including image denoising, tone mapping, and edge-preserving smoothing. Its ability to smooth images while preserving edges makes it valuable in scenarios where maintaining structural details is crucial.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dimension Reduction**\n",
    "### **Principal Component Analysis (PCA)**\n",
    "\n",
    "**Purpose:**  \n",
    "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while preserving as much variance (information) as possible. It achieves this by identifying the principal components, which are the directions (in the feature space) along which the variance of the data is maximized. PCA is widely used in feature extraction, data compression, noise reduction, and visualization.\n",
    "\n",
    "### **Steps and Mathematical Models**\n",
    "\n",
    "1. **Standardize the Data:**\n",
    "   - **Purpose:** Standardizing the data ensures that each feature contributes equally to the analysis. It is important when features have different units or scales.\n",
    "   - **Mathematics:** \n",
    "     - Given a dataset $ X $ with $ n $ samples and $ p $ features, the standardized data $ Z $ is computed as:\n",
    "       $$\n",
    "       Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
    "       $$\n",
    "       where $ \\mu_j $ is the mean of the $ j $-th feature, and $ \\sigma_j $ is the standard deviation of the $ j $-th feature.\n",
    "\n",
    "2. **Compute the Covariance Matrix:**\n",
    "   - **Purpose:** The covariance matrix captures the relationships (correlations) between different features.\n",
    "   - **Mathematics:**\n",
    "     - The covariance matrix $ \\Sigma $ for the standardized data $ Z $ is given by:\n",
    "       $$\n",
    "       \\Sigma = \\frac{1}{n-1} Z^T Z\n",
    "       $$\n",
    "       where $ Z^T $ is the transpose of the standardized data matrix $ Z $.\n",
    "\n",
    "3. **Compute the Eigenvalues and Eigenvectors:**\n",
    "   - **Purpose:** Eigenvalues and eigenvectors of the covariance matrix identify the principal components and their corresponding variances.\n",
    "   - **Mathematics:**\n",
    "     - Solve the eigenvalue equation:\n",
    "       $$\n",
    "       \\Sigma \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "       $$\n",
    "       where $ \\lambda $ is an eigenvalue and $ \\mathbf{v} $ is the corresponding eigenvector.\n",
    "     - The eigenvectors $ \\mathbf{v} $ are the directions of the principal components, and the eigenvalues $ \\lambda $ represent the variance captured by each principal component.\n",
    "\n",
    "4. **Sort Eigenvalues and Select Principal Components:**\n",
    "   - **Purpose:** Order the eigenvectors by decreasing eigenvalues, selecting the top $ k $ eigenvectors as the principal components.\n",
    "   - **Mathematics:**\n",
    "     - Sort the eigenvalues $ \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p $ and their corresponding eigenvectors.\n",
    "     - Select the top $ k $ eigenvectors $ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k $, where $ k $ is the desired number of dimensions for the reduced dataset.\n",
    "\n",
    "5. **Transform the Data:**\n",
    "   - **Purpose:** Project the original data onto the selected principal components, thus reducing the dimensionality.\n",
    "   - **Mathematics:**\n",
    "     - The transformed data $ Y $ is computed as:\n",
    "       $$\n",
    "       Y = Z \\mathbf{W}\n",
    "       $$\n",
    "       where $ \\mathbf{W} $ is the matrix of selected eigenvectors $ \\mathbf{W} = [\\mathbf{v}_1 \\, \\mathbf{v}_2 \\, \\ldots \\, \\mathbf{v}_k] $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Cases**\n",
    "\n",
    "1. **Dimensionality Reduction:**  \n",
    "   PCA is often used to reduce the number of features in a dataset while retaining the most important information. This is particularly useful when dealing with high-dimensional data, as it can help mitigate the curse of dimensionality, improve computational efficiency, and reduce overfitting in machine learning models.\n",
    "\n",
    "2. **Noise Reduction:**  \n",
    "   By projecting data onto the principal components with the largest variance, PCA can effectively filter out noise, which is often associated with components that have low variance.\n",
    "\n",
    "3. **Data Visualization:**  \n",
    "   PCA is frequently used to visualize high-dimensional data in 2D or 3D by projecting the data onto the first two or three principal components. This helps in exploring data patterns and detecting outliers.\n",
    "\n",
    "4. **Feature Extraction:**  \n",
    "   PCA can be used as a feature extraction technique, where the principal components are used as new features for machine learning models. These components are often more informative than the original features.\n",
    "\n",
    "5. **Preprocessing for Clustering and Classification:**  \n",
    "   Before applying clustering algorithms (e.g., K-means) or classification models, PCA is often used to preprocess the data, reducing its dimensionality and removing noise.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Intuition Behind PCA**\n",
    "\n",
    "- **Variance Maximization:**  \n",
    "  PCA identifies directions (principal components) along which the variance of the data is maximized. This is done by projecting the data onto these directions, which are orthogonal to each other.\n",
    "\n",
    "- **Orthogonality:**  \n",
    "  The principal components are orthogonal (uncorrelated), meaning that each principal component captures a unique portion of the variance in the data.\n",
    "\n",
    "- **Dimensionality Reduction:**  \n",
    "  By selecting the top $ k $ principal components, PCA reduces the number of dimensions while retaining the majority of the variance, simplifying the data without losing significant information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "PCA is a powerful tool for dimensionality reduction, noise reduction, and feature extraction in various data-driven applications. By transforming the data into a new set of orthogonal features (principal components), PCA helps simplify complex datasets, making them easier to analyze and model. Understanding the mathematical foundations of PCA allows for effective implementation and interpretation of its results in real-world scenarios.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Core Algorithms and Principles in Image Processing 图像分类、目标检测和图像分割的核心算法和原理列表**\n",
    "### 1. **图像分类（Image Classification）**\n",
    "图像分类是指将输入图像分类到特定类别的任务。核心算法包括：\n",
    "\n",
    "- **传统方法：**\n",
    "  - **SIFT（Scale-Invariant Feature Transform）：** 用于提取图像的局部特征点，并进行匹配和分类。 \n",
    "  - **HOG（Histogram of Oriented Gradients）：** 通过统计图像局部区域中的梯度方向来捕获物体的形状特征。\n",
    "  - **支持向量机（SVM）：** 基于特征向量进行分类的监督学习算法，传统上与SIFT或HOG结合使用。\n",
    "\n",
    "- **深度学习方法：**\n",
    "  - **LeNet-5:** 最早的卷积神经网络（CNN）之一，主要用于手写数字识别。它奠定了CNN的基本结构。\n",
    "  - **AlexNet:** 引入了更深的网络层数以及ReLU激活函数，使得深度学习在图像分类任务中取得突破。\n",
    "  - **VGGNet:** 使用更小的卷积核（3x3），但层数更深，提高了分类精度。VGG16和VGG19是其中的常见版本。\n",
    "  - **ResNet（Residual Network）：** 通过引入残差块（Residual Block），解决了深度网络中的梯度消失问题。ResNet50、ResNet101是常用的版本。\n",
    "  - **Inception Net（GoogLeNet）：** 使用多尺度卷积核在同一层中提取特征，并通过降低计算量使得模型更高效。\n",
    "  - **MobileNet:** 轻量级CNN，适用于移动设备，通过深度可分离卷积大幅减少参数量。\n",
    "\n",
    "### 2. **目标检测（Object Detection）**\n",
    "目标检测不仅要分类，还要定位物体的位置。核心算法包括：\n",
    "\n",
    "- **传统方法：**\n",
    "  - **Haar Cascade:** 基于Haar特征的快速目标检测算法，常用于人脸检测。\n",
    "  - **DPM（Deformable Parts Model）：** 将物体分解为多个可变形的部分，通过这些部分的组合来检测目标。\n",
    "\n",
    "- **深度学习方法：**\n",
    "  - **R-CNN（Region-based CNN）：** 通过选择性搜索生成候选区域，然后使用CNN对每个区域进行分类。\n",
    "  - **Fast R-CNN:** 在R-CNN的基础上进行了改进，通过共享卷积层大幅加快了检测速度。\n",
    "  - **Faster R-CNN:** 引入区域建议网络（RPN），进一步加快了目标检测过程。\n",
    "  - **YOLO（You Only Look Once）：** 将目标检测视为回归问题，通过单次前向传递同时完成分类和定位任务。YOLOv3、YOLOv4是常用版本。\n",
    "  - **SSD（Single Shot Multibox Detector）：** 采用多个尺度和特征图进行检测，兼顾速度和精度。\n",
    "  - **RetinaNet:** 引入Focal Loss来处理类别不平衡问题，改善了单阶段检测器的性能。\n",
    "\n",
    "### 3. **图像分割（Image Segmentation）**\n",
    "图像分割是将图像中的每个像素分类到特定的类别。核心算法包括：\n",
    "\n",
    "- **传统方法：**\n",
    "  - **K-means聚类：** 基于像素的颜色、位置等特征，将图像分割成不同的区域。\n",
    "  - **Graph Cut:** 将图像建模为图形，通过最小割找到最优分割。\n",
    "  - **GrabCut:** 结合Graph Cut和GMM（高斯混合模型），用户通过交互式标记图像区域进行分割。\n",
    "\n",
    "- **深度学习方法：**\n",
    "  - **FCN（Fully Convolutional Network）：** 将传统的CNN全连接层替换为卷积层，实现了对任意大小图像的像素级分类。\n",
    "  - **U-Net:** 基于编码器-解码器结构，广泛应用于医学图像分割。通过skip connection保持高分辨率特征。\n",
    "  - **SegNet:** 采用编码器-解码器架构，并使用最大池化索引来恢复分辨率，减少了参数量。\n",
    "  - **DeepLab系列：** 使用空洞卷积（Atrous Convolution）和条件随机场（CRF）进行高精度的语义分割。常见版本包括DeepLabv3和DeepLabv3+。\n",
    "  - **Mask R-CNN:** 在Faster R-CNN的基础上增加了一个分支，用于生成每个目标的像素级别的分割掩码。\n",
    "\n",
    "---\n",
    "\n",
    "## **Image segmentation classical techniques and algorithms**\n",
    "\n",
    "Classical image segmentation techniques encompass a variety of methods that aim to partition an image into meaningful regions or objects. Here are some notable classical segmentation algorithms:\n",
    "\n",
    "1. **Thresholding:**\n",
    "   - **Algorithm:** Simple and intuitive, it assigns pixels to different segments based on predefined intensity thresholds.\n",
    "   - **Use Case:** Effective for images with well-defined intensity differences.\n",
    "\n",
    "2. **Region Growing:**\n",
    "   - **Algorithm:** Seeds are iteratively grown by adding neighboring pixels that satisfy certain criteria (e.g., intensity similarity).\n",
    "   - **Use Case:** Suitable for images with distinct regions and homogeneous properties.\n",
    "\n",
    "3. **Edge-Based Segmentation:**\n",
    "   - **Algorithm:** Detects edges in an image using techniques like gradient-based edge detection (e.g., Sobel, Canny), and then groups pixels based on edge information.\n",
    "   - **Use Case:** Commonly used when boundaries between objects are prominent.\n",
    "\n",
    "4. **Watershed Transform:**\n",
    "   - **Algorithm:** Treats the image as a topographic surface and simulates flooding to identify catchment basins and watershed lines.\n",
    "   - **Use Case:** Effective for segmenting images with pronounced intensity differences.\n",
    "\n",
    "5. **K-Means Clustering:**\n",
    "   - **Algorithm:** Groups pixels into clusters based on similarity in feature space, such as color or texture.\n",
    "   - **Use Case:** Useful for images with distinct color regions.\n",
    "\n",
    "6. **Graph-Based Segmentation:**\n",
    "   - **Algorithm:** Constructs a graph representation of the image and partitions it into disjoint regions using techniques like normalized cut.\n",
    "   - **Use Case:** Suitable for images with complex structures and varying object sizes.\n",
    "\n",
    "7. **Active Contours (Snakes):**\n",
    "   - **Algorithm:** Defines a contour that evolves to minimize an energy function based on image features.\n",
    "   - **Use Case:** Effective for segmenting objects with distinct boundaries.\n",
    "\n",
    "8. **Mean Shift Segmentation:**\n",
    "   - **Algorithm:** Shifts data points in feature space towards the mode of the data distribution to group similar points.\n",
    "   - **Use Case:** Suitable for images with varying textures and color distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **传统图像分类的数学模型**\n",
    "### 1. **SIFT（Scale-Invariant Feature Transform）**\n",
    "SIFT 是一种用于提取图像局部特征的方法。主要步骤包括关键点检测、特征描述符生成以及特征匹配。以下是相关的数学公式和概念：\n",
    "A scale-space pyramid is a multi-scale representation of an image obtained by repeatedly applying Gaussian blurring and down-sampling. The purpose is to analyze an image at different scales to capture features of varying sizes. \n",
    "   - Create a scale-space pyramid by repeatedly applying Gaussian blurring at different scales to the input image.\n",
    "   - Compute the Difference of Gaussians (DoG) by subtracting adjacent blurred images in the pyramid.\n",
    "   - Identify local extrema in the DoG pyramid to find potential keypoint locations.\n",
    "\n",
    "\n",
    "3. **Orientation Assignment:**\n",
    "   - Assign a consistent orientation to each keypoint based on local image gradients.\n",
    "   - Construct a histogram of gradient orientations in the region around the keypoint.\n",
    "   - The dominant orientation in the histogram is assigned to the keypoint.\n",
    "   Here's how the canonical orientation is determined:\n",
    "\n",
    "   1. **Gradient Computation:**\n",
    "      - Compute the gradient magnitude ($M$) and orientation ($\\theta$) for each pixel in the local neighborhood around the keypoint.\n",
    "\n",
    "   2. **Histogram Accumulation:**\n",
    "      - Construct a histogram of gradient orientations in the local region. The histogram is usually divided into bins representing different orientation ranges.\n",
    "\n",
    "   3. **Dominant Orientation:**\n",
    "      - Identify the peak or peaks in the histogram to determine the dominant orientation(s).\n",
    "      - The dominant orientation is selected as the canonical orientation for the keypoint.\n",
    "\n",
    "   4. **Descriptor Rotation:**\n",
    "      - Rotate the keypoint descriptor based on the canonical orientation. This rotation ensures that the SIFT descriptor is invariant to changes in the keypoint's orientation.\n",
    "\n",
    "   By aligning keypoints based on their dominant orientations, SIFT ensures that the descriptors are rotationally invariant, making them robust to image transformations such as rotation. This contributes to the algorithm's effectiveness in matching keypoints across different views of the same scene.\n",
    "\n",
    "\n",
    "4. **Keypoint Descriptor:**\n",
    "   - Define a local image patch around each keypoint and transform it into a canonical orientation.\n",
    "   - Divide the patch into smaller sub-regions and compute gradient magnitudes and orientations.\n",
    "   - Create a descriptor vector by concatenating these gradient values, forming a representation invariant to rotation and scale changes.\n",
    "\n",
    "5. **Matching:**\n",
    "   - Compare descriptors between keypoints in different images to find correspondences.\n",
    "   - Use a distance metric (e.g., Euclidean distance) to measure the similarity between descriptors.\n",
    "#### **关键点检测：**\n",
    "1. **Scale-Space Extrema Detection:**\n",
    "\n",
    "   1. **高斯模糊 Gaussian Blurring:**\n",
    "      - Start with the original image $I$.\n",
    "      - Apply a series of Gaussian filters with increasing standard deviations to obtain blurred images at different scales.\n",
    "\n",
    "      $$ L(x, y, \\sigma) = G(x, y, \\sigma) \\ast I(x, y) $$\n",
    "\n",
    "      where $G(x, y, \\sigma)$ is the 2D Gaussian function with standard deviation $\\sigma$, and $\\ast$ denotes convolution.\n",
    "\n",
    "   2. **Down-sampling:**\n",
    "      - After each level in the pyramid, down-sample the image to create an image with reduced resolution.\n",
    "      - The down-sampling can be achieved by taking every nth pixel in both dimensions.\n",
    "\n",
    "   3. **Repeat:**\n",
    "      - Repeat the process to create a series of blurred and down-sampled images, forming the scale-space pyramid.\n",
    "\n",
    "2. **Keypoint Localization:**\n",
    "   - Refine keypoint locations by fitting a 3D quadratic function to the nearby samples in the DoG pyramid.\n",
    "   - Eliminate unstable keypoints based on their contrast and edge responses.\n",
    "   1. **Extrema Detection in Scale-Space:**\n",
    "      - Identify potential keypoints as local extrema in the Difference of Gaussians (DoG) scale-space pyramid.\n",
    "      - **尺度空间极值检测：** 通过多尺度的高斯差分（Difference of Gaussians, DoG）计算图像中的极值点。\n",
    "      $$\n",
    "      D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)\n",
    "      $$\n",
    "      其中，$ k $ 是一个常数倍率因子，通常取值为 $ \\sqrt{2} $。\n",
    "\n",
    "   2. **方向赋值：**\n",
    "    - **梯度计算：** 对每个关键点，计算其周围像素的梯度幅值和方向。\n",
    "      $$\n",
    "      m(x, y) = \\sqrt{(L(x+1, y) - L(x-1, y))^2 + (L(x, y+1) - L(x, y-1))^2}\n",
    "      $$\n",
    "      $$\n",
    "      \\theta(x, y) = \\tan^{-1} \\left(\\frac{L(x, y+1) - L(x, y-1)}{L(x+1, y) - L(x-1, y)}\\right)\n",
    "      $$\n",
    "\n",
    "   3. **Refinement using a 3D Quadratic Function:**\n",
    "      - Fit a 3D quadratic function to the sampled points around the potential keypoint in scale-space.\n",
    "      - The 3D quadratic function is defined as:\n",
    "      $$ D(x) = D + \\frac{\\partial D^T}{\\partial x}x + \\frac{1}{2}x^T\\frac{\\partial^2 D}{\\partial x^2}x $$\n",
    "      where $x$ is the offset from the sampled point, $D$ is the intensity of the sampled point, and $\\frac{\\partial D}{\\partial x}$ and $\\frac{\\partial^2 D}{\\partial x^2}$ are the gradient and Hessian matrix, respectively.\n",
    "\n",
    "   4. **Keypoint Localization:**\n",
    "      - Find the critical points (minima or maxima) of the fitted quadratic function.\n",
    "      - Refine the keypoint location based on the offset $x$ that minimizes or maximizes the quadratic function.\n",
    "\n",
    "   The 3D quadratic function provides a precise localization of keypoints in both spatial and scale dimensions. It allows SIFT to accurately determine the location and scale of keypoints, making them more robust to transformations such as rotation and scale changes.\n",
    "\n",
    "---\n",
    "   \n",
    "#### **SIFT的高斯模糊和G(x,y,sigma)矩阵例子 :**\n",
    "##### 1. **高斯核 (Gaussian Kernel)**\n",
    "高斯核是一个基于高斯函数的矩阵，用来对图像进行模糊处理。二维高斯函数的公式为：\n",
    "\n",
    "$$\n",
    "G(x, y, \\sigma) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "我们以 $ \\sigma = 1 $ 为例，计算一个 $ 3 \\times 3 $ 的高斯核。\n",
    "\n",
    "高斯核的每个元素计算如下：\n",
    "- 中心点：$ G(0, 0, 1) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{0^2 + 0^2}{2}\\right) \\approx 0.159 $\n",
    "- 邻近点：$ G(1, 0, 1) = G(0, 1, 1) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{1^2}{2}\\right) \\approx 0.098 $\n",
    "- 对角点：$ G(1, 1, 1) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{2}{2}\\right) \\approx 0.061 $\n",
    "\n",
    "根据这些计算值，得出的高斯核可以近似为：\n",
    "\n",
    "$$\n",
    "G = \\frac{1}{16} \\begin{pmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "这近似于：\n",
    "$$\n",
    "G = \\begin{pmatrix} 0.0625 & 0.125 & 0.0625 \\\\ 0.125 & 0.25 & 0.125 \\\\ 0.0625 & 0.125 & 0.0625 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "##### 2. **图像矩阵 (Image Matrix)**\n",
    "假设我们有一个 $ 5 \\times 5 $ 的灰度图像矩阵 $ I $：\n",
    "\n",
    "$$\n",
    "I = \\begin{pmatrix} 52 & 55 & 61 & 66 & 70 \\\\ 63 & 59 & 55 & 90 & 109 \\\\ 85 & 105 & 115 & 125 & 135 \\\\ 140 & 150 & 160 & 170 & 180 \\\\ 190 & 200 & 210 & 220 & 230 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "##### 3. **高斯模糊 (Gaussian Blurring)**\n",
    "为了对图像进行高斯模糊，我们需要将图像矩阵 $ I $ 和高斯核 $ G $ 进行卷积运算。具体的计算方法如下：\n",
    "\n",
    "卷积操作是将高斯核中心对准图像矩阵中的某一元素，然后在核的范围内，图像矩阵中的相应元素与高斯核的相应元素相乘并求和。该和作为卷积后的新图像矩阵中对应位置的值。\n",
    "\n",
    "##### **示例计算**\n",
    "我们以矩阵 $ I $ 中元素 $ 59 $ 的位置（即第2行，第2列）为例进行卷积操作：\n",
    "\n",
    "取与该元素相邻的 $ 3 \\times 3 $ 区域：\n",
    "$$\n",
    "\\begin{pmatrix} 52 & 55 & 61 \\\\ 63 & 59 & 55 \\\\ 85 & 105 & 115 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "然后将其与高斯核 $ G $ 相乘并求和：\n",
    "$$\n",
    "(52 \\times 0.0625) + (55 \\times 0.125) + (61 \\times 0.0625) + (63 \\times 0.125) + (59 \\times 0.25) + (55 \\times 0.125) + (85 \\times 0.0625) + (105 \\times 0.125) + (115 \\times 0.0625)\n",
    "$$\n",
    "\n",
    "计算结果约为：\n",
    "$$\n",
    "3.25 + 6.875 + 3.8125 + 7.875 + 14.75 + 6.875 + 5.3125 + 13.125 + 7.1875 = 69.0625\n",
    "$$\n",
    "\n",
    "因此，在高斯模糊处理后的图像矩阵中，位置(2,2)的值将为约69。\n",
    "\n",
    "##### 4. **结果**\n",
    "通过类似的方法，对图像矩阵中的每个位置都进行卷积运算，最终得到一个模糊后的新图像矩阵。这个新矩阵即为高斯模糊后的图像结果。\n",
    "\n",
    "通过这个例子，你可以看到SIFT中高斯模糊的实际计算过程，这一步骤帮助提取出图像中的关键点，减少噪声对特征检测的影响。\n",
    "\n",
    "---\n",
    "\n",
    "#### **$\\sigma$ parameter**\n",
    "在SIFT算法中，$\\sigma$ 是一个参数，是由算法的设计者预先设定的，而不是通过计算得出的。具体来说，$\\sigma$ 决定了高斯模糊的程度，即卷积核的扩展程度。不同的 $\\sigma$ 值会影响图像的模糊程度，因此选择合适的 $\\sigma$ 值对算法的性能有很大影响。\n",
    "\n",
    "##### 1. **为什么使用不同的 $\\sigma$ 值？**\n",
    "在SIFT中，$\\sigma$ 通常用于创建图像的多尺度表示，即尺度空间。SIFT的关键思想之一是要在不同尺度下检测关键点，这样可以保证关键点在图像缩放或旋转时仍然保持稳定。因此，SIFT会在多种 $\\sigma$ 值下生成多个模糊版本的图像，这些版本构成了所谓的“尺度空间”（Scale Space）。\n",
    "\n",
    "##### 2. **如何选择 $\\sigma$？**\n",
    "- **初始值：** SIFT通常从一个较小的 $\\sigma$ 值开始，称为 $\\sigma_0$（通常 $\\sigma_0$ 取值为1.6），这个值是经验上效果较好的起始值。\n",
    "- **尺度空间：** 之后，在尺度空间中通过逐步增加 $\\sigma$ 的值来生成一系列图像。例如，如果一个图像的 $\\sigma_0$ 是1.6，接下来可能会通过乘以一个常数 $ k $ 来逐渐增加 $\\sigma$（如 $ k = \\sqrt{2} $），即 $\\sigma_1 = 1.6 \\times \\sqrt{2}$， $\\sigma_2 = 1.6 \\times 2$ 等等。通常每个八度（Octave）会生成4-5个不同的尺度。\n",
    "\n",
    "##### 3. **$\\sigma$ 的物理意义**\n",
    "- **小的 $\\sigma$ 值：** 更小的 $\\sigma$ 值意味着更少的模糊，能够保留更多的图像细节，适合检测小尺寸或细节丰富的特征。\n",
    "- **大的 $\\sigma$ 值：** 较大的 $\\sigma$ 值会导致图像变得更加模糊，能够捕捉到图像中的更大结构或低频信息，适合检测大尺寸或整体性较强的特征。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **HOG（Histogram of Oriented Gradients）**\n",
    "#### **特征描述符**：\n",
    "- **直方图计算：** 将关键点周围的梯度方向分为8个方向，生成一个128维的描述符。\n",
    "HOG 是通过计算图像局部区域的梯度方向直方图来捕获形状特征的。以下是核心步骤：\n",
    "\n",
    "#### **梯度计算**：\n",
    "- **水平和垂直方向的梯度：**\n",
    "  $$\n",
    "  G_x = I(x+1, y) - I(x-1, y)\n",
    "  $$\n",
    "  $$\n",
    "  G_y = I(x, y+1) - I(x, y-1)\n",
    "  $$\n",
    "\n",
    "- **梯度幅值和方向：**\n",
    "  $$\n",
    "  G = \\sqrt{G_x^2 + G_y^2}\n",
    "  $$\n",
    "  $$\n",
    "  \\theta = \\tan^{-1} \\left(\\frac{G_y}{G_x}\\right)\n",
    "  $$\n",
    "\n",
    "#### **方向直方图**：\n",
    "- **分区：** 将图像分为若干个小的细胞（如8x8像素）。\n",
    "- **直方图：** 对每个细胞内的像素，根据其梯度方向和幅值，计算方向直方图。\n",
    "\n",
    "#### **特征向量**：\n",
    "- **归一化：** 将若干个细胞组合成块（block），并对每个块的直方图进行归一化处理，最终组合成特征向量。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **支持向量机（SVM, Support Vector Machine）**\n",
    "SVM 是一种监督学习算法，用于分类问题。SVM 的目标是找到能够最大化类别间边界（margin）的决策边界。数学模型如下：\n",
    "\n",
    "#### **判别函数**：\n",
    "- **线性SVM：**\n",
    "  $$\n",
    "  f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "  $$\n",
    "  其中，$ \\mathbf{w} $ 是weight vector 权重向量，$ b $ 是 bias 偏置项，$ \\mathbf{x} $ 是输入特征向量。\n",
    "\n",
    "- **decision function 决策边界：** SVM maximize the margin between the classes by solving 寻找最大化边界的超平面，使得：\n",
    "  $$\n",
    "  \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "  $$\n",
    "  subject to 条件为：\n",
    "  $$\n",
    "  y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
    "  $$\n",
    "  其中，$ y_i $ 是类别标签，通常为 $ +1 $ 或 $ -1 $。\n",
    "\n",
    "#### **核方法（非线性SVM）：**\n",
    "- **kernel trick 核函数：** 为了处理非线性问题，the kernel function maps input data inot higher dimensional space 将输入数据映射到高维空间：\n",
    "  $$\n",
    "  K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}_j)\n",
    "  $$\n",
    "  常用的核函数包括线性核、径向基函数（RBF）核、以及多项式核等。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **OpenCV Common Libraries**\n",
    "### 1. **Image Preprocessing**\n",
    "   - **Image Filtering:**\n",
    "     ```cpp\n",
    "     cv::Mat src, dst;\n",
    "     cv::GaussianBlur(src, dst, cv::Size(5, 5), 1.5);\n",
    "     cv::medianBlur(src, dst, 5);\n",
    "     cv::bilateralFilter(src, dst, 9, 75, 75);\n",
    "     ```\n",
    "   - **Histogram Equalization:**\n",
    "     ```cpp\n",
    "     cv::Mat gray, equalized;\n",
    "     cv::cvtColor(src, gray, cv::COLOR_BGR2GRAY);\n",
    "     cv::equalizeHist(gray, equalized);\n",
    "     ```\n",
    "   - **Edge Detection:**\n",
    "     ```cpp\n",
    "     cv::Mat edges;\n",
    "     cv::Canny(src, edges, 100, 200);\n",
    "     ```\n",
    "   - **Morphological Operations:**\n",
    "     ```cpp\n",
    "     cv::Mat morph;\n",
    "     cv::Mat kernel = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(3, 3));\n",
    "     cv::morphologyEx(src, morph, cv::MORPH_CLOSE, kernel);\n",
    "     ```\n",
    "\n",
    "### 2. **Feature Detection and Description**\n",
    "   - **SIFT and ORB (Available via OpenCV Contrib Module):**\n",
    "     ```cpp\n",
    "     cv::Ptr<cv::SIFT> sift = cv::SIFT::create();\n",
    "     std::vector<cv::KeyPoint> keypoints;\n",
    "     cv::Mat descriptors;\n",
    "     sift->detectAndCompute(src, cv::noArray(), keypoints, descriptors);\n",
    "     ```\n",
    "     ```cpp\n",
    "     cv::Ptr<cv::ORB> orb = cv::ORB::create();\n",
    "     orb->detectAndCompute(src, cv::noArray(), keypoints, descriptors);\n",
    "     ```\n",
    "   - **Harris Corner Detection:**\n",
    "     ```cpp\n",
    "     cv::Mat dst, dst_norm;\n",
    "     cv::cornerHarris(src_gray, dst, 2, 3, 0.04);\n",
    "     cv::normalize(dst, dst_norm, 0, 255, cv::NORM_MINMAX);\n",
    "     ```\n",
    "\n",
    "### 3. **Image Segmentation**\n",
    "   - **Thresholding:**\n",
    "     ```cpp\n",
    "     cv::Mat binary;\n",
    "     cv::threshold(src, binary, 128, 255, cv::THRESH_BINARY);\n",
    "     ```\n",
    "   - **Watershed Algorithm:**\n",
    "     ```cpp\n",
    "     cv::Mat markers;\n",
    "     cv::watershed(src, markers);\n",
    "     ```\n",
    "   - **Graph Cuts (GrabCut):**\n",
    "     ```cpp\n",
    "     cv::Rect rect(10, 10, src.cols - 20, src.rows - 20);\n",
    "     cv::Mat bgModel, fgModel;\n",
    "     cv::grabCut(src, markers, rect, bgModel, fgModel, 5, cv::GC_INIT_WITH_RECT);\n",
    "     ```\n",
    "\n",
    "### 4. **Object Detection and Recognition**\n",
    "   - **HOG Descriptor and Object Detection:**\n",
    "     ```cpp\n",
    "     cv::HOGDescriptor hog;\n",
    "     hog.setSVMDetector(cv::HOGDescriptor::getDefaultPeopleDetector());\n",
    "     std::vector<cv::Rect> found;\n",
    "     hog.detectMultiScale(src, found);\n",
    "     ```\n",
    "   - **Cascade Classifier (Viola-Jones):**\n",
    "     ```cpp\n",
    "     cv::CascadeClassifier face_cascade;\n",
    "     face_cascade.load(\"haarcascade_frontalface_default.xml\");\n",
    "     std::vector<cv::Rect> faces;\n",
    "     face_cascade.detectMultiScale(src, faces);\n",
    "     ```\n",
    "\n",
    "### 5. **Image Registration and Alignment**\n",
    "   - **Homography:**\n",
    "     ```cpp\n",
    "     cv::Mat H = cv::findHomography(src_points, dst_points, cv::RANSAC);\n",
    "     cv::warpPerspective(src, aligned, H, dst.size());\n",
    "     ```\n",
    "   - **Optical Flow:**\n",
    "     ```cpp\n",
    "     cv::Mat flow;\n",
    "     cv::calcOpticalFlowFarneback(prev_gray, next_gray, flow, 0.5, 3, 15, 3, 5, 1.2, 0);\n",
    "     ```\n",
    "\n",
    "### 6. **Depth Estimation and 3D Reconstruction**\n",
    "   - **Stereo Vision:**\n",
    "     ```cpp\n",
    "     cv::Ptr<cv::StereoBM> stereo = cv::StereoBM::create(16, 9);\n",
    "     cv::Mat disparity;\n",
    "     stereo->compute(left_image, right_image, disparity);\n",
    "     ```\n",
    "\n",
    "### 7. **Machine Learning and Deep Learning**\n",
    "   - **Using Pre-trained Models (e.g., YOLO, ResNet):**\n",
    "     ```cpp\n",
    "     cv::dnn::Net net = cv::dnn::readNetFromDarknet(\"yolov3.cfg\", \"yolov3.weights\");\n",
    "     net.setInput(cv::dnn::blobFromImage(src, 1 / 255.0, cv::Size(416, 416), cv::Scalar(0, 0, 0), true, false));\n",
    "     std::vector<cv::Mat> outs;\n",
    "     net.forward(outs, getOutputsNames(net));\n",
    "     ```\n",
    "\n",
    "### 8. **Image Fusion and Super-Resolution**\n",
    "   - **Image Fusion:**\n",
    "     ```cpp\n",
    "     cv::addWeighted(src1, 0.5, src2, 0.5, 0, fused);\n",
    "     ```\n",
    "   - **Super-Resolution:**\n",
    "     ```cpp\n",
    "     cv::Ptr<cv::dnn_superres::DnnSuperResImpl> sr = cv::dnn_superres::DnnSuperResImpl_create();\n",
    "     sr->readModel(\"EDSR_x3.pb\");\n",
    "     sr->setModel(\"edsr\", 3);\n",
    "     sr->upsample(src, result);\n",
    "     ```\n",
    "\n",
    "### 9. **Statistical and Mathematical Methods**\n",
    "   - **Fourier Transform:**\n",
    "     ```cpp\n",
    "     cv::Mat planes[] = {src_float, cv::Mat::zeros(src.size(), CV_32F)};\n",
    "     cv::merge(planes, 2, complexI);\n",
    "     cv::dft(complexI, complexI);\n",
    "     ```\n",
    "\n",
    "### 10. **Imaging System Understanding**\n",
    "   - **Calibration:**\n",
    "     ```cpp\n",
    "     cv::calibrateCamera(object_points, image_points, image_size, camera_matrix, dist_coeffs, rvecs, tvecs);\n",
    "     ```\n",
    "\n",
    "### **Integration with Other Tools**\n",
    "OpenCV can be integrated with other libraries like TensorFlow, PyTorch, or custom C++ libraries for specific tasks, allowing a more comprehensive and sophisticated approach to computer vision problems.\n",
    "\n",
    "Using OpenCV for these tasks will allow you to effectively implement the core algorithms and techniques required for the role at Applied Materials, taking advantage of its extensive documentation and community support to ensure robust and optimized implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplacian Filter\n",
    "#   Laplacian filtering is a second-order derivative method that highlights areas of rapid intensity change, \n",
    "#   effectively detecting edges by finding regions where the intensity gradient is high. \n",
    "#   It works by applying a Laplacian kernel to the image, which emphasizes edges but is sensitive to noise.\n",
    "# 1. **Laplacian Kernel:**\n",
    "#    - The kernel used in the function is the standard 3x3 Laplacian kernel with a sum of zero.\n",
    "#    - This kernel detects edges by highlighting areas where the intensity changes abruptly.\n",
    "# 2. **Padding:**\n",
    "#    - The image is padded with zeros to ensure that the filter can be applied to all pixels, including those at the edges of the image.\n",
    "# 3. **Convolution:**\n",
    "#    - The convolution operation is performed by sliding the kernel over the image and computing the weighted sum of the pixel values under the kernel. \n",
    "#    - The result is placed in the corresponding pixel of the output image.\n",
    "# 4. **Output:**\n",
    "#    - The output `filtered_image` contains the result of applying the Laplacian filter to the input `image`. \n",
    "#    - This filtered image highlights edges and areas of rapid intensity change.\n",
    "# The Laplacian filter implementation provided here uses a basic 3x3 kernel to detect edges in an image. \n",
    "# This filter is useful for edge detection tasks in image processing, and it is implemented using fundamental operations such as convolution, without relying on external libraries like OpenCV.\n",
    "# The code can be easily extended to use different kernels or handle larger images.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def apply_laplacian_filter(image):\n",
    "    \"\"\"Applies a Laplacian filter to a 2D image.\"\"\"\n",
    "    \n",
    "    # Define the Laplacian kernel (common 3x3 version)\n",
    "    # detecting edges along the horizontal and vertical directions\n",
    "    kernel = np.array([[0,  1, 0],\n",
    "                       [1, -4, 1],\n",
    "                       [0,  1, 0]])\n",
    "\n",
    "    # Alternatively, you can use the following kernel:\n",
    "    # detecting edges along all directions\n",
    "    # kernel = np.array([[ 1,  1,  1],\n",
    "    #                    [ 1, -8,  1],\n",
    "    #                    [ 1,  1,  1]])\n",
    "\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    # Calculate padding size\n",
    "    pad_height = kernel_height // 2\n",
    "    pad_width = kernel_width // 2\n",
    "\n",
    "    # Pad the image with zeros on all sides\n",
    "    padded_image = np.pad(image, ((pad_height, pad_height), (pad_width, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Initialize the output image\n",
    "    filtered_image = np.zeros_like(image)\n",
    "\n",
    "    # Convolve the image with the kernel\n",
    "    for i in range(image_height):\n",
    "        for j in range(image_width):\n",
    "            # Extract the region of interest\n",
    "            region = padded_image[i:i+kernel_height, j:j+kernel_width]\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            filtered_image[i, j] = np.sum(region * kernel)\n",
    "\n",
    "    return filtered_image\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample image (for demonstration purposes)\n",
    "    image = np.array([[50, 80, 50],\n",
    "                      [80, 120, 80],\n",
    "                      [50, 80, 50]], dtype=np.float64)\n",
    "\n",
    "    # Apply Laplacian filter\n",
    "    laplacian_image = apply_laplacian_filter(image)\n",
    "    print(\"Laplacian Filtered Image:\")\n",
    "    print(laplacian_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canny Edge Detection\n",
    "# https://en.wikipedia.org/wiki/Canny_edge_detector\n",
    "# The optimal function in Canny's detector is described by the sum of four exponential terms & can be approximated by the first derivative of a Gaussian.\n",
    "# The process of Canny edge detection algorithm can be broken down to five different steps:\n",
    "# - Apply Gaussian filter to smooth the image in order to remove the noise\n",
    "#    - **Smoothing (Gaussian Blur):** Convolve the image with a Gaussian filter to reduce noise.\n",
    "#       $$ G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-(x^2 + y^2) / (2\\sigma^2)} $$\n",
    "# - Find the intensity gradients of the image\n",
    "#    - **Intensity Gradient Calculation:** Compute the gradient of the smoothed image to find the intensity changes.\n",
    "#       $$ G_x = \\frac{\\partial G}{\\partial x}, \\quad G_y = \\frac{\\partial G}{\\partial y} $$\n",
    "#       $$ \\text{Gradient Magnitude (M)} = \\sqrt{G_x^2 + G_y^2} $$\n",
    "#       $$ \\text{Gradient Direction (θ)} = \\arctan\\left(\\frac{G_y}{G_x}\\right) $$\n",
    "# - Apply gradient magnitude thresholding or lower bound cut-off suppression to get rid of spurious response to edge detection\n",
    "# - Apply double threshold to determine potential edges\n",
    "# - Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import convolve, gaussian_filter\n",
    "from skimage import data\n",
    "from scipy import ndimage\n",
    "\n",
    "def gaussian_kernel(size, sigma=1):\n",
    "    \"\"\"Generates a 2D Gaussian kernel.\"\"\"\n",
    "    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
    "    return g / g.sum() # normalize the kernel\n",
    "\n",
    "def gradient_intensity(image):\n",
    "    \"\"\"Computes the intensity gradient of the image.\"\"\"\n",
    "    # sobel kernels: emphasize regions of high spatial frequency that correspond to edges.\n",
    "    Kx = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]) # kernel for horizontal gradient. larger coefficients (2 and -2) weight to the central difference.\n",
    "    Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]) # kernel for vertical gradient. difference between the top and bottom of a pixel highlights horizontal edges.\n",
    "\n",
    "    Ix = convolve(image, Kx)\n",
    "    Iy = convolve(image, Ky)\n",
    "\n",
    "    G = np.hypot(Ix, Iy)\n",
    "    G = G / G.max() * 255\n",
    "    theta = np.arctan2(Iy, Ix)\n",
    "    \n",
    "    return (G, theta)\n",
    "\n",
    "def non_maximum_suppression(G, theta):\n",
    "    \"\"\"Performs non-maximum suppression to thin out the edges.\"\"\"\n",
    "    M, N = G.shape\n",
    "    Z = np.zeros((M, N), dtype=np.int32)\n",
    "    angle = theta * 180. / np.pi\n",
    "    angle[angle < 0] += 180\n",
    "\n",
    "    for i in range(1, M-1):\n",
    "        for j in range(1, N-1):\n",
    "            try:\n",
    "                # neighbors\n",
    "                q = 255\n",
    "                r = 255\n",
    "                \n",
    "                # angle 0 (horizontal)\n",
    "                if (0 <= angle[i, j] < 22.5) or (157.5 <= angle[i, j] <= 180):\n",
    "                    q = G[i, j+1]\n",
    "                    r = G[i, j-1]\n",
    "                # angle 45 (diagonal)\n",
    "                elif 22.5 <= angle[i, j] < 67.5:\n",
    "                    q = G[i+1, j-1]\n",
    "                    r = G[i-1, j+1]\n",
    "                # angle 90 (vertical)\n",
    "                elif 67.5 <= angle[i, j] < 112.5:\n",
    "                    q = G[i+1, j]\n",
    "                    r = G[i-1, j]\n",
    "                # angle 135 (anit-diagonal)\n",
    "                elif 112.5 <= angle[i, j] < 157.5:\n",
    "                    q = G[i-1, j-1]\n",
    "                    r = G[i+1, j+1]\n",
    "\n",
    "                if (G[i, j] >= q) and (G[i, j] >= r):\n",
    "                    # current pixel gradient is the local maximum\n",
    "                    Z[i, j] = G[i, j]\n",
    "                else:\n",
    "                    # current pixel gradient is not the local maximum, suppress it\n",
    "                    Z[i, j] = 0\n",
    "\n",
    "            except IndexError as e:\n",
    "                pass\n",
    "    # Z contains the thinned out edges\n",
    "    return Z\n",
    "\n",
    "def threshold(image, lowThreshold, highThreshold):\n",
    "    \"\"\"Applies double thresholding.\"\"\"\n",
    "    highThreshold = image.max() * highThreshold\n",
    "    lowThreshold = highThreshold * lowThreshold\n",
    "    \n",
    "    M, N = image.shape\n",
    "    res = np.zeros((M,N), dtype=np.int32)\n",
    "    \n",
    "    weak = np.int32(25)\n",
    "    strong = np.int32(255)\n",
    "    \n",
    "    strong_i, strong_j = np.where(image >= highThreshold)\n",
    "    zeros_i, zeros_j = np.where(image < lowThreshold)\n",
    "    \n",
    "    weak_i, weak_j = np.where((image <= highThreshold) & (image >= lowThreshold))\n",
    "    \n",
    "    res[strong_i, strong_j] = strong\n",
    "    res[weak_i, weak_j] = weak\n",
    "    \n",
    "    return (res, weak, strong)\n",
    "\n",
    "def otsu_thresholding(image):\n",
    "    \"\"\"Calculates the optimal threshold value using Otsu's method.\"\"\"\n",
    "    # the algorithm returns a single intensity threshold that separate pixels into two classes, foreground and background. \n",
    "    # the threshold is determined by minimizing intra-class intensity variance by maximizing inter-class variance\n",
    "    hist, bin_edges = np.histogram(image.flatten(), bins=256, range=(0, 256))\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    weight1 = np.cumsum(hist)\n",
    "    weight2 = np.cumsum(hist[::-1])[::-1]\n",
    "    \n",
    "    mean1 = np.cumsum(hist * bin_centers) / weight1\n",
    "    mean2 = (np.cumsum((hist * bin_centers)[::-1]) / weight2[::-1])[::-1]\n",
    "    #  minimizing the intra-class variance is equivalent to maximizing inter-class variance\n",
    "    inter_class_variance = weight1[:-1] * weight2[1:] * (mean1[:-1] - mean2[1:]) ** 2\n",
    "\n",
    "    index_of_max_variance = np.argmax(inter_class_variance)\n",
    "    otsu_threshold = bin_centers[:-1][index_of_max_variance]\n",
    "\n",
    "    return otsu_threshold\n",
    "\n",
    "def threshold_with_otsu(image):\n",
    "    \"\"\"Applies Otsu's method to determine optimal high and low thresholds.\"\"\"\n",
    "    otsu_thresh = otsu_thresholding(image)\n",
    "    \n",
    "    # Use Otsu's threshold as the high threshold, and a fraction of it as the low threshold\n",
    "    highThreshold = otsu_thresh\n",
    "    lowThreshold = otsu_thresh * 0.5  # You can adjust this factor as needed\n",
    "    \n",
    "    strong = 255\n",
    "    weak = np.int32(25)\n",
    "    \n",
    "    M, N = image.shape\n",
    "    res = np.zeros((M, N), dtype=np.int32)\n",
    "\n",
    "    strong_i, strong_j = np.where(image >= highThreshold)\n",
    "    weak_i, weak_j = np.where((image < highThreshold) & (image >= lowThreshold))\n",
    "    \n",
    "    res[strong_i, strong_j] = strong\n",
    "    res[weak_i, weak_j] = weak\n",
    "    \n",
    "    return res, weak, strong\n",
    "\n",
    "def hysteresis(img, weak, strong=255):\n",
    "    \"\"\"Performs edge tracking by hysteresis.\"\"\"\n",
    "    M, N = img.shape\n",
    "    for i in range(1, M-1):\n",
    "        for j in range(1, N-1):\n",
    "            if (img[i, j] == weak):\n",
    "                try:\n",
    "                    # refine the edge map by converting weak edges into strong edges if they are connected to strong edges. \n",
    "                    if ((img[i+1, j-1] == strong) or (img[i+1, j] == strong) or (img[i+1, j+1] == strong)\n",
    "                        or (img[i, j-1] == strong) or (img[i, j+1] == strong)\n",
    "                        or (img[i-1, j-1] == strong) or (img[i-1, j] == strong) or (img[i-1, j+1] == strong)):\n",
    "                        img[i, j] = strong\n",
    "                    # otherwise, the weak edges are discarded.\n",
    "                    else:\n",
    "                        img[i, j] = 0\n",
    "                except IndexError as e:\n",
    "                    pass\n",
    "    return img\n",
    "\n",
    "def canny_edge_detector(image, sigma=1, lowThreshold=0.05, highThreshold=0.15):\n",
    "    \"\"\"Applies the full Canny edge detection algorithm.\"\"\"\n",
    "    # Step 1: Gaussian Blur\n",
    "    image_blurred = gaussian_filter(image, sigma=sigma)\n",
    "    \n",
    "    # Step 2: Gradient Calculation\n",
    "    G, theta = gradient_intensity(image_blurred)\n",
    "    \n",
    "    # Step 3: Non-Maximum Suppression\n",
    "    Z = non_maximum_suppression(G, theta)\n",
    "    \n",
    "    # Step 4: Double Threshold\n",
    "    res, weak, strong = threshold(Z, lowThreshold, highThreshold)\n",
    "    \n",
    "    # Step 5: Hysteresis\n",
    "    edges = hysteresis(res, weak, strong)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def canny_edge_detector_with_otsu(image, sigma=1.0):\n",
    "    \"\"\"Applies the Canny edge detection algorithm using Otsu's method for thresholding.\"\"\"\n",
    "    G, theta = gradient_intensity(gaussian_filter(image, sigma))\n",
    "    Z = non_maximum_suppression(G, theta)\n",
    "    res, weak, strong = threshold_with_otsu(Z)\n",
    "    edges = hysteresis(res, weak, strong)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load an image (for demonstration purposes, use a simple 2D array)\n",
    "    image = data.camera()  # Using skimage's sample data for simplicity\n",
    "    edges = canny_edge_detector(image, sigma=1.4, lowThreshold=0.1, highThreshold=0.2)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Canny Edges\")\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HoG Feature Extraction\n",
    "# https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "# The Histogram of Oriented Gradients (HoG) is a feature descriptor used for object detection in computer vision and image processing.\n",
    "# Extracts features from an image by capturing the distribution of gradient orientations within localized regions (cells) of the image. \n",
    "# It is widely used for object detection, especially in human detection.\n",
    "# The HoG feature extraction process can be broken down into the following steps:\n",
    "# - **Gradient Calculation:** Compute the gradient magnitude and direction of the image using derivative filters (e.g., Sobel filters).\n",
    "# - **Orientation and Magnitude Calculation:** Compute the gradient orientation and magnitude at each pixel.\n",
    "# - **Histogram Calculation:** Compute a histogram of gradient orientations within each cell.\n",
    "# - **Block Normalization:** Normalize the histograms across blocks to improve invariance to changes in illumination and contrast.\n",
    "# - **Feature Vector:** Concatenate the normalized histograms to form the final HoG feature vector.\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "def compute_gradients(image):\n",
    "    \"\"\"Compute the gradient magnitude and direction of the image.\"\"\"\n",
    "    # sobel kernels: emphasize regions of high spatial frequency that correspond to edges.\n",
    "    Kx = np.array([[-1, 0, 1], \n",
    "                   [-2, 0, 2], \n",
    "                   [-1, 0, 1]])\n",
    "    Ky = np.array([[1, 2, 1], \n",
    "                   [0, 0, 0], \n",
    "                   [-1, -2, -1]])\n",
    "\n",
    "    Ix = convolve(image, Kx)\n",
    "    Iy = convolve(image, Ky)\n",
    "\n",
    "    magnitude = np.sqrt(Ix**2 + Iy**2)\n",
    "    direction = np.arctan2(Iy, Ix) * (180 / np.pi) % 180  # Convert to degrees\n",
    "    \n",
    "    return magnitude, direction\n",
    "\n",
    "def compute_hog_features(image, cell_size=8, block_size=2, bins=9):\n",
    "    \"\"\"Compute the Histogram of Oriented Gradients (HoG) for an image.\"\"\"\n",
    "    # Step 1: Compute gradients\n",
    "    magnitude, direction = compute_gradients(image)\n",
    "\n",
    "    # Step 2: Divide the image into cells\n",
    "    height, width = image.shape\n",
    "    num_cells_x = width // cell_size\n",
    "    num_cells_y = height // cell_size\n",
    "\n",
    "    hog_vector = []\n",
    "\n",
    "    # Step 3: Compute the histogram of gradients in each cell\n",
    "    for i in range(num_cells_y):\n",
    "        for j in range(num_cells_x):\n",
    "            cell_magnitude = magnitude[i*cell_size:(i+1)*cell_size, j*cell_size:(j+1)*cell_size]\n",
    "            cell_direction = direction[i*cell_size:(i+1)*cell_size, j*cell_size:(j+1)*cell_size]\n",
    "\n",
    "            histogram = np.zeros(bins)\n",
    "            bin_width = 180 / bins\n",
    "\n",
    "            for k in range(cell_size):\n",
    "                for l in range(cell_size):\n",
    "                    magnitude_val = cell_magnitude[k, l]\n",
    "                    direction_val = cell_direction[k, l]\n",
    "\n",
    "                    bin_idx = int(direction_val // bin_width)\n",
    "                    histogram[bin_idx] += magnitude_val\n",
    "\n",
    "            hog_vector.append(histogram)\n",
    "\n",
    "    # Step 4: Normalize across blocks\n",
    "    hog_vector = np.array(hog_vector)\n",
    "    hog_vector = hog_vector.reshape(num_cells_y, num_cells_x, bins)\n",
    "\n",
    "    num_blocks_x = num_cells_x - block_size + 1\n",
    "    num_blocks_y = num_cells_y - block_size + 1\n",
    "\n",
    "    normalized_hog_vector = []\n",
    "\n",
    "    for i in range(num_blocks_y):\n",
    "        for j in range(num_blocks_x):\n",
    "            block = hog_vector[i:i+block_size, j:j+block_size, :]\n",
    "            block = block.flatten()\n",
    "            normalized_block = block / np.sqrt(np.sum(block**2) + 1e-5)  # L2-Hys normalization\n",
    "            normalized_hog_vector.append(normalized_block)\n",
    "\n",
    "    return np.array(normalized_hog_vector).flatten()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example image (for demonstration, you should use a real image)\n",
    "    image = np.array([[255, 255, 255, 255, 0, 0, 0, 0],\n",
    "                      [255, 255, 255, 255, 0, 0, 0, 0],\n",
    "                      [255, 255, 255, 255, 0, 0, 0, 0],\n",
    "                      [255, 255, 255, 255, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 255, 255, 255, 255],\n",
    "                      [0, 0, 0, 0, 255, 255, 255, 255],\n",
    "                      [0, 0, 0, 0, 255, 255, 255, 255],\n",
    "                      [0, 0, 0, 0, 255, 255, 255, 255]], dtype=np.float64)\n",
    "\n",
    "    hog_features = compute_hog_features(image, cell_size=2, block_size=2, bins=9)\n",
    "    print(\"HoG Feature Vector:\")\n",
    "    print(hog_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT (Scale-Invariant Feature Transform)\n",
    "# https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\n",
    "# 1.  Scale-sapce Extrema Detection\n",
    "#     - Apply Gaussian filters at different scales to create a scale-space pyramid.\n",
    "#     - Detect local extrema in the Difference of Gaussians (DoG) pyramid to identify potential keypoints.\n",
    "#       - Construct a histogram of gradient orientations in the local region. The histogram is usually divided into bins representing different orientation ranges.\n",
    "#     - Dominant Orientation\n",
    "#       - Identify the peak or peaks in the histogram to determine the dominant orientation(s).\n",
    "#       - The dominant orientation is selected as the canonical orientation for the keypoint.\n",
    "# 2. **Keypoint Descriptor:**\n",
    "#    - Define a local image patch around each keypoint and transform it into a canonical orientation.\n",
    "#    - Divide the patch into smaller sub-regions and compute gradient magnitudes and orientations.\n",
    "#    - Create a descriptor vector by concatenating these gradient values, forming a representation invariant to rotation and scale changes.\n",
    "#    - Refine keypoint locations by fitting a 3D quadratic function to the nearby samples in the DoG pyramid.\n",
    "#    - Eliminate unstable keypoints based on their contrast and edge responses.\n",
    "#    1. **Extrema Detection in Scale-Space:**\n",
    "#       - Identify potential keypoints as local extrema in the Difference of Gaussians (DoG) scale-space pyramid.\n",
    "#       - **尺度空间极值检测：** 通过多尺度的高斯差分（Difference of Gaussians, DoG）计算图像中的极值点。\n",
    "#       $$\n",
    "#       D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)\n",
    "#       $$\n",
    "#       其中，$ k $ 是一个常数倍率因子，通常取值为 $ \\sqrt{2} $。\n",
    "#    2. **方向赋值：**\n",
    "#     - **梯度计算：** 对每个关键点，计算其周围像素的梯度幅值和方向。\n",
    "#       $$\n",
    "#       m(x, y) = \\sqrt{(L(x+1, y) - L(x-1, y))^2 + (L(x, y+1) - L(x, y-1))^2}\n",
    "#       $$\n",
    "#       $$\n",
    "#       \\theta(x, y) = \\tan^{-1} \\left(\\frac{L(x, y+1) - L(x, y-1)}{L(x+1, y) - L(x-1, y)}\\right)\n",
    "#       $$\n",
    "#    3. **Refinement using a 3D Quadratic Function:**\n",
    "#       - Fit a 3D quadratic function to the sampled points around the potential keypoint in scale-space.\n",
    "#       - The 3D quadratic function is defined as:\n",
    "#       $$ D(x) = D + \\frac{\\partial D^T}{\\partial x}x + \\frac{1}{2}x^T\\frac{\\partial^2 D}{\\partial x^2}x $$\n",
    "#       where $x$ is the offset from the sampled point, $D$ is the intensity of the sampled point, and $\\frac{\\partial D}{\\partial x}$ and $\\frac{\\partial^2 D}{\\partial x^2}$ are the gradient and Hessian matrix, respectively.\n",
    "#    4. **Keypoint Localization:**\n",
    "#       - Find the critical points (minima or maxima) of the fitted quadratic function.\n",
    "#       - Refine the keypoint location based on the offset $x$ that minimizes or maximizes the quadratic function.\n",
    "# 3. **Matching:**\n",
    "#    - Compare descriptors between keypoints in different images to find correspondences.\n",
    "#    - Use a distance metric (e.g., Euclidean distance) to measure the similarity between descriptors.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Scale-space Extrema Detection\n",
    "This step involves creating a series of blurred images (Gaussian pyramid) \n",
    "and then computing the Difference of Gaussians (DoG) to find potential keypoints.\n",
    "\"\"\"\n",
    "def gaussian_kernel(size, sigma):\n",
    "    \"\"\"Generate a Gaussian kernel.\"\"\"\n",
    "    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
    "    return g / g.sum()\n",
    "\n",
    "def difference_of_gaussians(image, sigma1, sigma2):\n",
    "    \"\"\"Compute the Difference of Gaussians (DoG) for an image.\"\"\"\n",
    "    blurred_image1 = gaussian_filter(image, sigma=sigma1)\n",
    "    blurred_image2 = gaussian_filter(image, sigma=sigma2)\n",
    "    dog_image = blurred_image1 - blurred_image2\n",
    "    return dog_image\n",
    "\n",
    "def generate_gaussian_pyramid(image, num_octaves, num_scales, initial_sigma=1.6):\n",
    "    \"\"\"Generate a Gaussian pyramid.\"\"\"\n",
    "    pyramid = []\n",
    "    k = 2 ** (1 / num_scales)\n",
    "    for octave in range(num_octaves):\n",
    "        octave_images = []\n",
    "        sigma = initial_sigma * (2 ** octave)\n",
    "        for scale in range(num_scales):\n",
    "            blurred_image = gaussian_filter(image, sigma * (k ** scale))\n",
    "            octave_images.append(blurred_image)\n",
    "        pyramid.append(octave_images)\n",
    "    return pyramid\n",
    "\n",
    "def generate_dog_pyramid(gaussian_pyramid):\n",
    "    \"\"\"Generate the Difference of Gaussians (DoG) pyramid.\"\"\"\n",
    "    dog_pyramid = []\n",
    "    for octave in gaussian_pyramid:\n",
    "        octave_dogs = []\n",
    "        for i in range(1, len(octave)):\n",
    "            dog_image = octave[i] - octave[i-1]\n",
    "            octave_dogs.append(dog_image)\n",
    "        dog_pyramid.append(octave_dogs)\n",
    "    return dog_pyramid\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Keypoint Detection and Localization\n",
    "identify local maxima and minima in the DoG images as potential keypoints. \n",
    "We will also refine their positions by checking their contrast.\n",
    "\"\"\"\n",
    "def find_keypoints(dog_pyramid, threshold=0.04):\n",
    "    \"\"\"Detect keypoints in the DoG pyramid.\"\"\"\n",
    "    keypoints = []\n",
    "    for o in range(len(dog_pyramid)):\n",
    "        for i in range(1, len(dog_pyramid[o]) - 1):\n",
    "            dog_prev = dog_pyramid[o][i-1]\n",
    "            dog_curr = dog_pyramid[o][i]\n",
    "            dog_next = dog_pyramid[o][i+1]\n",
    "\n",
    "            for y in range(1, dog_curr.shape[0] - 1):\n",
    "                for x in range(1, dog_curr.shape[1] - 1):\n",
    "                    patch = dog_curr[y-1:y+2, x-1:x+2]\n",
    "                    value = patch[1, 1]\n",
    "\n",
    "                    # Check if it's an extrema\n",
    "                    if (value > threshold and \n",
    "                        value == np.max([dog_prev[y-1:y+2, x-1:x+2], patch, dog_next[y-1:y+2, x-1:x+2]])):\n",
    "                        keypoints.append((x, y, o, i))\n",
    "                    elif (value < -threshold and \n",
    "                          value == np.min([dog_prev[y-1:y+2, x-1:x+2], patch, dog_next[y-1:y+2, x-1:x+2]])):\n",
    "                        keypoints.append((x, y, o, i))\n",
    "    return keypoints\n",
    "\n",
    "\"\"\" \n",
    "Step 3: Orientation Assignment\n",
    "For each keypoint, \n",
    "compute the gradient magnitude and orientation in the local region and assign a dominant orientation, \n",
    "making the keypoint rotation-invariant.\n",
    "\"\"\"\n",
    "def compute_gradients(image):\n",
    "    \"\"\"Compute the gradients (magnitude and orientation) of the image.\"\"\"\n",
    "    Kx = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "    Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    \n",
    "    Ix = convolve(image, Kx)\n",
    "    Iy = convolve(image, Ky)\n",
    "    \n",
    "    magnitude = np.sqrt(Ix**2 + Iy**2)\n",
    "    orientation = np.arctan2(Iy, Ix) * (180 / np.pi)\n",
    "    \n",
    "    return magnitude, orientation\n",
    "\n",
    "def assign_orientation(keypoints, gaussian_pyramid):\n",
    "    \"\"\"Assign orientation to each keypoint.\"\"\"\n",
    "    keypoints_oriented = []\n",
    "    for x, y, o, i in keypoints:\n",
    "        image = gaussian_pyramid[o][i]\n",
    "        magnitude, orientation = compute_gradients(image)\n",
    "        \n",
    "        patch_mag = magnitude[y-1:y+2, x-1:x+2]\n",
    "        patch_ori = orientation[y-1:y+2, x-1:x+2]\n",
    "        \n",
    "        hist, _ = np.histogram(patch_ori, bins=36, range=(0, 360), weights=patch_mag) # 360/36 = 10 degrees per bin\n",
    "        dominant_orientation = np.argmax(hist) * 10  # 10 degrees per bin. multiplied by 10 to convert it back to an actual orientation in degrees\n",
    "        \n",
    "        keypoints_oriented.append((x, y, o, i, dominant_orientation))\n",
    "    \n",
    "    return keypoints_oriented\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load an image (for demonstration purposes, use a simple 2D array)\n",
    "    image = np.random.rand(128, 128)  # Replace this with an actual image\n",
    "\n",
    "    # Step 1: Generate Gaussian and DoG pyramids\n",
    "    gaussian_pyramid = generate_gaussian_pyramid(image, num_octaves=3, num_scales=3)\n",
    "    dog_pyramid = generate_dog_pyramid(gaussian_pyramid)\n",
    "\n",
    "    # Step 2: Find keypoints\n",
    "    keypoints = find_keypoints(dog_pyramid)\n",
    "\n",
    "    # Step 3: Assign orientation to keypoints\n",
    "    keypoints_oriented = assign_orientation(keypoints, gaussian_pyramid)\n",
    "\n",
    "    print(\"Keypoints with Orientations:\")\n",
    "    for kp in keypoints_oriented:\n",
    "        print(kp)\n",
    "    # Now, `keypoints_oriented` contains the detected keypoints with their assigned orientations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Gaussian Pyramid and Its Physical Meaning**\n",
    "\n",
    "The Gaussian pyramid is a key component of the SIFT algorithm, and it is used to create a multi-scale representation of an image. Each layer of the pyramid represents the image at a different scale, which helps in detecting features that are invariant to scale (i.e., features that remain recognizable regardless of the size of the object in the image).\n",
    "\n",
    "### **Structure of the Gaussian Pyramid**\n",
    "\n",
    "1. **Octaves:**\n",
    "   - The pyramid is divided into several \"octaves.\" Each octave is a group of images that are progressively downsampled versions of the original image.\n",
    "   - Within each octave, the image resolution is the same, but the amount of Gaussian blur applied to the image increases progressively.\n",
    "\n",
    "2. **Scales (or Levels):**\n",
    "   - Each octave contains multiple scales (or levels). Each scale is created by applying a Gaussian blur with an increasing sigma value to the image.\n",
    "   - The first scale in each octave usually starts with the initial image (or a downsampled version of the initial image for subsequent octaves), and then the Gaussian blur is applied repeatedly to generate the subsequent scales.\n",
    "\n",
    "### **Physical Meaning of Each Layer**\n",
    "\n",
    "1. **First Layer (Lowest Sigma) in the First Octave:**\n",
    "   - This is the original image or the image with minimal blurring. It contains most of the fine details and noise. This layer is sensitive to small, high-frequency details.\n",
    "\n",
    "2. **Subsequent Layers in the First Octave:**\n",
    "   - As you move down the scales in the first octave, the sigma value increases, and the image becomes more blurred. The fine details start to disappear, and only the larger, more significant structures remain visible.\n",
    "   - The higher the sigma, the less sensitive the image is to small details and noise. Larger structures (lower-frequency components) become more prominent.\n",
    "\n",
    "3. **First Layer in Subsequent Octaves:**\n",
    "   - When moving to the next octave, the image is downsampled (typically by a factor of 2). This reduces the resolution of the image by half, which means the image size is smaller, but it is still blurred with the initial sigma value for that octave.\n",
    "   - This layer contains larger structures from the previous octave but at a smaller scale (half the resolution).\n",
    "\n",
    "4. **Subsequent Layers in Subsequent Octaves:**\n",
    "   - Similar to the first octave, as you move down the scales in subsequent octaves, the sigma value increases, and the image becomes more blurred.\n",
    "   - Each layer in these octaves corresponds to detecting even larger features, and the image continues to lose fine details.\n",
    "\n",
    "### **Difference of Gaussians (DoG) Pyramid:**\n",
    "- The DoG pyramid is constructed by subtracting adjacent layers in the Gaussian pyramid within the same octave. This highlights regions of the image where there are significant changes in intensity, such as edges and corners, which are potential keypoints.\n",
    "\n",
    "### **Physical Meaning:**\n",
    "- **Fine-to-Coarse Feature Detection:**\n",
    "  - The top layers of each octave detect fine details, while the bottom layers detect coarser, larger features.\n",
    "  - This approach allows the SIFT algorithm to detect features at various scales, ensuring that the keypoints are robust to changes in the size of the objects in the image.\n",
    "\n",
    "- **Scale-Invariance:**\n",
    "  - By analyzing the image at multiple scales, SIFT can detect keypoints that are invariant to scaling. This means that the same feature can be detected regardless of whether the object appears small or large in the image.\n",
    "\n",
    "### **Visualization:**\n",
    "\n",
    "If you were to visualize the Gaussian pyramid:\n",
    "\n",
    "- The first octave might look like this:\n",
    "  - Scale 1 (Sigma 1.0): Sharp, detailed image.\n",
    "  - Scale 2 (Sigma 1.6): Slightly blurred, with some loss of fine detail.\n",
    "  - Scale 3 (Sigma 2.0): More blurred, focusing on larger structures.\n",
    "\n",
    "- The second octave would be:\n",
    "  - Scale 1 (Sigma 1.0): Downsampled image, retaining larger features from the first octave.\n",
    "  - Scale 2 (Sigma 1.6): Further blurred, with even more focus on larger structures.\n",
    "  - Scale 3 (Sigma 2.0): Blurred to the point where only the most significant features remain.\n",
    "\n",
    "### **Summary:**\n",
    "- **Gaussian Pyramid:**\n",
    "  - A structure that represents the image at multiple scales, with each octave representing a progressively downsampled version of the image, and each scale within an octave representing increasing levels of Gaussian blur.\n",
    "  \n",
    "- **Physical Meaning:**\n",
    "  - The pyramid layers transition from fine detail and high sensitivity to noise at the top to coarse detail and robustness to noise at the bottom, enabling SIFT to detect features that are invariant to scale and robust across different resolutions.\n",
    "\n",
    "This multi-scale representation is crucial for the SIFT algorithm's ability to detect stable keypoints that are consistent regardless of the object's size or the image resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Harris Corner Detection algorithm**\n",
    "is a popular method used in computer vision to detect corners within an image. It identifies points in an image where the local neighborhood shows significant intensity changes in all directions.\n",
    "\n",
    "### **Steps to Implement Harris Corner Detection:**\n",
    "1. **Compute Image Gradients:**\n",
    "   - Compute the gradients of the image in the x and y directions.\n",
    "2. **Compute Products of Gradients:**\n",
    "   - Compute the products of the gradients at each pixel (Ix², Iy², and IxIy).\n",
    "3. **Compute the Structure Tensor:**\n",
    "   - Apply a Gaussian filter to the gradient products to obtain the sums of products (Sxx, Syy, Sxy) within a neighborhood.\n",
    "4. **Compute the Harris Response:**\n",
    "   - Calculate the Harris response for each pixel using the determinant and trace of the structure tensor matrix.\n",
    "5. **Thresholding:**\n",
    "   - Identify pixels with a Harris response greater than a certain threshold as corners.\n",
    "\n",
    "### **Summary:**\n",
    "This code implements the Harris Corner Detection algorithm from scratch using NumPy and SciPy. The key steps include calculating image gradients, building the structure tensor, computing the Harris response, and applying a threshold to identify corners. This implementation is a fundamental approach to understanding how corner detection works and can be applied to various computer vision tasks where corner features are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harris Corner Detection\n",
    "# https://en.wikipedia.org/wiki/Harris_Corner_Detector\n",
    "# The Harris corner detection algorithm is a popular method for detecting corners in an image.\n",
    "# Harris' corner detector takes the differential of the corner score into account with reference to direction directly, \n",
    "# instead of using shifting patches for every 45 degree angles, and has been proved to be more accurate in distinguishing between edges and corners\n",
    "# A corner is a point whose local neighborhood stands in two dominant and different edge directions. \n",
    "# In other words, a corner can be interpreted as the junction of two edges, where an edge is a sudden change in image brightness. \n",
    "# Corners are the important features in the image, and they are generally termed as interest points which are invariant to translation, rotation and illumination. Although corners are only a small percentage of the image, they contain the most important features in restoring image information, and they can be used to minimize the amount of processed data for motion tracking, image stitching, building 2D mosaics, stereo vision, image representation and other related computer vision areas.\n",
    "#  The Harris corner detection algorithm can be summarized as follows:\n",
    "# 1. **Compute Gradients (`Ix` and `Iy`):**\n",
    "#    - Convolve the image with Sobel-like kernels (`Kx` and `Ky`) to compute the image gradients in the x and y directions.\n",
    "# 2. **Compute Products of Gradients:**\n",
    "#    - Calculate the squared gradients (`Ixx` and `Iyy`) and the product of the gradients (`Ixy`).\n",
    "# 3. **Structure Tensor (Gaussian Smoothing):**\n",
    "#    - Apply Gaussian filtering to the products of gradients to compute the sums of the products within a local neighborhood (this represents the structure tensor matrix).\n",
    "# 4. **Harris Response Calculation:**\n",
    "#    - Compute the Harris response using the determinant and trace of the structure tensor matrix. The formula is:\n",
    "#      $$\n",
    "#      R = \\text{det}(M) - k \\cdot (\\text{trace}(M))^2\n",
    "#      $$\n",
    "#    - Here, `det(M)` is the determinant of the matrix $M$ and `trace(M)` is the trace of $M$.\n",
    "# 5. **Thresholding to Detect Corners:**\n",
    "#    - Threshold the Harris response to identify corners, setting pixels in the `corners` array to 1 where the response exceeds the threshold.\n",
    "# ### **Output:**\n",
    "# - `corners`: A binary image indicating the locations of the detected corners.\n",
    "# - `harris_response`: The Harris response image, where high values indicate strong corners.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve, gaussian_filter\n",
    "\n",
    "def harris_corner_detection(image, k=0.04, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Perform Harris corner detection on an image.\n",
    "    \n",
    "    Parameters:\n",
    "    image : numpy.ndarray\n",
    "        The input grayscale image.\n",
    "    k : float\n",
    "        The Harris detector free parameter (typically in the range [0.04, 0.06]).\n",
    "    threshold : float\n",
    "        Threshold for detecting corners (relative to the maximum value).\n",
    "    \n",
    "    Returns:\n",
    "    corners : numpy.ndarray\n",
    "        A binary image indicating the locations of corners.\n",
    "    harris_response : numpy.ndarray\n",
    "        The Harris response image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute the gradients\n",
    "    Kx = np.array([[-1, 0, 1], \n",
    "                   [-1, 0, 1], \n",
    "                   [-1, 0, 1]])\n",
    "    Ky = np.array([[ 1,  1,  1], \n",
    "                   [ 0,  0,  0], \n",
    "                   [-1, -1, -1]])\n",
    "\n",
    "    Ix = convolve(image, Kx)\n",
    "    Iy = convolve(image, Ky)\n",
    "    \n",
    "    # Step 2: Compute products of gradients\n",
    "    Ixx = Ix ** 2\n",
    "    Iyy = Iy ** 2\n",
    "    Ixy = Ix * Iy\n",
    "\n",
    "    # Step 3: Apply Gaussian filter to the products of gradients\n",
    "    Sxx = gaussian_filter(Ixx, sigma=1)\n",
    "    Syy = gaussian_filter(Iyy, sigma=1)\n",
    "    Sxy = gaussian_filter(Ixy, sigma=1)\n",
    "\n",
    "    # Step 4: Compute the Harris response\n",
    "    det_M = (Sxx * Syy) - (Sxy ** 2)\n",
    "    trace_M = Sxx + Syy\n",
    "    harris_response = det_M - k * (trace_M ** 2)\n",
    "\n",
    "    # Step 5: Thresholding to detect corners\n",
    "    corners = np.zeros_like(harris_response)\n",
    "    corners[harris_response > threshold * harris_response.max()] = 1\n",
    "\n",
    "    # Step 6: Non-maximum suppression\n",
    "    # Only keep local maxima in the Harris response\n",
    "    for y in range(1, harris_response.shape[0] - 1):\n",
    "        for x in range(1, harris_response.shape[1] - 1):\n",
    "            if harris_response[y, x] != np.max(harris_response[y-1:y+2, x-1:x+2]):\n",
    "                corners[y, x] = 0\n",
    "    \n",
    "    return corners, harris_response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample grayscale image (replace with actual image data)\n",
    "    image = np.random.rand(128, 128)\n",
    "\n",
    "    # Apply Harris corner detection\n",
    "    corners, harris_response = harris_corner_detection(image, k=0.04, threshold=0.01)\n",
    "\n",
    "    print(\"Corners Detected (Binary Image):\")\n",
    "    print(corners)\n",
    "    print(\"\\nHarris Response Image:\")\n",
    "    print(harris_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hough Transformation**\n",
    "### **Step-by-Step Hough Transform Implementation**\n",
    "\n",
    "1. **Edge Detection**: \n",
    "   - First, we need to detect edges in the image, typically using an edge detection algorithm like Canny. For simplicity, we'll use a simple Sobel operator for edge detection.\n",
    "2. **Hough Transform for Line Detection**:\n",
    "   - We'll map each edge point in the image to all possible lines that could pass through that point. These lines are represented in polar coordinates (rho, theta).\n",
    "3. **Accumulator Array**:\n",
    "   - We'll use an accumulator array to count the number of edge points that contribute to each possible line (rho, theta). The peaks in this array represent the most likely lines in the image.\n",
    "4. **Finding the Peaks**:\n",
    "   - The locations of the peaks in the accumulator array correspond to the parameters (rho, theta) of the lines in the image.\n",
    "### **Summary:**\n",
    "This implementation demonstrates how to perform line detection using the Hough Transform without relying on OpenCV. The core of the method involves transforming the problem of detecting lines in the image space into a problem of finding peaks in the Hough parameter space. This approach can be extended to detect other shapes, such as circles, by modifying the Hough space and voting procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houge Transform\n",
    "# https://en.wikipedia.org/wiki/Hough_transform\n",
    "# ### **Explanation:**\n",
    "# 1. **Edge Detection (sobel_edge_detection)**:\n",
    "#    - We use a simple Sobel operator to detect edges in the image. The Sobel operator computes the gradient magnitude of the image, highlighting regions with significant intensity changes.\n",
    "# 2. **Hough Transform (hough_transform)**:\n",
    "#    - We define a parameter space in terms of rho (the distance from the origin to the closest point on the line) and theta (the angle of the normal to the line).\n",
    "#    - For each edge pixel in the image, we calculate all possible (rho, theta) pairs that could represent a line passing through that pixel.\n",
    "#    - We accumulate votes in the Hough space for each possible line.\n",
    "# 3. **Find Peaks (find_peaks)**:\n",
    "#    - After voting in the Hough space, the accumulator array contains peaks where many edge pixels align along a line.\n",
    "#    - We find these peaks, which correspond to the most likely lines in the original image.\n",
    "# 4. **Visualization**:\n",
    "#    - The detected lines are visualized by plotting the peaks in the Hough space.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sobel_edge_detection(image):\n",
    "    \"\"\"\n",
    "    Simple Sobel edge detection.\n",
    "    \n",
    "    Parameters:\n",
    "    image : numpy.ndarray\n",
    "        The input grayscale image.\n",
    "    \n",
    "    Returns:\n",
    "    edges : numpy.ndarray\n",
    "        The edge-detected image.\n",
    "    \"\"\"\n",
    "    Kx = np.array([[-1, 0, 1], \n",
    "                   [-2, 0, 2], \n",
    "                   [-1, 0, 1]])\n",
    "    Ky = np.array([[ 1,  2,  1], \n",
    "                   [ 0,  0,  0], \n",
    "                   [-1, -2, -1]])\n",
    "    \n",
    "    Ix = convolve(image, Kx)\n",
    "    Iy = convolve(image, Ky)\n",
    "    \n",
    "    magnitude = np.sqrt(Ix**2 + Iy**2)\n",
    "    \n",
    "    return magnitude\n",
    "\n",
    "def hough_transform(image, num_thetas=180):\n",
    "    \"\"\"\n",
    "    Perform the Hough transform for line detection.\n",
    "    \n",
    "    Parameters:\n",
    "    image : numpy.ndarray\n",
    "        The edge-detected image.\n",
    "    num_thetas : int\n",
    "        The number of angles to consider between 0 and 180 degrees.\n",
    "    \n",
    "    Returns:\n",
    "    accumulator : numpy.ndarray\n",
    "        The accumulator array representing the Hough space.\n",
    "    thetas : numpy.ndarray\n",
    "        Array of theta values.\n",
    "    rhos : numpy.ndarray\n",
    "        Array of rho values.\n",
    "    \"\"\"\n",
    "    # Step 1: Define the Hough space\n",
    "    thetas = np.deg2rad(np.linspace(-90.0, 90.0, num_thetas))\n",
    "    width, height = image.shape\n",
    "    diag_len = int(np.sqrt(width**2 + height**2))  # Maximum possible rho\n",
    "    rhos = np.linspace(-diag_len, diag_len, diag_len * 2)\n",
    "\n",
    "    # Step 2: Create the accumulator array\n",
    "    accumulator = np.zeros((2 * diag_len, num_thetas), dtype=np.int32)\n",
    "\n",
    "    # Step 3: Find the edge (non-zero) pixels\n",
    "    y_idxs, x_idxs = np.nonzero(image)  # (row, col) indexes\n",
    "\n",
    "    # Step 4: Vote in the Hough space\n",
    "    for i in range(len(x_idxs)):\n",
    "        x = x_idxs[i]\n",
    "        y = y_idxs[i]\n",
    "\n",
    "        for t_idx in range(len(thetas)):\n",
    "            theta = thetas[t_idx]\n",
    "            rho = int(x * np.cos(theta) + y * np.sin(theta))\n",
    "            rho_idx = np.argmin(np.abs(rhos - rho))\n",
    "            accumulator[rho_idx, t_idx] += 1\n",
    "\n",
    "    return accumulator, thetas, rhos\n",
    "\n",
    "def find_peaks(accumulator, num_peaks, threshold=0):\n",
    "    \"\"\"\n",
    "    Find peaks in the Hough accumulator array.\n",
    "    \n",
    "    Parameters:\n",
    "    accumulator : numpy.ndarray\n",
    "        The Hough accumulator array.\n",
    "    num_peaks : int\n",
    "        The number of peaks to find.\n",
    "    threshold : int\n",
    "        Minimum value in the accumulator to be considered a peak.\n",
    "    \n",
    "    Returns:\n",
    "    peaks : list of tuples\n",
    "        List of (rho_index, theta_index) corresponding to the peaks in the accumulator.\n",
    "    \"\"\"\n",
    "    peaks = []\n",
    "    temp_accumulator = np.copy(accumulator)\n",
    "    \n",
    "    for _ in range(num_peaks):\n",
    "        idx = np.argmax(temp_accumulator)\n",
    "        rho_idx, theta_idx = np.unravel_index(idx, temp_accumulator.shape)\n",
    "        if temp_accumulator[rho_idx, theta_idx] > threshold:\n",
    "            peaks.append((rho_idx, theta_idx))\n",
    "            temp_accumulator[rho_idx, theta_idx] = 0  # Zero out the peak to find the next one\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a simple binary image with lines\n",
    "    image = np.zeros((100, 100), dtype=np.uint8)\n",
    "    image[30, :] = 255  # Horizontal line\n",
    "    image[:, 50] = 255  # Vertical line\n",
    "    \n",
    "    # Step 1: Perform Sobel edge detection\n",
    "    edges = sobel_edge_detection(image)\n",
    "    \n",
    "    # Step 2: Apply Hough Transform\n",
    "    accumulator, thetas, rhos = hough_transform(edges)\n",
    "    \n",
    "    # Step 3: Find the peaks in the accumulator\n",
    "    peaks = find_peaks(accumulator, num_peaks=2, threshold=50)\n",
    "    \n",
    "    # Step 4: Plot the results\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.title('Edge-detected image')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(accumulator, cmap='hot', aspect='auto')\n",
    "    plt.title('Hough Transform')\n",
    "    plt.xlabel('Theta (radians)')\n",
    "    plt.ylabel('Rho (pixels)')\n",
    "    \n",
    "    for peak in peaks:\n",
    "        rho_idx, theta_idx = peak\n",
    "        plt.plot(theta_idx, rho_idx, 'wo')  # Plot the peaks\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "# https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "# PCA is a dimensionality reduction technique that is commonly used in machine learning and data analysis.\n",
    "# Step-by-Step PCA Implementation\n",
    "# Center the Data:\n",
    "#   - Subtract the mean of each feature from the dataset to center the data around the origin.\n",
    "# Compute the Covariance Matrix:\n",
    "#   - Calculate the covariance matrix of the centered data to understand how the features vary with each other.\n",
    "# Compute the Eigenvalues and Eigenvectors:\n",
    "#   - Find the eigenvalues and eigenvectors of the covariance matrix. \n",
    "#   - The eigenvectors represent the directions of the new axes (principal components), \n",
    "#   - and the eigenvalues represent the amount of variance captured by each principal component.\n",
    "# Sort Eigenvectors:\n",
    "#   - Sort the eigenvectors by the magnitude of their corresponding eigenvalues in descending order. This ensures that the first principal component captures the most variance.\n",
    "# Project the Data:\n",
    "#   - Project the original data onto the new set of axes defined by the principal components.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pca(X, num_components):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) on the dataset X.\n",
    "    \n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        The input data matrix with shape (n_samples, n_features).\n",
    "    num_components : int\n",
    "        The number of principal components to return.\n",
    "        \n",
    "    Returns:\n",
    "    X_pca : numpy.ndarray\n",
    "        The data projected onto the top 'num_components' principal components.\n",
    "    components : numpy.ndarray\n",
    "        The principal components (eigenvectors).\n",
    "    explained_variance : numpy.ndarray\n",
    "        The variance explained by each of the selected components (eigenvalues).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Center the data\n",
    "    X_meaned = X - np.mean(X, axis=0)\n",
    "\n",
    "    # Step 2: Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(X_meaned, rowvar=False)\n",
    "\n",
    "    # Step 3: Compute the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # Step 4: Sort the eigenvalues and corresponding eigenvectors in descending order\n",
    "    sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvalues = eigenvalues[sorted_index]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "\n",
    "    # Step 5: Select the top 'num_components' eigenvectors\n",
    "    components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "    # Step 6: Project the data onto the selected components\n",
    "    X_pca = np.dot(X_meaned, components)\n",
    "\n",
    "    return X_pca, components, sorted_eigenvalues[:num_components]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample dataset\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(10, 5)  # 10 samples, 5 features\n",
    "\n",
    "    # Perform PCA, reducing the data to 2 components\n",
    "    X_pca, components, explained_variance = pca(X, num_components=2)\n",
    "\n",
    "    print(\"Original Data:\\n\", X)\n",
    "    print(\"\\nPCA Reduced Data:\\n\", X_pca)\n",
    "    print(\"\\nPrincipal Components:\\n\", components)\n",
    "    print(\"\\nExplained Variance by Principal Components:\\n\", explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "# https://en.wikipedia.org/wiki/K-means_clustering\n",
    "import numpy as np\n",
    "\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        The input data matrix with shape (n_samples, n_features).\n",
    "    k : int\n",
    "        The number of clusters.\n",
    "    max_iters : int\n",
    "        Maximum number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "    centroids : numpy.ndarray\n",
    "        The final cluster centroids.\n",
    "    labels : numpy.ndarray\n",
    "        The label for each data point indicating which cluster it belongs to.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize centroids randomly from the data points\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = X[np.random.choice(n_samples, k, replace=False)]\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Step 2: Assign each point to the nearest centroid\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Step 3: Update centroids to be the mean of points in each cluster\n",
    "        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n",
    "        \n",
    "        # Step 4: Check for convergence (if centroids do not change)\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return centroids, labels\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample dataset\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(100, 2)  # 100 samples, 2 features\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    k = 3\n",
    "    centroids, labels = kmeans(X, k)\n",
    "\n",
    "    print(\"Cluster Centroids:\\n\", centroids)\n",
    "    print(\"\\nLabels for each point:\\n\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN (K-Nearest Neighbors) Classification\n",
    "# https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k=3):\n",
    "    \"\"\"\n",
    "    Perform K-Nearest Neighbors (KNN) classification.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train : numpy.ndarray\n",
    "        The training data matrix with shape (n_samples, n_features).\n",
    "    y_train : numpy.ndarray\n",
    "        The labels for the training data.\n",
    "    X_test : numpy.ndarray\n",
    "        The test data matrix with shape (n_test_samples, n_features).\n",
    "    k : int\n",
    "        The number of nearest neighbors to consider.\n",
    "    \n",
    "    Returns:\n",
    "    y_pred : numpy.ndarray\n",
    "        The predicted labels for the test data.\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    \n",
    "    for x in X_test:\n",
    "        # Step 1: Compute distances from x to all training samples\n",
    "        distances = np.linalg.norm(X_train - x, axis=1)\n",
    "        \n",
    "        # Step 2: Identify the k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = y_train[k_indices]\n",
    "        \n",
    "        # Step 3: Perform a majority vote to classify the test point\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        y_pred.append(most_common[0][0])\n",
    "    \n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample dataset\n",
    "    np.random.seed(0)\n",
    "    X_train = np.random.rand(10, 2)  # 10 samples, 2 features\n",
    "    y_train = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # Labels for the training data\n",
    "    \n",
    "    # Test data\n",
    "    X_test = np.random.rand(5, 2)  # 5 test samples, 2 features\n",
    "    \n",
    "    # Perform KNN classification\n",
    "    k = 3\n",
    "    y_pred = knn_predict(X_train, y_train, X_test, k)\n",
    "\n",
    "    print(\"Predicted Labels for Test Data:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Support Vector Machine (SVM)**\n",
    "\n",
    "1. **Data Preparation**: Convert images into a feature vector.\n",
    "2. **SVM Classifier Implementation**: Using a linear SVM for simplicity.\n",
    "3. **Training the SVM**: Using the hinge loss and gradient descent.\n",
    "4. **Prediction**: Classifying new images based on the trained SVM model.\n",
    "### **Summary**\n",
    "This code provides a basic implementation of an SVM for image classification from scratch. The process involves loading images, converting them to feature vectors, training a linear SVM, and making predictions. This approach is a good starting point for understanding how SVMs work in the context of image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machine (SVM) Implementation\n",
    "# https://en.wikipedia.org/wiki/Support-vector_machine\n",
    "# The SVM algorithm aims to find the optimal hyperplane that separates the data into different classes.\n",
    "# ### **Explanation:**\n",
    "# 1. **Data Preparation (`load_images_as_vectors`)**:\n",
    "#    - This function loads images, converts them to grayscale, resizes them, and flattens them into vectors that the SVM can process.\n",
    "# 2. **SVM Classifier (`LinearSVM`)**:\n",
    "#    - **`fit` Method**: Trains the SVM by minimizing the hinge loss function using gradient descent. The hinge loss is used because it's suitable for binary classification in SVM.\n",
    "#    - **`predict` Method**: Classifies new data by computing the linear combination of input features and weights, followed by applying the sign function.\n",
    "# 3. **Training**:\n",
    "#    - The `fit` function iterates over the data, updating the weights and bias based on whether the current sample is correctly classified. It applies regularization to prevent overfitting.\n",
    "# 4. **Prediction**:\n",
    "#    - The `predict` function uses the learned weights and bias to classify new samples by determining which side of the decision boundary they lie on.\n",
    "# ### Training and Evaluation**\n",
    "# In the example usage, we load image data, train the SVM, and predict the labels on the same data (for demonstration). In practice, you would split the data into training and test sets to evaluate the SVM's performance.\n",
    "# ### Limitations**\n",
    "# - **Linear SVM**: The implementation provided is for a linear SVM. For more complex tasks, you might need a kernelized SVM (e.g., RBF kernel), which is more challenging to implement from scratch.\n",
    "# - **Gradient Descent**: This implementation uses basic gradient descent, which might be slow. In practice, more sophisticated optimization techniques (like SMO for SVMs) are used.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ### **1. Data Preparation**\n",
    "# Before training the SVM, we need to prepare the image data. Typically, this involves converting images into a flattened vector of pixel intensities.\n",
    "def load_images_as_vectors(image_paths, image_size=(28, 28)):\n",
    "    \"\"\"\n",
    "    Load images and convert them into flattened vectors.\n",
    "\n",
    "    Parameters:\n",
    "    image_paths : list\n",
    "        List of paths to the image files.\n",
    "    image_size : tuple\n",
    "        The size to which each image will be resized.\n",
    "\n",
    "    Returns:\n",
    "    X : numpy.ndarray\n",
    "        Array of shape (n_samples, n_features) with flattened image data.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for path in image_paths:\n",
    "        image = load_image(path, image_size)\n",
    "        X.append(image.flatten())  # Flatten the image to a vector\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "def load_image(path, image_size):\n",
    "    \"\"\"Load an image, resize it, and convert to grayscale.\"\"\"\n",
    "    from PIL import Image  # Using PIL for image processing\n",
    "\n",
    "    img = Image.open(path).convert('L')  # Convert to grayscale\n",
    "    img = img.resize(image_size, Image.ANTIALIAS)\n",
    "    return np.array(img) / 255.0  # Normalize pixel values\n",
    "\n",
    "# ### **2. SVM Classifier Implementation**\n",
    "# Training the SVM using gradient descent to minimize the hinge loss.\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SVM model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy.ndarray\n",
    "            Training data of shape (n_samples, n_features).\n",
    "        y : numpy.ndarray\n",
    "            Target labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Convert labels to {-1, 1} for SVM\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # Correct classification, so we apply the regularization only\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    # Misclassification, apply the hinge loss gradient\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.learning_rate * y_[idx]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for the given data.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy.ndarray\n",
    "            Data to predict, of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray\n",
    "            Predicted labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.w) - self.b\n",
    "        return np.sign(linear_output)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample image paths and labels\n",
    "    image_paths = ['image1.png', 'image2.png', 'image3.png']  # Replace with actual paths\n",
    "    labels = np.array([0, 1, 0])  # Binary labels (e.g., 0 for cat, 1 for dog)\n",
    "\n",
    "    # Load images and prepare feature vectors\n",
    "    X = load_images_as_vectors(image_paths, image_size=(28, 28))\n",
    "    \n",
    "    # Train the SVM\n",
    "    svm = LinearSVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "    svm.fit(X, labels)\n",
    "    \n",
    "    # Predict on the training set\n",
    "    predictions = svm.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"Accuracy:\", np.mean(predictions == labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
